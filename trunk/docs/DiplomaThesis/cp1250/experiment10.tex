%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment}
\label{sec:experiment}
Pùvodním zámìrem experimentu v~této práci bylo vyzkoušet nìkterou existující metodu automatické sémantické anotace. Pøi hledání vhodné metody jsme zjistili, že ve~vìtšinì projektù je algoritmická dokumentace použitých metod velmi struèná (alespoò dokumentace, která je veøejnì k dispozici). Získat nìjaký použitelný kód nebo knihovnu bylo problematické. Výjimku tvoøí projekty KIM (popsaný v~sekci \ref{sec:KIM}) a GATE (sekce \ref{sec:GATE}). Avšak zamìøení tìchto dvou projektù pøíliš nevyhovovalo požadavkùm na~náš experiment.

Pøestavba architektury projektu KIM by byla pomìrnì nároèná a její výsledek nejistý. Pravdìpodobnì by vznikl pouze slabší \uv{bratr} bulharského KIM, který je podpoøen masivní databází informací v~pozadí. %která zde byla v~prùbìhu projektu KIM vytvoøena.

Projekt GATE je naproti KIM otevøený a modulární. Jedná se o velmi obecnou softwarovou základnu pro lingvistickou anotaci a extrakci informací, která nabízí mnoho možností, k~dispozici je velké množství funkèních modulù. Ale k~èemu pøesnì bychom GATE chtìli použít? Ponechali jsme tedy GATE jako otevøenou možnost a zaèali pøesnìji hledat a specifikovat problém, který budeme øešit.

Paralelnì s~touto prací jsme mohli sledovat vývoj prací Dušana Marušèáka a Roberta Novotného !!!citovat!!!. V~tìchto pracích se pokoušejí o~anotaci / extrakci informací ze~strukturovaných web-stránek. Tento pøístup se èasto oznaèuje jako konstrukce \emph{wrapper-u}. Obì tyto práce se opírají o~zajímavý nápad využití opakujících se struktur uvnitø stránek. Vzniklé metody jsou pak témìø nezávislé na~konkrétní podobì vstupní stránky. Avšak pevná (HTML) struktura stránky podmiòuje použití tìchto metod.

Zaèali jsme si uvìdomovat, mezeru v~oblasti extrakce informací z~pøirozeného textu v~Èeském jazyce. Nejsou nám známy výsledky žádné práce, která by se tímto tématem zabývala. Pøitom èeská poèítaèová lingvistika je na~velmi vysoké úrovni.

Otevøela se nám možnost spolupráce s~Martinem Labským a Vojtìchem Svátekem na~èásti projektu The RAINBOW Project\footnote{http://rainbow.vse.cz/}. Konkrétnì bychom se zde zabývali rozšíøením jejich systému pro extrakci informací pomocí \uv{extrakèní ontologie}. Jedná se o~propracovaný systém založený na~široké paletì extrakèních pravidel, která jsou definovaná v~extrakèní ontologii. Jde však také pøedevším o konstrukci wrapper-u. Jedna z~možností naší spolupráce mìla spoèívat v~rozšíøení palety pravidel jejich projektu o~pravidla založená na~lingvistice.

K~žádné spolupráci zatím nedošlo, ale v~experimentu této práce se pokoušíme otestovat dostupné nástroje pro~lingvistickou anotaci èeských textù a prozkoumat možnosti jejich využití pro~extrakci informací a automatickou sémantickou anotaci.

%Jmenovitì se jednalo o tyto nástroje: !!!!!!!!!!!!!doplnit!!!!!!!!!!!!!!!!. Byly

%!!!!!!!Pzor následující odstavec jsem okopíroval i do kapitoly pøínosy - vyøešit

%Postup experimentu se v jednotlivých fázích snaží kopírovat skuteèné akce, které by bylo nutné provést v opravdovém projektu zamìøeném na sémantickou anotaci. V práci tak vzniká jednoduchá základní analýza tohoto typu projektù. Ve skuteèném projektu pak bude možné ji pøinejmenším jako inspiraci využít.


\section{Osnova prací}
\label{sec:osnaova_praci}
Postupem prací v~experimentu se snažíme simulovat postup opravdového projektu, který by byl zamìøený na dodateènou automatickou sémantickou anotaci nìkterých zdrojù webu. Tedy jsme v~situaci, kdy chceme nìjakým zpùsobem využít data na webu publikovaná. Abychom mohli tato data použít, potøebujeme je získat v~takové formì, aby se dala strojovì zpracovávat a vyhodnocovat. Na webu jsou však tato data publikována tak, aby si je mohli prohlížet obyèejní lidští návštìvníci, nemají strukturu, kterou požadujeme.

Data která nás zajímají\footnote{Podrobnìjší diskuse o~vstupních datech experimentu je v~oddíle \ref{sec:data_in}.} mohou být vyjádøena pøirozeným jazykem v~textech èlánkù nìkolika webových portálù. V~našem experimentu se jedná o~èlánky hasièského zpravodajství z~èeských regionù na portálu MVÈR (podrobnìji v~oddíle \ref{sec:data_in}). Abychom se od tìchto èlánkù dostali k~datùm, která potøebujeme, budeme postupovat podle následující osnovy.

\begin{figure}[!thb]
\begin{center}
\includegraphics[width=7cm, height=9cm]{../AP_schema1.eps}
\caption{Schéma aplikace } \label{pict:AP_schema1}
\end{center}
\end{figure}


\subsection{Pøíprava vstupních dat}
První, co musíme udìlat, je stáhnout požadované èlánky z~internetu k~dalšímu zpracování. Programy, které se zabývají touto èinností nazýváme \emph{web crawler}. V~našem experimentu jsme naprogramovali velmi jednoduchý web crawler, který využívá kanál RSS\footnote{RSS -- RDF Site Summary bývá oznaèováno \cite{SemWeb_Matulik} jako technologie sémantického webu.} publikovaný na~portálu MVÈR a stáhne všechny web-stránky s~èlánky, které nás zajímají.

Nyní potøebujeme ze~stažených èlánkù extrahovat text, který budeme analyzovat. V~pøípadì hasièských èlánkù to nebyl velký problém. Staèil jednoduchý skript, který pomocí nìkolika regulárních výrazù oddìlil text èlánku od HTML struktury web-stránky. Pøi našem druhém pokusu s~daty evidence úpadcù ÈR (viz \ref{sec:upadci}) jsme narazili na problém se~specifickými formáty textu (DOC a RTF). Automatické zpracování tìchto dat by bylo implementaènì nároèné a kladlo by pøemrštìné èasové nároky. Pro~potøeby experimentu jsme tato data zpracovali v~malém rozsahu ruènì.

Nesnadným problémem je obecná automatizace pøedchozích dvou procedur. Napøíklad pro~stahování \uv{zajímavých} èlánkù by se dal použít univerzální web crawler nìjakého internetového vyhledávaèe, tím je napøíklad Egothor\footnote{http://www.egothor.org/} vyvíjený týmem Lea Galamboše. Stránky, které tento crawler stahuje, by se filtrovaly pomocí heuristiky. Ta by vybrala stránky, které má cenu dále zpracovávat. Následuje problém, jak na~stránce automaticky najít texty, které nás zajímají. Pravdìpodobnì by se i tento problém dal øešit pomocí nìjaké heuristiky s~podporou metod pro konstrukci wrapper-u. Poznamenejme ještì, že Egothor kromì HTML zpracovává i zdroje ve~formátech PDF, PS, DOC a XLS.

Poslední transformace, kterou jsme pøed lingvistickým zpracováním textù provedli, byl pøevod kódování èeských znakù, pøeklad znakových entit HTML (\verb|&nbsp; &amp;| ...) a sjednocení zápisu èasových údajù (10:45 $\rightarrow$ 10.45, dvojteèku považoval lingvistický analyzátor za oddìlovaè slov, zatímco èasový údaj zapsaný s~teèkou vyhodnocuje správnì). Tyto transformace se dají snadno automatizovat, pouze pøi pøevodu kódování èeských znakù musíme správnì urèit originální kódování zdroje.

Podrobnosti o~implementaci této fáze našeho experimentu je možné nalézt v~sekci \ref{sec:data_prepare}.

\subsection{Lingvistická anotace}
Nyní se nacházíme v situaci, kdy máme texty, které chceme analyzovat uložené v~prostých textových souborech ve~správném kódování (ISO 8859-2). Spustíme jejich automatickou lingvistickou anotaci, která se skládá z~øetìzu nástrojù Tools for machine annotation - PDT 2.0 (viz sekce \ref{sec:anot_tools}) a nástroje pro~tektogramatickou analýzu (sekce \ref{sec:t-nalysis}). 


Výstupem bude 

Pokud bych chtìli náš software veøejnì prezentovat nebo dokonce prodávat, bylo by nutné instalaci zjednodušit a ošetøit právní nároky tøetích stran. To však nebylo pøedmìtem našeho experimentu. Problematiku instalace jednotlivých komponent necháváme jejich autorùm.

\subsection{Extrakce dat}
\subsection{Formální reprezentace dat}
Technicky pouhá XSLT-transformace

Teoreticky hledání, rozvažování nad úèelem použití dat, tvorba ontologie.

Tuto fází jsme v~experimentu vynechali. 

nedostatek surových dat


Pokud by se nám podaøilo konceptuálnì správnì data zachytit, vznikla by skuteènì sémantická anotace tìchto dat, která by byla nezávislá na úèelu použití.

Vzniklé vzory nejsou závislé na vstupních datech, jsou závislé pouze na jazyce (na Èeštinì). Navíc tektogramatický popis se snaží rozpouštìt rozdíly mezi jednotlivými jazyky. Pøi pøekladu lemmat (napøíklad pomocí WordNetu a ILI -- viz oddíl \ref{sec:EuroWordNet}) by se tyto vzory mohli stát též na jazyce nezávislé. Ovšem za~pøedpokladu, že bychom dokázali tektogramaticky analyzovat i ostatní jazyky. Mohla by pak vzniknout databáze lingvisticko-sémantických vzorù, která by se dala sdílet a rozšiøovat v~širokém spektru projektù.



\section{Vstupní data}
\label{sec:data_in}
Dlouho jsme hledali vhodný zdroj dat, na~kterém bychom pomocí experimentu ukázali výhody lingvistického pøístupu k~extrakci informací a sémantické anotaci. Potøebovali jsme zdroj, kde jsou data vyjádøena pøirozeným jazykem ve~volném textu. K~tomu, aby naše automatická metoda mohla ukázat nìjaký pøínos proti prostému manuálnímu pøepisu dat, potøebujeme, aby se v~textech opakovali informace podobného typu a aby byly podobnì vyjádøeny. V~textech díky tomu mùžeme najít vzory opakujících se tvrzení a pomocí nich hromadnì extrahovat data, která tato tvrzení vyjadøují.

Opakující se zprávy

Pro experiment byla vybrána a použita data ze dvou pomìrnì odlišných zdrojù.

Další možnosti:
analytické reporty, wikipedie (nemoci), sportovní zápasy, výroèní zprávy podnikù, PDT


Proè hasièi a úpadci?
\begin{itemize}
  \item nejvìtší míra opakování, zprávy se jedna druhé velmi podobají, pøitom pokaždé nesou rùzná data.
  \item Snadné stažení z~webu a extrakce textù.
\end{itemize}

\textbf{Proè ne korpus PDT?} --- Dùraz byl kladen na co možná nejvìtší pøiblížení se k~podmínkám a problémùm skuteèného projektu. V takovém pøípadì bychom se tìžko mohli opøít o to, že by nám data která chceme analyzovat nìkdo ruènì lingvisticky anotoval.

+ pokusy s PDT sample data. Nicménì pokusy nad ruèními lingvistickými anotacemi dat PDT\footnote{Jedná se o \emph{sample data} PDT 2.0, http://ufal.mf\/f.cuni.cz/pdt2.0/data/sample/} probìhly.

\subsection{Hasièi}
\label{sec:hasici}
V~našem experimentu se jedná o~èlánky hasièského zpravodajství z~èeských regionù na portálu MVÈR

Bleskové hasièské zpravodajství RSS z regionù

Ministerstvo vnitra Èeské republiky

http://www.mvcr.cz/rss/regionhzs.html

\subsection{Úpadci}
\label{sec:upadci}

Evidence úpadcù

Ministerstvo spravedlnosti Èeské republiky

http://portal.justice.cz/

\section{Výstupní data}
%Vzhledem k tomu, že pojem sémantické anotace, jak ho zmiòuji v kapitole \ref{sec:sem_anotace}, je velmi široký, není ani pøesnì urèeno, jaká data by pøi procesu sémantické anotace mìla vzniknout.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software}
\subsection{Skripty pro pøípravu dat}
\label{sec:data_prepare}

\subsection{Modul pro extrakci informací}
Makro btred

\subsection{Hledání pøíbuzných slov pomocí WordNetu}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Návrhy a zkušenosti}
\section{Dotazování nad lingvistickými závislostními stromy}
\label{sec:tree_query}

%kapitola Netgraph:: V prùbìhu experimentu, který je popsán v~kapitole \ref{sec:experiment}, jsme narazili na~potøebu dotazovacího jazyka, pomocí kterého bychom se mohli programovì dotazovat na~hodnoty atributù lingvistických stromù. Též by se nám velmi hodil jazyk, ve kterém by bylo možné vyjádøit vzory stromù, které se v~korpusu našeho experimentu èastìji vyskytují. Dotazovací jazyk, který používá aplikace Netgraph, je naší pøedstavì velmi blízký. Tuto problematiku podrobnìji rozebíráme v~oddíle \ref{sec:tree_query}.

Skryté uzly v~nástroji Netgraph XXX btred a rùzné roviny, \verb|PML_T::GetANodes($this)|

\begin{center}
\includegraphics[scale=0.8]{../NetGraph1.eps}
\end{center}

[]([]([],[]),[]) %pøekreslit

Za každým uzlem následuje v~kulatých závorkách èárkami oddìlený seznam jeho dìtí.

\subsection{Indexace}
\label{sec:tree_indexing}

Indexace XML dat (pro optimalizaci vyhodnocování dotazù XPath, XQuery a podobných) je v~souèasnosti otevøeným problémem. Stále ještì hledáme algoritmus, který by dokázal efektivnì indexovat XML data libovolné struktury. Tuto situaci ilustrují napøíklad èlánky
\cite{saptial_index}, \cite{baca_znalosti}.

\section{Indukce vzorù}
\label{sec:patern_induction} 