%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment}
\label{sec:experiment}
Pùvodním zámìrem experimentu v~této práci bylo vyzkoušet nìkterou existující metodu automatické sémantické anotace. Pøi hledání vhodné metody jsme zjistili, že ve~vìtšinì projektù je algoritmická dokumentace použitých metod velmi struèná (alespoò dokumentace, která je veøejnì k dispozici). Získat nìjaký použitelný kód nebo knihovnu bylo problematické. Výjimku tvoøí projekty KIM (popsaný v~sekci \ref{sec:KIM}) a GATE (sekce \ref{sec:GATE}). Avšak zamìøení tìchto dvou projektù pøíliš nevyhovovalo požadavkùm na~náš experiment.

Pøestavba architektury projektu KIM by byla pomìrnì nároèná a její výsledek nejistý. Pravdìpodobnì by vznikl pouze slabší \uv{bratr} bulharského KIM, který je podpoøen masivní databází informací v~pozadí. %která zde byla v~prùbìhu projektu KIM vytvoøena.

Projekt GATE je naproti KIM otevøený a modulární. Jedná se o velmi obecnou softwarovou základnu pro lingvistickou anotaci a extrakci informací, která nabízí mnoho možností, k~dispozici je velké množství funkèních modulù. Ale k~èemu pøesnì bychom GATE chtìli použít? Ponechali jsme tedy GATE jako otevøenou možnost a zaèali pøesnìji hledat a specifikovat problém, který budeme øešit.

Paralelnì s~touto prací jsme mohli sledovat vývoj prací Dušana Marušèáka a Roberta Novotného !!!citovat!!!. V~tìchto pracích se pokoušejí o~anotaci / extrakci informací ze~strukturovaných web-stránek. Tento pøístup se èasto oznaèuje jako konstrukce \emph{wrapper-u}. Obì tyto práce se opírají o~zajímavý nápad využití opakujících se struktur uvnitø stránek. Vzniklé metody jsou pak témìø nezávislé na~konkrétní podobì vstupní stránky. Avšak pevná (HTML) struktura stránky podmiòuje použití tìchto metod.

Zaèali jsme si uvìdomovat, mezeru v~oblasti extrakce informací z~pøirozeného textu v~Èeském jazyce. Nejsou nám známy výsledky žádné práce, která by se tímto tématem zabývala. Pøitom èeská poèítaèová lingvistika je na~velmi vysoké úrovni.

Otevøela se nám možnost spolupráce s~Martinem Labským a Vojtìchem Svátekem na~èásti projektu The RAINBOW Project\footnote{http://rainbow.vse.cz/}. Konkrétnì bychom se zde zabývali rozšíøením jejich systému pro extrakci informací pomocí \uv{extrakèní ontologie}. Jedná se o~propracovaný systém založený na~široké paletì extrakèních pravidel, která jsou definovaná v~extrakèní ontologii. Jde však také pøedevším o konstrukci wrapper-u. Jedna z~možností naší spolupráce mìla spoèívat v~rozšíøení palety pravidel jejich projektu o~pravidla založená na~lingvistice.

K~žádné spolupráci zatím nedošlo, ale v~experimentu této práce se pokoušíme otestovat dostupné nástroje pro~lingvistickou anotaci èeských textù a prozkoumat možnosti jejich využití pro~extrakci informací a automatickou sémantickou anotaci.

%Jmenovitì se jednalo o tyto nástroje: !!!!!!!!!!!!!doplnit!!!!!!!!!!!!!!!!. Byly

%!!!!!!!Pzor následující odstavec jsem okopíroval i do kapitoly pøínosy - vyøešit

%Postup experimentu se v jednotlivých fázích snaží kopírovat skuteèné akce, které by bylo nutné provést v opravdovém projektu zamìøeném na sémantickou anotaci. V práci tak vzniká jednoduchá základní analýza tohoto typu projektù. Ve skuteèném projektu pak bude možné ji pøinejmenším jako inspiraci využít.


\section{Osnova prací}
\label{sec:osnaova_praci}
Postupem prací v~experimentu se snažíme simulovat postup opravdového projektu, který by byl zamìøený na dodateènou automatickou sémantickou anotaci nìkterých zdrojù webu. Tedy jsme v~situaci, kdy chceme nìjakým zpùsobem využít data na webu publikovaná. Abychom mohli tato data použít, potøebujeme je získat v~takové formì, aby se dala strojovì zpracovávat a vyhodnocovat. Na webu jsou však tato data publikována tak, aby si je mohli prohlížet obyèejní lidští návštìvníci, nemají strukturu, kterou požadujeme.

Data která nás zajímají\footnote{Podrobnìjší diskuse o~vstupních datech experimentu je v~oddíle \ref{sec:data_in}.} mohou být vyjádøena pøirozeným jazykem v~textech èlánkù nìkolika webových portálù. V~našem experimentu se jedná o~èlánky hasièského zpravodajství z~èeských regionù na portálu MVÈR (podrobnìji v~oddíle \ref{sec:data_in}). Abychom se od tìchto èlánkù dostali k~datùm, která potøebujeme, budeme postupovat podle následující osnovy.

\begin{figure}[!thb]
\begin{center}
\includegraphics[width=7cm, height=9cm]{../AP_schema1.eps}
\caption{Schéma aplikace } \label{pict:AP_schema1}
\end{center}
\end{figure}


\subsection{Pøíprava vstupních dat}
\label{sec:phase_data_prepare}
První, co musíme udìlat, je stáhnout požadované èlánky z~internetu k~dalšímu zpracování. Programy, které se zabývají touto èinností nazýváme \emph{web crawler}. V~našem experimentu jsme naprogramovali velmi jednoduchý web crawler, který využívá kanál RSS\footnote{RSS -- RDF Site Summary bývá oznaèováno \cite{SemWeb_Matulik} jako technologie sémantického webu.} publikovaný na~portálu MVÈR a stáhne všechny web-stránky s~èlánky, které nás zajímají.

Nyní potøebujeme ze~stažených èlánkù extrahovat text, který budeme analyzovat. V~pøípadì hasièských èlánkù to nebyl velký problém. Staèil jednoduchý skript, který pomocí nìkolika regulárních výrazù oddìlil text èlánku od HTML struktury web-stránky. Pøi našem druhém pokusu s~daty evidence úpadcù ÈR (viz \ref{sec:upadci}) jsme narazili na problém se~specifickými formáty textu (DOC a RTF). Automatické zpracování tìchto dat by bylo implementaènì nároèné a kladlo by pøemrštìné èasové nároky. Pro~potøeby experimentu jsme tato data zpracovali v~malém rozsahu ruènì.

Nesnadným problémem je obecná automatizace pøedchozích dvou procedur. Napøíklad pro~stahování \uv{zajímavých} èlánkù by se dal použít univerzální web crawler nìjakého internetového vyhledávaèe, tím je napøíklad Egothor\footnote{http://www.egothor.org/} vyvíjený týmem Lea Galamboše. Stránky, které tento crawler stahuje, by se filtrovaly pomocí heuristiky. Ta by vybrala stránky, které má cenu dále zpracovávat. Následuje problém, jak na~stránce automaticky najít texty, které nás zajímají. Pravdìpodobnì by se i tento problém dal øešit pomocí nìjaké heuristiky s~podporou metod pro konstrukci wrapper-u. Poznamenejme ještì, že Egothor kromì HTML zpracovává i zdroje ve~formátech PDF, PS, DOC a XLS.

Poslední transformace, kterou jsme pøed lingvistickým zpracováním textù provedli, byl pøevod kódování èeských znakù, pøeklad znakových entit HTML (\verb|&nbsp; &amp;| ...) a sjednocení zápisu èasových údajù (10:45 $\rightarrow$ 10.45, dvojteèku považoval lingvistický analyzátor za oddìlovaè slov, zatímco èasový údaj zapsaný s~teèkou vyhodnocuje správnì). Tyto transformace se dají snadno automatizovat, pouze pøi pøevodu kódování èeských znakù musíme správnì urèit originální kódování zdroje.

Podrobnosti o~implementaci této fáze našeho experimentu je možné nalézt v~sekci \ref{sec:data_prepare}.

\subsection{Lingvistická anotace}
\label{sec:phase_ling_anot}
Tato fáze pøedstavuje pøevod prostých textù na strukturovaná data lingvistických anotací. V~souèasné dobì nemáme na~výbìr moc možností jak tuto fázi realizovat. Kromì lingvistických analyzátorù PDT, existují ještì nástroje vyvíjené na~Masarykovì univerzitì v~Brnì, o~nich se zmiòuje napøíklad práce \cite{Capek}. Bylo by jistì zajímavé v~experimentu obì varianty porovnat, ale už samotné poskládání nástrojù PDT bylo organizaènì pomìrnì nároèné, realizace téhož s~brnìnskou stranou by práci èasovì protáhla a domníváme se, že by pro tuto práci nebyla \uv{pøevratným} pøínosem. Tento potenciální pøínos ale nepopíráme.

V~našem experimentu se nyní nacházíme v situaci, kdy máme texty, které chceme analyzovat uložené v~prostých textových souborech ve~správném kódování (ISO 8859-2). Spustíme jejich automatickou lingvistickou anotaci, která se skládá z~øetìzu nástrojù Tools for machine annotation - PDT 2.0 (viz sekce \ref{sec:anot_tools}) a nástroje pro~tektogramatickou analýzu (sekce \ref{sec:t-nalysis}). Po delší dobì (lingvistická anotace je èasovì pomìrnì nároèná) získáme výstup v~podobì lingvistických anotací na~všech rovinách popisu PDT, uložených ve formátu PML i PLS. Kvalita automaticky generovaných lingvistických anotací se rùzní vìta od vìty. Domníváme se však, že pro typ aplikací, který zde simulujeme, je kvalita anotací více-ménì dostaèující.

Programová realizace této fáze experimentu je popsaná v~sekci \ref{sec:data_prepare}.

\subsection{Extrakce dat}
V této fázi se budeme snažit pomocí struktury lingvistických anotací extrahovat data obsažená v~pùvodních textech.

\subsection{Formální reprezentace dat}
Technicky pouhá XSLT-transformace

Teoreticky hledání, rozvažování nad úèelem použití dat, tvorba ontologie.

Tuto fází jsme v~experimentu vynechali.

nedostatek surových dat


Pokud by se nám podaøilo konceptuálnì správnì data zachytit, vznikla by skuteènì sémantická anotace tìchto dat, která by byla nezávislá na úèelu použití.

Vzniklé vzory nejsou závislé na vstupních datech, jsou závislé pouze na jazyce (na Èeštinì). Navíc tektogramatický popis se snaží rozpouštìt rozdíly mezi jednotlivými jazyky. Pøi pøekladu lemmat (napøíklad pomocí WordNetu a ILI -- viz oddíl \ref{sec:EuroWordNet}) by se tyto vzory mohli stát též na jazyce nezávislé. Ovšem za~pøedpokladu, že bychom dokázali tektogramaticky analyzovat i ostatní jazyky. Mohla by pak vzniknout databáze lingvisticko-sémantických vzorù, která by se dala sdílet a rozšiøovat v~širokém spektru projektù.



\section{Vstupní data}
\label{sec:data_in}
Dlouho jsme hledali vhodný zdroj dat, na~kterém bychom pomocí experimentu ukázali výhody lingvistického pøístupu k~extrakci informací a sémantické anotaci. Potøebovali jsme zdroj, kde jsou data vyjádøena pøirozeným jazykem ve~volném textu. K~tomu, aby naše automatická metoda mohla ukázat nìjaký pøínos proti prostému manuálnímu pøepisu dat, potøebujeme, aby se v~textech opakovali informace podobného typu a aby byly podobnì vyjádøeny. V~textech díky tomu mùžeme najít vzory opakujících se tvrzení a pomocí nich hromadnì extrahovat data, která tato tvrzení vyjadøují.

Opakující se zprávy

Pro experiment byla vybrána a použita data ze dvou pomìrnì odlišných zdrojù.

Další možnosti:
analytické reporty, wikipedie (nemoci), sportovní zápasy, výroèní zprávy podnikù, PDT


Proè hasièi a úpadci?
\begin{itemize}
  \item nejvìtší míra opakování, zprávy se jedna druhé velmi podobají, pøitom pokaždé nesou rùzná data.
  \item Snadné stažení z~webu a extrakce textù.
\end{itemize}

\textbf{Proè ne korpus PDT?} --- Dùraz byl kladen na co možná nejvìtší pøiblížení se k~podmínkám a problémùm skuteèného projektu. V takovém pøípadì bychom se tìžko mohli opøít o to, že by nám data která chceme analyzovat nìkdo ruènì lingvisticky anotoval.

+ pokusy s PDT sample data. Nicménì pokusy nad ruèními lingvistickými anotacemi dat PDT\footnote{Jedná se o \emph{sample data} PDT 2.0, http://ufal.mf\/f.cuni.cz/pdt2.0/data/sample/} probìhly.

\subsection{Hasièi}
\label{sec:hasici}
Ministerstvo vnitra Èeské republiky pravidelnì zveøejòuje krátké zprávy o~akcích, kterých se úèastnily hasièské sbory jednotlivých regionù ÈR. Tyto zprávy se nám z~výše zmínìných dùvodù zdály vhodné jako zdrojová data experimentu. Tyto èlánky jsou k~dispozici na internetovém portálu MVÈR jako \uv{Bleskové hasièské zpravodajství RSS z~regionù}\footnote{http://www.mvcr.cz/rss/regionhzs.html}. Odtud je pomocí programových nástrojù (popsaných v~sekci \ref{sec:data_prepare}) stahujeme a dále zpracováváme. Aktuálnì zpracovávaný archiv obsahuje pøibližnì 700 èlánkù (celkem cca 1MB textových dat).

Tyto zprávy se nejèastìji týkají výjezdu hasièských oddílù k~dopravní nehodì nebo požáru, ménì èasto informují o~rùzných hasièských slavnostech a dalších zásazích hasièské služby (ochrana vodních zdrojù pøed chemickým zneèištìním, opatøení proti ptaèí chøipce, kuriózní nehody v~domácnostech).

\subsection{Úpadci}
\label{sec:upadci}

Evidence úpadcù

Ministerstvo spravedlnosti Èeské republiky

http://portal.justice.cz/

\section{Výstupní data}
%Vzhledem k tomu, že pojem sémantické anotace, jak ho zmiòuji v kapitole \ref{sec:sem_anotace}, je velmi široký, není ani pøesnì urèeno, jaká data by pøi procesu sémantické anotace mìla vzniknout.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software}
Pro ukládání a verzování programové èásti práce jsme využili služeb veøejného serveru BerliOS\footnote{Naše stránky zde mají adresu http://czsem.berlios.de/}, kde nám byl poskytnut úèet spolu s~veøejným SVN repository\footnote{Adresa naší SVN repository: http://svn.berlios.de/svnroot/repos/czsem/trunk/}. V~tomto SVN úložišti je možné nalézt aktuální verze softwarových komponent i textù této práce. Na CD-ROM pøiloženém k~této práci je v~adresáøi \emph{czsem} umístìna kopie SVN repository ve~verzi z~10.~8. Chystáme se však práci dále vyvíjet, proto doporuèujeme data v~SVN repository na~pøiloženém CD-ROM pøed používáním èi prohlížením aktualizovat (Pro~podrobnosti viz soubor ReadMe.txt na~pøiloženém CD-ROM).

\subsection{Instalace}
\label{sec:install}

Pro otestování jednotlivých programových èástí je nutná pomìrnì komplikovaná instalace použitých lingvistických nástrojù. K~publikaci nìkterých z~nich navíc nemáme svolení. Ètenáø, který pravdìpodobnì bude chtít programovou èást práce vyzkoušet, má dvì možnosti. Buï si všechny nástroje i s~licencí pro~jejich používání obstará sám nebo se mùže obrátit na~autory práce a vyžádat si zapùjèení tìchto nástrojù za~úèelem testování této práce -- tato èinnost pak bude pokryta platnou licencí autorù práce. Druhá varianta zahrnuje i propùjèení uživatelského úètu (vèetnì hesla) pro~pøístup k~webovému rozhraní èeského WordNetu (pro~podrobnosti o~èeském WordNetu viz~\ref{sec:wordnet_cz}).

Postup instalace vèetnì odkazù na~poskytovatele jednotlivých nástrojù je popsaný v~souboru \emph{install.txt}\footnote{Soubor install.txt je umístìný v~adresáøi \emph{docs/UserGuide} SVN repository.}.

Autoøi práce jsou si vìdomi toho, že pokud by chtìli svùj software veøejnì publikovat nebo dokonce prodávat, bylo by nutné instalaci zjednodušit a ošetøit právní nároky tøetích stran. To však nebylo pøedmìtem tohoto experimentu. Problematiku instalace externích komponent ponechme jejich autorùm.

\subsection{Skripty pro pøípravu dat}
\label{sec:data_prepare}
Skripty, pomocí kterých se stáhnou a transformují stránky hasièského zpravodajství jsou umístìné v~adresáøi \emph{data/hasici} SVN repository. Jedná se o~sadu \emph{bash} skriptù, které by mìli fungovat na vìtšinì UNIX-ových systémù. Pøed jejich spuštìním je však nutné do~systému nainstalovat potøebné lingvistické nástroje (pro popis instalace viz \ref{sec:install}). Kompletní dávku spustíme (na~pozadí) skriptem \emph{run\_parse\_background}. Tato dávka postupnì vykoná všechny akce popsané v~sekcích \ref{sec:phase_data_prepare} a \ref{sec:phase_ling_anot}. Výstupní lingvistické anotace budou uloženy v~adresáøi \emph{data/hasici/pml}. Celý bìh této dávky mùže trvat i více než tøi hodiny. Hlášení o~prùbìhu jednotlivých akcí je zaznamenáváno v~log-souboru \emph{parse.log}. Podrobnosti k~funkci jednotlivých skriptù je možné nalézt v~komentáøích uvnitø každého skriptu.


\subsection{Modul pro extrakci informací}
Makro btred

\subsection{Hledání pøíbuzných slov pomocí WordNetu}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Návrhy a zkušenosti}
\section{Dotazování nad lingvistickými závislostními stromy}
\label{sec:tree_query}

%kapitola Netgraph:: V prùbìhu experimentu, který je popsán v~kapitole \ref{sec:experiment}, jsme narazili na~potøebu dotazovacího jazyka, pomocí kterého bychom se mohli programovì dotazovat na~hodnoty atributù lingvistických stromù. Též by se nám velmi hodil jazyk, ve kterém by bylo možné vyjádøit vzory stromù, které se v~korpusu našeho experimentu èastìji vyskytují. Dotazovací jazyk, který používá aplikace Netgraph, je naší pøedstavì velmi blízký. Tuto problematiku podrobnìji rozebíráme v~oddíle \ref{sec:tree_query}.

Skryté uzly v~nástroji Netgraph XXX btred a rùzné roviny, \verb|PML_T::GetANodes($this)|

\begin{center}
\includegraphics[scale=0.8]{../NetGraph1.eps}
\end{center}

[]([]([],[]),[]) %pøekreslit

Za každým uzlem následuje v~kulatých závorkách èárkami oddìlený seznam jeho dìtí.

\subsection{Indexace}
\label{sec:tree_indexing}

Indexace XML dat (pro optimalizaci vyhodnocování dotazù XPath, XQuery a podobných) je v~souèasnosti otevøeným problémem. Stále ještì hledáme algoritmus, který by dokázal efektivnì indexovat XML data libovolné struktury. Tuto situaci ilustrují napøíklad èlánky
\cite{saptial_index}, \cite{baca_znalosti}.

\section{Indukce vzorù}
\label{sec:patern_induction} 