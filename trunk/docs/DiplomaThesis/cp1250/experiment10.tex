%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment}
\label{sec:experiment}
Pùvodním zámìrem experimentu v~této práci bylo vyzkoušet nìkterou existující metodu automatické sémantické anotace. Pøi hledání vhodné metody jsme zjistili, e ve~vìtšinì projektù je algoritmická dokumentace pouitıch metod velmi struèná (alespoò dokumentace, která je veøejnì k dispozici). Získat nìjakı pouitelnı kód nebo knihovnu bylo problematické. Vıjimku tvoøí projekty KIM (popsanı v~sekci \ref{sec:KIM}) a GATE (sekce \ref{sec:GATE}). Avšak zamìøení tìchto dvou projektù pøíliš nevyhovovalo poadavkùm na~náš experiment.

Pøestavba architektury projektu KIM by byla pomìrnì nároèná a její vısledek nejistı. Pravdìpodobnì by vznikl pouze slabší \uv{bratr} bulharského KIM, kterı je podpoøen masivní databází informací v~pozadí. %která zde byla v~prùbìhu projektu KIM vytvoøena.

Projekt GATE je naproti KIM otevøenı a modulární. Jedná se o velmi obecnou softwarovou základnu pro lingvistickou anotaci a extrakci informací, která nabízí mnoho moností, k~dispozici je velké mnoství funkèních modulù. Ale k~èemu pøesnì bychom GATE chtìli pouít? Ponechali jsme tedy GATE jako otevøenou monost a zaèali pøesnìji hledat a specifikovat problém, kterı budeme øešit.

Paralelnì s~touto prací jsme mohli sledovat vıvoj prací Dušana Marušèáka a Roberta Novotného !!!citovat!!!. V~tìchto pracích se pokoušejí o~anotaci / extrakci informací ze~strukturovanıch web-stránek. Tento pøístup se èasto oznaèuje jako konstrukce \emph{wrapper-u}. Obì tyto práce se opírají o~zajímavı nápad vyuití opakujících se struktur uvnitø stránek. Vzniklé metody jsou pak témìø nezávislé na~konkrétní podobì vstupní stránky. Avšak pevná (HTML) struktura stránky podmiòuje pouití tìchto metod.

Zaèali jsme si uvìdomovat, mezeru v~oblasti extrakce informací z~pøirozeného textu v~Èeském jazyce. Nejsou nám známy vısledky ádné práce, která by se tímto tématem zabıvala. Pøitom èeská poèítaèová lingvistika je na~velmi vysoké úrovni.

Otevøela se nám monost spolupráce s~Martinem Labskım a Vojtìchem Svátekem na~èásti projektu The RAINBOW Project\footnote{http://rainbow.vse.cz/}. Konkrétnì bychom se zde zabıvali rozšíøením jejich systému pro extrakci informací pomocí \uv{extrakèní ontologie}. Jedná se o~propracovanı systém zaloenı na~široké paletì extrakèních pravidel, která jsou definovaná v~extrakèní ontologii. Jde však také pøedevším o konstrukci wrapper-u. Jedna z~moností naší spolupráce mìla spoèívat v~rozšíøení palety pravidel jejich projektu o~pravidla zaloená na~lingvistice.

K~ádné spolupráci zatím nedošlo, ale v~experimentu této práce se pokoušíme otestovat dostupné nástroje pro~lingvistickou anotaci èeskıch textù a prozkoumat monosti jejich vyuití pro~extrakci informací a automatickou sémantickou anotaci.

%Jmenovitì se jednalo o tyto nástroje: !!!!!!!!!!!!!doplnit!!!!!!!!!!!!!!!!. Byly

%!!!!!!!Pzor následující odstavec jsem okopíroval i do kapitoly pøínosy - vyøešit

%Postup experimentu se v jednotlivıch fázích snaí kopírovat skuteèné akce, které by bylo nutné provést v opravdovém projektu zamìøeném na sémantickou anotaci. V práci tak vzniká jednoduchá základní analıza tohoto typu projektù. Ve skuteèném projektu pak bude moné ji pøinejmenším jako inspiraci vyuít.


\section{Osnova prací}
\label{sec:osnaova_praci}
Postupem prací v~experimentu se snaíme simulovat postup opravdového projektu, kterı by byl zamìøenı na dodateènou automatickou sémantickou anotaci nìkterıch zdrojù webu. Tedy jsme v~situaci, kdy chceme nìjakım zpùsobem vyuít data na webu publikovaná. Abychom mohli tato data pouít, potøebujeme je získat v~takové formì, aby se dala strojovì zpracovávat a vyhodnocovat. Na webu jsou však tato data publikována tak, aby si je mohli prohlíet obyèejní lidští návštìvníci, nemají strukturu, kterou poadujeme.

Data která nás zajímají\footnote{Podrobnìjší diskuse o~vstupních datech experimentu je v~oddíle \ref{sec:data_in}.} mohou bıt vyjádøena pøirozenım jazykem v~textech èlánkù nìkolika webovıch portálù. V~našem experimentu se jedná o~èlánky hasièského zpravodajství z~èeskıch regionù na portálu MVÈR (podrobnìji v~oddíle \ref{sec:data_in}). Abychom se od tìchto èlánkù dostali k~datùm, která potøebujeme, budeme postupovat podle následující osnovy, která je graficky znázornìna na~obrázku \ref{pict:AP_schema1}.

\begin{figure}[!thb]
\begin{center}
\includegraphics[width=7cm, height=9cm]{../AP_schema1.eps}
\caption{Schéma aplikace } \label{pict:AP_schema1}
\end{center}
\end{figure}


\subsection{Pøíprava vstupních dat}
\label{sec:phase_data_prepare}
První, co musíme udìlat, je stáhnout poadované èlánky z~internetu k~dalšímu zpracování. Programy, které se zabıvají touto èinností nazıváme \emph{web crawler}. V~našem experimentu jsme naprogramovali velmi jednoduchı web crawler, kterı vyuívá kanál RSS\footnote{RSS -- RDF Site Summary bıvá oznaèováno \cite{SemWeb_Matulik} jako technologie sémantického webu.} publikovanı na~portálu MVÈR a stáhne všechny web-stránky s~èlánky, které nás zajímají.

Nyní potøebujeme ze~staenıch èlánkù extrahovat text, kterı budeme analyzovat. V~pøípadì hasièskıch èlánkù to nebyl velkı problém. Staèil jednoduchı skript, kterı pomocí nìkolika regulárních vırazù oddìlil text èlánku od HTML struktury web-stránky. Pøi našem druhém pokusu s~daty evidence úpadcù ÈR (viz \ref{sec:upadci}) jsme narazili na problém se~specifickımi formáty textu (pøedevším formát DOC). Automatické zpracování tìchto dat by bylo implementaènì nároèné a kladlo by pøemrštìné èasové nároky. Pro~potøeby experimentu jsme tato data zpracovali v~malém rozsahu ruènì.

Nesnadnım problémem je obecná automatizace pøedchozích dvou procedur. Napøíklad pro~stahování \uv{zajímavıch} èlánkù by se dal pouít univerzální web crawler nìjakého internetového vyhledávaèe, tím je napøíklad Egothor\footnote{http://www.egothor.org/} vyvíjenı tımem Lea Galamboše. Stránky, které tento crawler stahuje, by se filtrovaly pomocí heuristiky. Ta by vybrala stránky, které má cenu dále zpracovávat. Následuje problém, jak na~stránce automaticky najít texty, které nás zajímají. Pravdìpodobnì by se i tento problém dal øešit pomocí nìjaké heuristiky s~podporou metod pro konstrukci wrapper-u. Poznamenejme ještì, e Egothor kromì HTML zpracovává i zdroje ve~formátech PDF, PS, DOC a XLS.

Poslední transformace, kterou jsme pøed lingvistickım zpracováním textù provedli, byl pøevod kódování èeskıch znakù, pøeklad znakovıch entit HTML (\verb|&nbsp; &amp;| ...) a sjednocení zápisu èasovıch údajù (10:45 $\rightarrow$ 10.45, dvojteèku povaoval lingvistickı analyzátor za oddìlovaè slov, zatímco èasovı údaj zapsanı s~teèkou vyhodnocuje správnì). Tyto transformace se dají snadno automatizovat, pouze pøi pøevodu kódování èeskıch znakù musíme správnì urèit originální kódování zdroje.

Podrobnosti o~implementaci této fáze našeho experimentu je moné nalézt v~sekci \ref{sec:data_prepare}.

\subsection{Lingvistická anotace}
\label{sec:phase_ling_anot}
Tato fáze pøedstavuje pøevod prostıch textù na strukturovaná data lingvistickıch anotací. V~souèasné dobì nemáme na~vıbìr moc moností jak tuto fázi realizovat. Kromì lingvistickıch analyzátorù PDT, existují ještì nástroje vyvíjené na~Masarykovì univerzitì v~Brnì, o~nich se zmiòuje napøíklad práce \cite{Capek}. Bylo by jistì zajímavé v~experimentu obì varianty porovnat, ale u samotné poskládání nástrojù PDT bylo organizaènì pomìrnì nároèné, realizace tého s~brnìnskou stranou by práci èasovì protáhla a domníváme se, e by pro tuto práci nebyla \uv{pøevratnım} pøínosem. Tento potenciální pøínos ale nepopíráme.

V~našem experimentu se nyní nacházíme v situaci, kdy máme texty, které chceme analyzovat uloené v~prostıch textovıch souborech ve~správném kódování (ISO 8859-2). Spustíme jejich automatickou lingvistickou anotaci, která se skládá z~øetìzu nástrojù Tools for machine annotation - PDT 2.0 (viz sekce \ref{sec:anot_tools}) a nástroje pro~tektogramatickou analızu (sekce \ref{sec:t-nalysis}). Po delší dobì (lingvistická anotace je èasovì pomìrnì nároèná) získáme vıstup v~podobì lingvistickıch anotací na~všech rovinách popisu PDT, uloenıch ve formátu PML i PLS. Kvalita automaticky generovanıch lingvistickıch anotací se rùzní vìta od vìty. Domníváme se však, e pro typ aplikací, kterı zde simulujeme, je kvalita anotací více-ménì dostaèující.

Programová realizace této fáze experimentu je popsaná v~sekci \ref{sec:data_prepare}.

\subsection{Extrakce dat}
V této fázi se budeme snait pomocí struktury lingvistickıch anotací extrahovat data obsaená v~pùvodních textech. Popíšeme zde \uv{øešení}, které jsme zvolili. Jedná se ale spíš o~prùzkum ne o~nìjakou ucelenou metodu. Pøí popisu tohoto øešení se budeme snait komentovat další alternativy a monosti.

\subsubsection{XML nebo btred?}
Nejprve jsme se chtìli s~daty blíe seznámit. V~tomto bodì jsme museli provést první rozhodnutí, toti jestli bude pro~naše úèely vhodnìjší na~programové úrovni pracovat s~lingvistickımi daty pøímo nebo prostøednictvím nástroje btred (popsanı v~sekci \ref{sec:tred}).

Lingvistická data ve~formátu PML jsou uloena jako~XML pomìrnì sloité struktury. Manipulace s~XML je v~souèasné dobì podpoøena širokou paletou programovıch nástrojù a nepøedstavuje pro~programátora velkou pøekáku, ale pøedpokládá detailní znalost struktury zpracovávanıch dat.

Btred, kterı nám byl lingvisty doporuèován, nabízí mnoho uiteènıch funkcí pro~manipulaci s~PML daty, tøi práci s~nástrojem btred tedy programátor nepotøebuje tak podrobné znalosti formátu PML, programátor ale musí zvládnout funkce tohoto nástroje. Jedinou moností, jak s~nástrojem btred pracovat, je naprogramování vlastního btred-makra, které pak tento nástroj nad~lingvistickımi daty vyhodnotí. Tato makra se zapisují v jazyce Perl, kterı je v~oblasti softwarovıch systémù zøídka pouívanı. Integrace btred-makra se zbytkem softwarového systému mùe bıt komplikovanìjší.

K~poèáteènímu prùzkumu jsme zvolili druhou variantu -- btred. Pro komplexní projekt by ale bylo vhodné tuto volbu ještì zváit a porovnat s monostmi vytvoøení a pouití dotazovacího jazyka nad lingvistickımi stromy, viz dále.

\subsubsection{Frekvenèní analıza}
Jako základní pohled na data nám poslouila frekvenèní analıza uzlù v~lingvistickıch stromech, konkrétnì analıza tektogramatickıch lemmat, zvláštì pak její podmnoina omezená pouze na~slovesa. Vısledky tìchto analız jsou uloeny v~souborech\footnote{Vısledky frekvenèních analız jsou uloeny v~souborech freq.txt a freq\_verb.txt pro~kadı datovı zdroj zvláš, konkrétnì v~adresáøích \emph{data/hasici} a \emph{data/upadci}.} \emph{freq.txt} a \emph{freq\_verb.txt} SVN repository.

\subsubsection{Pravidla pro extrakci dat}
Díky frekvenèním analızám jsme se mohli zamìøit na~ta tvrzení, která se v~textech èasto vyskytují. Vizuální prohlídka jednotlivıch vìt nám pak umonila vypozorovat ve~vìtách jednoduché vzory typu: Na slovese \texttt{zranit} visí pod~funktorem \texttt{PAT} podstrom vìty, kterı blíe specifikuje osoby, jich poèty, a druh zranìní, které utrpìly a tento podstrom má opìt ve~vìtšinì pøípadù podobnou strukturu. Na základì tìchto pozorování je moné zkonstruovat deterministická programová pravidla pro~extrakci dat. O~konstrukci nìkolika takovıch pravidel jsme se pokusili.

Ukázalo se, e naprogramování extrakèního pravidla pomocí nástroje btred pøedstavuje i pro~velmi jednoduchı vzor mnoho práce. Pøesto, e jsme se snaili program strukturovat do vìtšího mnoství obecnìjších funkcí a podprocedur, byl zápis pravidla velmi nepøehlednı. Tento závìr není pøekvapivı, více-ménì jsme ho pøedpokládali. Díky tomuto pokusu jsme ale podrobnìji poznali úskalí, která konstrukce tìchto pravidel pøináší.

\subsubsection{Zápis pravidel}
Programování extrakèních pravidel èistì pomocí nástroje btred nám ukázalo vıhody, které by pøinesl formální dotazovací jazyk nad~lingvistickımi stromy. Tento jazyk by dále umonil formalizovat pravidla pro~extrakci dat a formální zápis tìchto pravidel by umonil jejich uivatelskou editaci, strojovou indukci i sdílení. Bohuel ádnı takovı pøímo pouitelnı dotazovací jazyk pro~lingvistické stromy neexistuje. V~oddíle \ref{sec:tree_query} jsme se pokusili o~návrh takového jazyka a v~oddíle \ref{sec:patern_induction} se zamıšlíme nad~monostmi strojové indukce pravidel pro~extrakci dat.

\subsubsection{Vyuití WordNetu}
U v~zadání této práce je zmínka o~monosti vyuít databázi WordNet. Tato monost se nabízí právì zde. Pomocí lexikální sítì WordNetu by bylo moné zobecnit extrakèní pravidla. Napøíklad tam, kde bychom v~pùvodním extrakèním pravidle (bez WordNetu) poadovali pøesnou hodnotu tektogramatického lemma, bychom mohli povolit i jeho libovolné synonymum. Na jiném místì bychom mohli poadovat libovolné hyponymum pøípadnì libovolnı prvek podstromu lexikální dìdiènosti, napøíklad ve~zprávách o~dopravních nehodách bychom libovolné \emph{motorové vozidlo} nalezli v~podstromu lexikální dìdiènosti tohoto spojení.

Pøi~zkoumání èeského WordNetu jsme bohuel zjistili nedostateèné pokrytí èeské slovní zásoby a pomìrnì øídké provázání \emph{synsetù} sémantickımi hranami. Zvláštì patrné je to, pokud se zamìøíme na~nìjakou specifickou oblast, jako jsou napøíklad motorová vozidla. Nepovaujeme tedy za~pøínosné souèasnı èeskı WordNet pøímo na~extrakèní pravidla napojit. Na~druhou stranu se nemusíme vzdávat zobecnìní, které by sémantická lexikální sí pøinesla a mùeme na~základì WordNetu a dalších obdobnıch zdrojù takovou sí vytvoøit. Tato sí mùe bıt úzce specializovaná na~doménu ve~které se pohybujeme, nemusí zdaleka dosahovat rozsahu a plné obecnosti, která je na~WordNetu pozoruhodná.

Podrobnosti o~programovıch nástrojích, které jsme pro~zkoumání èeského WordNetu vyvinuly je moné nalézt v~sekci \ref{sec:wn_find}.

\subsubsection{Shrnutí}
V~praktickém experimentu jsme zjistili, e extrakci dat z~lingvisticky anotovanıch textù je moné provést pomocí extrakèních pravidel. Podrobnosti o~pravidlech, která jsme programovì realizovali pøedkládáme v~sekci~\ref{sec:extract_exp}. Vytvoøení takovıch funkèních pravidel je ale se souèasnımi programovımi prostøedky zbyteènì pracné a málo úèelné. Efektivní návrh a vyuití extrakèních pravidel by umonil dotazovací jazyk a jeho interpret nad~lingvistickımi stromy. Návrh takového jazyka pøedkládáme v~oddíle \ref{sec:tree_query}.

Další moností, jak extrahovat data z~lingvisticky anotovanıch textù, je zapojení nìkteré metody strojového uèení. Pro~tyto metody bychom musely vytvoøit trénovací data. Vytvoøení takovıch dat pøedstavuje mnoho ruèní práce, která však mùe bıt srovnatelná s~prací nutnou k~návrhu extrakèních pravidel.
%Zajímavou moností by mohla bıt kombinace obou pøístupù, toti metodou strojového uèení hledat extrakèní pravidla. ----- to je asi blbost ... museli bychom mít pravidla i tren. data
Za~nejlepší monost povaujeme poloautomatické vytvoøení extrakèních pravidel z~opakujících se vzorù ve~zkoumanıch datech. Diskuse nad~monostmi indukce takovıch vzorù je v~oddíle~\ref{sec:patern_induction}.

\subsection{Formální reprezentace dat}
Nacházíme se nyní v~situaci, kdy se nám podaøilo z~lingvisticky anotovanıch textù extrahovat data, která nás zajímají. Chtìli bychom je uchovávat v~takové formì, která by vystihovala jejich sémantiku. Vyuijeme tedy nìjakı konceptuální formální popis (ontologii), pomocí kterého data zapíšeme. Vznikne tak interpretace tìchto dat pomocí slovníku zvolené ontologie. Libovolnı programovı nástroj, kterı \uv{porozumí} dané ontologii, bude moci s~tìmito daty pracovat.

Technicky není tato fáze nároèná. Jedná se o~jednoduchou datovou transformaci, kterou je napøíklad pro~XML moné realizovat pomocí XSLT. V~našem experimentu jsme tuto fázi programovì nerealizovali, dùvodem byl mimo jiné nedostatek extrahovanıch dat.

Nároènost této fáze spoèívá v~nalezení, pøípadnì vytvoøení vhodné ontologie, pomocí které budeme data interpretovat. Je potøeba zváit všechny moné pøípady, ve~kterıch by se data dala pouít a najít takové øešení, které bude ve~vìtšinì pøípadù vyhovovat. Konkrétnì pouít rozšíøenou ontologii, pøípadnì maximálnì usnadnit mapování pouité ontologie na~koncepty ostatních. Pokud se podaøí konceptuálnì správnì data zachytit, vznikne skuteènì sémantická anotace tìchto dat, která není závislá na~úèelu jejich pouití.

Extrakèní vzory vzniklé v~pøedchozí fázi nejsou závislé na~vstupních datech, jsou závislé pouze na jazyce (na Èeštinì). Navíc tektogramatickı popis se snaí rozpouštìt rozdíly mezi jednotlivımi jazyky. Pøi pøekladu lemmat (napøíklad pomocí WordNetu a ILI -- viz oddíl \ref{sec:EuroWordNet}) by se tyto vzory mohli stát té na~jazyce nezávislé. Ovšem za~pøedpokladu, e bychom dokázali tektogramaticky analyzovat i ostatní jazyky a zdokonalili souèasnı EuroWordNet. Mohla by tak vzniknout databáze lingvisticko-sémantickıch vzorù, která by se dala sdílet a rozšiøovat v~širokém spektru projektù.



\section{Vstupní data}
\label{sec:data_in}
Dlouho jsme hledali vhodnı zdroj dat, na~kterém bychom pomocí experimentu ukázali vıhody lingvistického pøístupu k~extrakci informací a sémantické anotaci. Potøebovali jsme zdroj, kde jsou data vyjádøena pøirozenım jazykem ve~volném textu. K~tomu, aby naše automatická metoda mohla ukázat nìjakı pøínos proti prostému manuálnímu pøepisu dat, potøebujeme, aby se v~textech opakovali informace podobného typu a aby byly podobnì vyjádøeny. V~textech díky tomu mùeme najít vzory opakujících se tvrzení a pomocí nich hromadnì extrahovat data, která tato tvrzení nesou.

Tyto podmínky splòuje širší spektrum zdrojù. Uvaovali jsme o~anotaci analytickıch reportù o vısledcích dataminingu, o~èláncích otevøené encyklopedie Wikipedia, o~zprávách ze~sportovních zápasù i o~vıroèních zprávách podnikù ÈR. Vıhody sémanticky anotovanıch reportù zmiòujeme u v~úvodu (sekce \ref{sec:data_mine}).  V~pøípadì Wikipedia encyklopedie bychom naše sémantické anotace mohli ukládat pomocí SMW (viz sekce \ref{sec:SMW}) a pøispívat tak k~vytvoøení sémantické Wikipedie. V~tomto pøípadì bychom se ale museli zamìøit na~nìjakou uší oblast èlánkù.

Nakonec jsme vybrali dva pomìrnì odlišné zdroje: hasièské zpravodajství a databázi úpadcù ÈR. Hlavními dùvody tohoto rozhodnutí byla vysoká míra opakování se podobnıch témat ve~zprávách a pomìrnì snadná dostupnost tìchto dat. Oba zdroje podrobnìji popíšeme níe.

Další ponìkud odlišnou moností vstupních dat pøedstavuje korpus PDT. V~tomto pøípadì bychom se mohli opøít o~velmi kvalitní \uv{ruèní} lingvistické anotace. Data korpusu PDT jsou pestrá a opakující témata bychom zde hledali obtínì. Navíc dùraz experimentu byl kladen na co moná nejvìtší pøiblíení se k~podmínkám a problémùm skuteèného projektu. V~takovém pøípadì bychom se tìko mohli opøít o~to, e by nám data která chceme analyzovat nìkdo ruènì lingvisticky anotoval. Nicménì pokusy s~ruèními lingvistickımi anotacemi dat PDT\footnote{Jedná se o \emph{sample data} PDT 2.0, http://ufal.mf\/f.cuni.cz/pdt2.0/data/sample/} (pøedevším z~tréninkovıch dùvodù) probìhly.

\subsection{Hasièi}
\label{sec:hasici}
Ministerstvo vnitra Èeské republiky pravidelnì zveøejòuje krátké zprávy o~akcích, kterıch se úèastnily hasièské sbory jednotlivıch regionù ÈR. Tyto zprávy se nám z~vıše zmínìnıch dùvodù zdály vhodné jako zdrojová data experimentu. Tyto èlánky jsou k~dispozici na internetovém portálu MVÈR jako \uv{Bleskové hasièské zpravodajství RSS z~regionù}\footnote{http://www.mvcr.cz/rss/regionhzs.html}. Odtud je pomocí programovıch nástrojù (popsanıch v~sekci \ref{sec:data_prepare}) stahujeme a dále zpracováváme. Aktuálnì zpracovávanı archiv obsahuje pøiblinì 700 èlánkù (celkem cca 1MB textovıch dat).

Tyto zprávy se nejèastìji tıkají vıjezdu hasièskıch oddílù k~dopravní nehodì nebo poáru, ménì èasto informují o~rùznıch hasièskıch slavnostech a dalších zásazích hasièské sluby (ochrana vodních zdrojù pøed chemickım zneèištìním, opatøení proti ptaèí chøipce, kuriózní nehody v~domácnostech).

\subsection{Úpadci}
\label{sec:upadci}
Na Informaèním serveru èeského soudnictví\footnote{http://portal.justice.cz/} je veøejnì dostupná \emph{Evidence úpadcù} Ministerstva spravedlnosti Èeské republiky. Tato rozsáhlá a neustále aktualizovaná evidence obsahuje kromì dalších dat i texty jednotlivıch soudních rozhodnutí a ustanovení. Právì tyto texty jsme zvolili jako vstupní data experimentu.

Automatické staení a proèištìní dat z~tohoto datového zdroje by bylo pomìrnì komplikované. Texty jsou na~serveru uloené ve~formátu DOC, k~tìmto dokumentùm neexistují perzistentní URL, stahování jednotlivıch dokumentù probíhá pøes náhodnou doèasnì zvolenou adresu, která vzniká pøi generování web-stránky s detailním popisem kadého konkurzu. Ještì nároènìjší by však bylo automaticky extrahovat texty, které nás zajímají z~jednotlivıch DOC souborù. Tyto soubory nemají jednotnou strukturu, jsou napsané rùznımi autory, jsou rùznì formátované, texty ustálenıch nadpisù se vyskytují v~nìkolika variantách. 

Tento datovı zdroj hraje v~našem experimentu spíše doplòkovou roli. Chtìli jsme porovnat kvalitu lingvistickıch anotací a monosti naší metody na~datech v~další doménì. K~tomuto úèelu jsme tato data zpracovali v~malém rozsahu ruènì, jsou uloena v~souboru \emph{data/upadci/sample\_data.txt} SVN repository.


%\section{Vıstupní data}
%Vzhledem k tomu, e pojem sémantické anotace, jak ho zmiòuji v kapitole \ref{sec:sem_anotace}, je velmi širokı, není ani pøesnì urèeno, jaká data by pøi procesu sémantické anotace mìla vzniknout.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software}
Pro ukládání a verzování programové èásti práce jsme vyuili slueb veøejného serveru BerliOS\footnote{Naše stránky zde mají adresu http://czsem.berlios.de/}, kde nám byl poskytnut úèet spolu s~veøejnım SVN repository\footnote{Adresa naší SVN repository: http://svn.berlios.de/svnroot/repos/czsem/trunk/}. V~tomto SVN úloišti je moné nalézt aktuální verze softwarovıch komponent i textù této práce. Na CD-ROM pøiloeném k~této práci je v~adresáøi \emph{czsem} umístìna kopie SVN repository ve~verzi z~10.~8. Chystáme se však práci dále vyvíjet, proto doporuèujeme data v~SVN repository na~pøiloeném CD-ROM pøed pouíváním èi prohlíením aktualizovat (Pro~podrobnosti viz soubor ReadMe.txt na~pøiloeném CD-ROM).

\subsection{Instalace}
\label{sec:install}

Pro otestování jednotlivıch programovıch èástí je nutná pomìrnì komplikovaná instalace pouitıch lingvistickıch nástrojù. K~publikaci nìkterıch z~nich navíc nemáme svolení. Ètenáø, kterı pravdìpodobnì bude chtít programovou èást práce vyzkoušet, má dvì monosti. Buï si všechny nástroje i s~licencí pro~jejich pouívání obstará sám nebo se mùe obrátit na~autory práce a vyádat si zapùjèení tìchto nástrojù za~úèelem testování této práce -- tato èinnost pak bude pokryta platnou licencí autorù práce. Druhá varianta zahrnuje i propùjèení uivatelského úètu (vèetnì hesla) pro~pøístup k~webovému rozhraní èeského WordNetu (pro~podrobnosti o~èeském WordNetu viz~\ref{sec:wordnet_cz}).

Postup instalace vèetnì odkazù na~poskytovatele jednotlivıch nástrojù je popsanı v~souboru \emph{install.txt}\footnote{Soubor install.txt je umístìnı v~adresáøi \emph{docs/UserGuide} SVN repository.}.

Autoøi práce jsou si vìdomi toho, e pokud by chtìli svùj software veøejnì publikovat nebo dokonce prodávat, bylo by nutné instalaci zjednodušit a ošetøit právní nároky tøetích stran. To ale nebylo pøedmìtem tohoto experimentu. Problematiku instalace externích komponent ponechme jejich autorùm. Nejménì pøíjemná je instalace tektogramatického analyzátoru (sekce \ref{sec:t-nalysis}), tento nástroj je ale stále ve~vıvoji a jeho instalaci je nutné povaovat za~doèasné øešení.

\subsection{Skripty pro pøípravu dat}
\label{sec:data_prepare}
Skripty, pomocí kterıch se stáhnou a transformují stránky hasièského zpravodajství jsou umístìné v~adresáøi \emph{data/hasici} SVN repository. Jedná se o~sadu \emph{bash} skriptù, které by mìli fungovat na vìtšinì UNIX-ovıch systémù. Pøed jejich spuštìním je nutné do~systému nainstalovat potøebné lingvistické nástroje (pro popis instalace viz \ref{sec:install}). Kompletní dávku spustíme (na~pozadí) skriptem \emph{run\_parse\_background}. Tato dávka postupnì vykoná všechny akce popsané v~sekcích \ref{sec:phase_data_prepare} a \ref{sec:phase_ling_anot}. Vıstupní lingvistické anotace budou uloeny v~adresáøi \emph{data/hasici/pml}. Celı bìh této dávky mùe trvat i více ne tøi hodiny. Hlášení o~prùbìhu jednotlivıch akcí je zaznamenáváno v~log-souboru \emph{parse.log}. Podrobnosti k~funkci jednotlivıch skriptù je moné nalézt v~komentáøích uvnitø kadého skriptu.


\subsection{Modul pro extrakci informací}
\label{sec:extract_exp}

Makro btred

skripty pro frekvenèní analızu \emph{data/hasici/freq.btred}

Pro podrobnosti o~funkci jednotlivıch skriptù se opìt odvoláváme na~kód jednotlivıch skriptù a komentáøe uvnitø.

\subsection{Hledání pøíbuznıch slov pomocí WordNetu}
\label{sec:wn_find}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Návrhy a zkušenosti}
\section{Dotazování nad lingvistickımi závislostními stromy}
\label{sec:tree_query}

%kapitola Netgraph:: V prùbìhu experimentu, kterı je popsán v~kapitole \ref{sec:experiment}, jsme narazili na~potøebu dotazovacího jazyka, pomocí kterého bychom se mohli programovì dotazovat na~hodnoty atributù lingvistickıch stromù. Té by se nám velmi hodil jazyk, ve kterém by bylo moné vyjádøit vzory stromù, které se v~korpusu našeho experimentu èastìji vyskytují. Dotazovací jazyk, kterı pouívá aplikace Netgraph, je naší pøedstavì velmi blízkı. Tuto problematiku podrobnìji rozebíráme v~oddíle \ref{sec:tree_query}.

interpretace - XPath, XPath pøehlednìjší v~lineárním zápisu, vizualizace dotazu podobnì jako v NG klientu

Skryté uzly v~nástroji Netgraph XXX btred a rùzné roviny, \verb|PML_T::GetANodes($this)|

\begin{center}
\includegraphics[scale=0.8]{../NetGraph1.eps}
\end{center}

[]([]([],[]),[]) %pøekreslit

Za kadım uzlem následuje v~kulatıch závorkách èárkami oddìlenı seznam jeho dìtí.

\subsection{Extrakèní pravidla}

\subsection{Indexace}
\label{sec:tree_indexing}

Indexace XML dat (pro optimalizaci vyhodnocování dotazù XPath, XQuery a podobnıch) je v~souèasnosti otevøenım problémem. Stále ještì hledáme algoritmus, kterı by dokázal efektivnì indexovat XML data libovolné struktury. Tuto situaci ilustrují napøíklad èlánky
\cite{saptial_index}, \cite{baca_znalosti}.


\section{Indukce vzorù}
\label{sec:patern_induction}
klasifikace - dendrogram
od frekvenèní analızy lemmat dále po rùznıch osách struktury, rùzné délky tranzitivních hran stromu, zahrnou rùzné atributy

konstrukce dendrogramu interaktivní, napø. se fixují jednotlivá lemmata

pøevod vzoru na pravidlo: uivatel oznaèí atributy které ho zajímají, zapíše transformaci vıstupu na formální reprezentaci znalostí (4. krok osnovy), lze té vizualizovat ontologii a mapovat vizuálnì.

Oznaèí korpus a nasadit strojové uèení.
.. pravidla by vznikala jako èerná - a šedá skøíòka
.. problém s vytváøením korpusu, ruèní anotace


\section{Závìr práce}
Otevírá mnoho moností

Základní prùzkum
\\- teoretickı
\\- praktickı
