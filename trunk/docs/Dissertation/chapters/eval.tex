\chapter{Experiments and Evaluation} \label{sec:ch_eval}

Experiments that were performed to evaluate our methods and approaches will be presented in this chapter. Four main sections separate the experiments by topic, to which they belong.

\section{Evaluation of Manual Rules} \graphicspath{{../img/ch50/}} \label{sec:manual_eval}


In this section, two experiments will be presented to explain the usefulness of our extraction method based on manually designed rules. 
The first one provides measurements on a higher amount of texts without manual gold standard annotations, while the second experiment was done on a small manually annotated collection. 

\subsection{Czech Fireman Quantitative} \label{sec:manual_quant_experiment}

\begin{table}[b!]
	\centering
		\includegraphics[angle=-90,width=0.75\hsize]{tab_manual_rules}
	\caption[Quantitative evaluation of manually created rules.]{Quantitative  evaluation of manually created rules (without manual annotations).}
	\label{tab:manual_tab_manual_rules}
\end{table}

We evaluated three extraction rules (one procedural and two Netgraph based) on a set of 814 texts of news of several Czech fire departments and measured several statics. All the rules had the same goal: to find numbers of people that died or were injured during an accident. The procedural rule was the same as in Figure~\ref{fig:btred_rule} and the Netgraph based rules correspond with the rule in Figure~\ref{fig:manual_extract_patern}. The only difference between the Netgraph rules is that, in the first case (Netgraph 1), all rule nodes except number 1 (action\_type) were set as optional, while in the second case (Netgraph 2), the node~1 and also the~node~4~(participant) were compulsory and the remaining nodes were again optional. This little difference caused some interesting effects -- see below. 

Table~\ref{tab:manual_tab_manual_rules} summarizes the main statics that were measured. Description of individual values follows.


\begin{description}
	\item[Files:] \hfill \\
The same set of 814 files (texts) was used in all the experiments.

	\item[Rule matches:] \hfill \\
The presence of optional nodes in a query increases the number of possibilities how a~Netgraph query can be matched to a single linguistic tree. An optional node might or might not be marked in the result. The number of possible matches is also increased if there are more compatible nodes in a candidate tree that can be matched with a single query node. This can be true also for the \emph{participant} query node (4) in the case of the two Netgraph based rules if a sentence mentions more than one affected person. In the current implementation, only one of all possible matches is taken to the output (See also some details bellow in the description of \emph{effective rule matches}.) This issue can be regarded as a mistake in the rule design -- it does not count with such possibility, or it can be taken as a drawback of the current evaluation algorithm of the Netgraph based method -- it should put all the possibilities to the output. Note that this issue does not concern the procedural method, which outputs all the matching participants.

	\item[Unique matched trees:] \hfill \\
This number represents the number of unique trees matched by the extraction rule.

	\item[Effective rule matches:] \hfill \\
Because the procedural rules and the Netgraph based rules are evaluated in a different way, the way of selection of effective matches (matches that are used for the output) is also different. In the procedural case all matches are used because every such match is tied up with a different verb in a sentence. In this case more matches per sentence (tree) are only possible for complex sentences with more verbs and therefore every match is applicable because it is connected with a different piece of information.\\
In the Netgraph case it is necessary to select the most relevant matches of all possible ones. The first longest (maximum of optional nodes) match for each tree is selected. This is unfortunately not optimal and also not consistent with the procedural case, but it is the easiest option for the implementation (Netgraph can be directly used in that case.)

	\item[Negation, participant, participant quantity (count), action type:] \hfill \\
These values represent numbers of matching nodes (or more precisely of pieces of extracted information) of the given type. For example values of participant are the numbers of all participants (node number 4 in the Netgraph based rule) identified by the extraction rule and values of \emph{přežít} (survive) are numbers of matching action type nodes with the value of \emph{přežít} (survive). Note that some postprocessing of the Netgraph based output was necessary to count TRUE and FLASE negation values.

	\item[Participant quantity:] \hfill \\
Values in this group are all connected with the quantity kind of information. It expresses the quantity of participants involved in the corresponding incident action. This kind of information is numeric so some numerical calculations can be made (average value, median and maximum value). Again postprocessing of the Netgraph based output was necessary to obtain these values -- in this case translation of seven kinds of numerals to numbers (\emph{jeden} - 1, \emph{dva} - 2, \emph{tři} - 3, \emph{čtyři} - 4, (the numeral `five' was not encountered), \emph{šest} - 6, \emph{sedm} - 7, \emph{osm} - 8). Note that the vale of average is strongly affected by the very few high numbers present in the results (the values 1 and 2 accounted for more than half of the results because the median is 2.)\\
Values of participant quantity also demonstrate several errors made by the individual extraction rules, see the details bellow.
\end{description}





















\subsubsection{Detected errors}

Results of the extraction were investigated only partly; no formal evaluation is available in this case (except on a small evaluation set, see the second experiment bellow in Section~\ref{sec:manual_eval_qualitative}). About 10-20\% of information was not extracted because of errors of linguistic parsing; majority of the errors was made in long complex sentences, which are known to be difficult for linguistic processing. There was only a few of false positives. A nice example can be traced from the maximum values of participant quantity in Table~\ref{tab:manual_tab_manual_rules}. Three different numbers were found in the three experiments: 27800, 4988 and 333. The number of 27800 is actually a number of chickens that died during one of the accidents. The number was extracted by the first Netgraph based rule because the query node 4 (participant) was marked as optional and omitted during the evaluation. This node normally ensures that the participant is one of: person, man, woman, child, driver, etc.
Numbers 4988 and 333 are both correct. They were both used in the same sentence, which summarized numbers of injured (4988) and killed (333) people during one whole year. Although the sentence was quite complex it was linguistically parsed correctly and also the procedural rule managed to extract both numbers correctly. The Netgraph based rule extracted only the first number in the sentence because the current evaluation algorithm does not allow multiple matches per sentence (see above the comments of rule matches and effective rule matches).


\subsection{Czech Fireman Qualitative} \label{sec:manual_eval_qualitative}


\begin{table}[b!]
	\centering
	\begin{tabular}{|r|c|c|c|c|c|c|}
		\hline
		 & correct & missing & spurious & recall & precision & $F_1$\\
		\hline
		injuries & 3 & 29 & 0 & 0.09 & 1 & 0.17\\
		\hline
		fatalities & 1 & 10 & 0 & 0.09 & 1 & 0.17\\
		\hline
	\end{tabular}
	\caption[Evaluation of the manually created rule on the manually annotated dataset.]{Evaluation of the manually created rule form Figure~\ref{fig:manual_extract_patern} on the manually annotated dataset.}
	\label{tab:manual_extract_patern_eval}
\end{table}



In the second experiment, a manually annotated collection of 50 fireman news texts was used (the dataset was described in Section~\ref{sec:data_fireman_annotated}). Having the extraction rule from the previous experiment and a set of manually annotated texts it is natural to ask a question about the success of the extraction rule on this collection. Table~\ref{tab:manual_extract_patern_eval} summarizes the results. These results are far from satisfactory; the recall of 0.09 is something that is far from any acceptable use. Several explanations of the issue can be provided. The extraction rule serves more for a demonstration than for exhausting coverage of all possible cases. The extraction rule looks for particular verb (to injure, to die, etc.) but the information can be also expressed by an adjective (injured driver, death passenger, etc.); another extraction rules should be constructed for these and other cases. The training collection used for the design was also of a different spectrum of texts.



On the other hand this experiment shows how a manually annotated collection contributes to the quality of extraction rules. We can never know if an extraction rule is usable until a formal evaluated is made. Also the fact that the precision is strictly 1 should be noted. This means that the extraction rule made no mistake when it provided some output.



\subsubsection{Manual Design of Rules Using Training Data Set}  

\begin{table}
	\centering
	\begin{tabular}{|r|c|c|c|c|c|c|}
		\hline
		 & correct & missing & spurious & recall & precision & $F_1$\\
		\hline
		manual rules & 5 & 2 & 0 & 0.71 & 1 & 0.83\\
		\hline
		ILP rules & 5 & 2 & 0 & 0.71 & 1 & 0.83\\
		\hline
	\end{tabular}
	\caption[Evaluation of the manually created rules and ILP learned rules.]{Evaluation of the manually created rules and ILP learned rules (manually annotated dataset was used for rule design (training half) and evaluation (testing half) -- see the description of the second experiment in text.)}
	\label{tab:manual_damage_manual_eval}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|r|c|c|c|c|c|c|}
		\hline
		 & correct & missing & spurious & recall & precision & $F_1$\\
		\hline
		manual rules & 4 & 1 & 1 & 0.8 & 0.8 & 0.8\\
		\hline
		ILP rules & 4 & 1 & 1 & 0.8 & 0.8 & 0.8\\
		\hline
	\end{tabular}
	\caption{Cross coverage comparison of extraction methods.}
	\label{tab:manual_damage_cross_method}
\end{table}




Next question, that naturally emerges, is: How would be the performance if the rules were designed with the support of a manually annotated collection? An additional experiment was made to answer this question. The collection was split into two even parts -- training part and testing part. A manually created rule was designed so that it correctly matched with all annotations of the training part and then it was evaluated on the testing part. For the validity of the experiment it was necessary that the designer did not have any knowledge about the data of the testing part; that is why a different extraction task was used (`damage' instead of 'injuries and fatalities'). We have also compared the performance of the manually created rule with a rule learned by the ILP machine learning engine.

The results are summarized in Table~\ref{tab:manual_damage_manual_eval}. Both kinds of rules (manually designed and learned by ILP) performed the same (recall: 0.71, precision: 1); both the methods correctly found 5 instances and they were both unable to find 2 instances. From the cross coverage comparison in 
Table~\ref{tab:manual_damage_cross_method} it is apparent that the methods agreed on 4 instances and each method was able to discover one instance that the other did not discover. Such results could be accepted for a practical application but we must not forget the fact that the collection is very small and only a single evidence is provided by the experiment, so it does not provide any statistical significance (getting statistically significant results would require more experiments, preferably  with various datasets, extraction tasks and human designers.) 

\clearpage

\section{Evaluation of Learned Rules} \label{sec:learning_eval}

In this section, we present two experiments with two manually annotated datasets. Both the datasets are of the kind of \emph{event extraction encoded as entity recognition} described in Section~\ref{sec:problems_event_entities}. The performance of our method based on ILP machine learning was measured and compared with an alternative extraction method based on propositional machine learning.

Evaluation of machine learning approaches is easier than evaluation of approaches based on manual extraction rules because it is not necessary to construct extraction rules for each experiment manually. Thanks to this fact and the availability of manually annotated datasets, the evaluation is more comprehensive than in the previous section.

\subsection{Examples of Learned Rules}

\begin{figure}
\begin{minted}[linenos,  fontsize=\footnotesize,
               frame=lines]{prolog}
% [cars - Rule 3] [Pos cover = 5 Neg cover = 0]
mention(cars,A) :-
   'lex.rf'(B,A), sempos(B,'n.denot'), tDependency(C,B), t_lemma(C,vozidlo), 
   functor(C,'ACT'), number(C,sg).   % vozidlo ~ vehicle

% [damage - Rule 1] [Pos cover = 14 Neg cover = 0]
mention(damage,A) :-
   'lex.rf'(B,A), sempos(B,'n.quant.def'), tDependency(C,B), tDependency(C,D), 
   t_lemma(D,'vyšetřovatel').   % vyšetřovatel ~ investigating officer

% [end_subtree - Rule 7] [Pos cover = 6 Neg cover = 0]
mention(end_subtree,A) :-
   'lex.rf'(B,A), sempos(B,'n.quant.def'), tDependency(C,B), t_lemma(C,'ukončit').
	    % ukončit ~ finish

% [start - Rule 2] [Pos cover = 15 Neg cover = 0]
mention(start,A) :-
   'lex.rf'(B,A), functor(B,'TWHEN'), tDependency(C,B), tDependency(C,D), 
   t_lemma(D,ohlásit).   % ohlásit ~ report (e.g. a fire)

% [injuries - Rule 1] [Pos cover = 7 Neg cover = 0]
mention(injuries,A) :-
   'lex.rf'(B,A), functor(B,'PAT'), tDependency(B,C), t_lemma(C,'zraněný'), 
   tDependency(D,B), aspect(D,cpl).   % zraněný ~ injured

% [fatalities - Rule 1] [Pos cover = 3 Neg cover = 0]
mention(fatalities,A) :-
   'lex.rf'(B,A), functor(B,'PAT'), tDependency(C,B), t_lemma(C,srazit).
	    % srazit ~ knock down

% [professional_unit - Rule 1] [Pos cover = 17 Neg cover = 0]
mention(professional_unit,A) :-
   'lex.rf'(B,A), functor(B,'LOC'), gender(B,fem), tDependency(C,B), 
   functor(C,'CONJ'), overlap_Lookup_tToken(D,B).

% [amateur_unit - Rule 1] [Pos cover = 19 Neg cover = 0]
mention(amateur_unit,A) :-
   'lex.rf'(B,A), tDependency(C,B), tDependency(D,C), tDependency(D,E), 
   t_lemma(E,dobrovolný).   % dobrovolný ~ voluntary
\end{minted}
	\caption[Rules with largest coverage for each task.]{Rules with largest coverage for each task learned from the whole dataset \sectiondoubleref{sec:data_fireman_annotated}.}
	\label{fig:learning_eval_rules}
\end{figure}

\begin{table}[p!]
\centering 
\begin{tabular}
{lcrclcrcl@{\hspace{0.1cm}}cc}

\multicolumn{11}{c}{Strict Precision}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
              cars &&      0.324 &  $\pm$  &       0.387 & &      0.380 &  $\pm$  &       0.249 &  \\
            damage &&      0.901 &  $\pm$  &       0.178 & &      0.860 &  $\pm$  &       0.176 &  \\
       end subtree &&      0.529 &  $\pm$  &       0.381 & &      0.499 &  $\pm$  &       0.242 &  \\
             start &&      0.929 &  $\pm$  &       0.109 & &      0.651 &  $\pm$  &       0.152 & $\bullet$ \\
          injuries &&      0.667 &  $\pm$  &       0.291 & &      0.398 &  $\pm$  &       0.205 & $\bullet$ \\
        fatalities &&      0.814 &  $\pm$  &       0.379 & &      0.307 &  $\pm$  &       0.390 & $\bullet$ \\
  professional unit &&      0.500 &  $\pm$  &       0.241 & &      0.677 &  $\pm$  &       0.138 & $\circ$ \\
      amateur unit &&      0.863 &  $\pm$  &       0.256 & &      0.546 &  $\pm$  &       0.293 & $\bullet$ \\
\hline
           overall &&      0.691 &  $\pm$  &       0.358 & &      0.540 &  $\pm$  &       0.297 & $\bullet$ \\
\hline
\\

\multicolumn{11}{c}{Strict Recall}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
              cars &&      0.088 &  $\pm$  &       0.129 & &      0.353 &  $\pm$  &       0.231 & $\circ$ \\
            damage &&      0.821 &  $\pm$  &       0.261 & &      0.933 &  $\pm$  &       0.148 & $\circ$ \\
       end subtree &&      0.231 &  $\pm$  &       0.203 & &      0.601 &  $\pm$  &       0.249 & $\circ$ \\
             start &&      0.908 &  $\pm$  &       0.115 & &      0.978 &  $\pm$  &       0.058 & $\circ$ \\
          injuries &&      0.574 &  $\pm$  &       0.309 & &      0.814 &  $\pm$  &       0.224 & $\circ$ \\
        fatalities &&      0.388 &  $\pm$  &       0.449 & &      0.536 &  $\pm$  &       0.452 & $\circ$ \\
  professional unit &&      0.506 &  $\pm$  &       0.191 & &      0.811 &  $\pm$  &       0.138 & $\circ$ \\
      amateur unit &&      0.886 &  $\pm$  &       0.210 & &      0.955 &  $\pm$  &       0.096 & $\circ$ \\
\hline
           overall &&      0.550 &  $\pm$  &       0.382 & &      0.748 &  $\pm$  &       0.312 & $\circ$ \\
\hline
\\

\multicolumn{11}{c}{Strict $F_1$}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
              cars &&      0.109 &  $\pm$  &       0.147 & &      0.335 &  $\pm$  &       0.205 & $\circ$ \\
            damage &&      0.828 &  $\pm$  &       0.217 & &      0.876 &  $\pm$  &       0.131 & $\circ$ \\
       end subtree &&      0.283 &  $\pm$  &       0.219 & &      0.525 &  $\pm$  &       0.213 & $\circ$ \\
             start &&      0.912 &  $\pm$  &       0.089 & &      0.771 &  $\pm$  &       0.111 & $\bullet$ \\
          injuries &&      0.543 &  $\pm$  &       0.280 & &      0.498 &  $\pm$  &       0.204 &  \\
        fatalities &&      0.306 &  $\pm$  &       0.420 & &      0.222 &  $\pm$  &       0.308 &  \\
  professional unit &&      0.491 &  $\pm$  &       0.200 & &      0.730 &  $\pm$  &       0.118 & $\circ$ \\
      amateur unit &&      0.827 &  $\pm$  &       0.253 & &      0.634 &  $\pm$  &       0.296 & $\bullet$ \\
\hline
           overall &&      0.537 &  $\pm$  &       0.369 & &      0.574 &  $\pm$  &       0.295 & $\circ$ \\
\hline
\\
\multicolumn{11}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular}

\caption{Evaluation on Czech Fireman dataset} \label{tab:learning_eval_fir_perfom}
\end{table}









In Figure~\ref{fig:learning_eval_rules}, we present the most representative examples of extraction rules learned from the whole dataset \sectiondoubleref{sec:data_fireman_annotated}. The rule with largest coverage for each extraction task was selected and it is provided in the figure. Each rule demonstrates a connection of the target token, annotated as `\emph{mention(task\_name)}’, with other parts of the sentence through linguistic syntax structures. 

For example the second rule (\emph{damage} task, lines 6-9) connects the node A with its tectogrammatical counterpart -- numeral B (\emph{n.quant.def}) and with node D (`vyšetřovatel') representing the investigating officer who stated the mount of damage. 



\subsection{Evaluation Methods and Measures}

Evaluation experiments have to be preformed repeatedly in order to investigate statistical significance. A dataset consisting of individual documents is randomly split into the training and testing part in each experiment run. The ratio between the training and testing part depends on the size of the dataset. It is important to have enough training data and therefore larger training part and smaller testing part is used if the dataset is too small. The cross validation technique meets exactly this requirement: the more cross validation folds, the greater the ratio. In following sections presenting individual experiments, different numbers of cross validation folds are used according to the sizes of the datasets.

In each experiment run, the performance is evaluated using several performance measures; precision, recall and $F_1$ (harmonic mean of precision and recall) are most often used measures for information extraction experiments. Partially correct (or overlapping) matches can occur when annotations do not match exactly (they are overlapping). Strict, lenient and average variants of performance measures allow dealing with partially correct matches in different ways:
\begin{itemize}
	\item     Strict measures considers all partially correct matches as incorrect (spurious).
	\item     Lenient measures considers all partially correct matches as correct.
	\item     Average measures takes the average of strict and lenient.
\end{itemize}

See also the GATE documentation chapter about performance evaluation\footurl{http://gate.ac.uk/userguide/chap:eval}
and few details about the statistical significance in Section~\ref{sec:conclusion_statsig}.



\subsection{Comparison with PAUM Classifier} \label{sec:learning_eval_PAUM}
To compare our solution with other alternatives, we took the PAUM (Perceptron Algorithm with Uneven Margins) propositional learner from GATE \citep{Li:Paum}. The quality of propositional learning from texts is strongly dependent on the selection of right features. We obtained quite good results with features of a window of two preceding and two following token lemmas and morphological tags. The precision was further improved by adding the feature of \emph{analytical function}\footurl{http://ufal.mff.cuni.cz/pdt2.0/doc/manuals/en/a-layer/html/ch03.html\#s1-list-anal-func} from the syntactic parser and information about presence of named entities.


%\subsection{Cross validation}
%We used the 10-fold cross validation in the evaluation. Thanks to this technique the evaluation is simple. After processing all the folds each document is processed with some of the ten learned models such that the particular document was not used in learning of that model, so all documents are unseen by the model applied on them. At the end we just compare the gold standard annotations with the learned ones in all documents.



\subsection{Czech Fireman Performance}



\begin{table}[b!]
\centering 
\begin{tabular}
{lcrclcrcl@{\hspace{0.1cm}}cc}

\multicolumn{11}{c}{Time Training}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
              cars &&     3:28.0 &  $\pm$  &      0:28.3 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
            damage &&     0:23.4 &  $\pm$  &      0:06.2 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
       end subtree &&     1:34.2 &  $\pm$  &      0:14.4 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
             start &&     0:28.9 &  $\pm$  &      0:03.1 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
          injuries &&     1:30.7 &  $\pm$  &      0:15.0 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
        fatalities &&     0:49.2 &  $\pm$  &      0:10.6 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
  professional unit &&     2:37.4 &  $\pm$  &      0:27.4 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
      amateur unit &&     0:24.0 &  $\pm$  &      0:04.6 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
\hline
           overall &&     1:24.5 &  $\pm$  &      1:05.8 & &     0:02.2 &  $\pm$  &      0:00.1 & $\bullet$ \\
\hline
\\

\multicolumn{11}{c}{Time Testing}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
              cars &&     0:00.9 &  $\pm$  &      0:00.2 & &     0:00.3 &  $\pm$  &      0:00.1 & $\bullet$ \\
            damage &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
       end subtree &&     0:00.9 &  $\pm$  &      0:00.2 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
             start &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
          injuries &&     0:00.9 &  $\pm$  &      0:00.2 & &     0:00.3 &  $\pm$  &      0:00.1 & $\bullet$ \\
        fatalities &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
  professional unit &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
      amateur unit &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
\hline
           overall &&     0:00.9 &  $\pm$  &      0:00.1 & &     0:00.3 &  $\pm$  &      0:00.0 & $\bullet$ \\
\hline
\\
\multicolumn{11}{c}{$\bullet$ statistically significant decrease}\\
\end{tabular}

\medskip
The time in this table is cumulative (sum over all folds in one experiment run).

\caption{Time spent by ML engines on the Czech Fireman dataset.} \label{tab:learning_eval_fir_time}
\end{table}





%\clearpage


Table~\ref{tab:learning_eval_fir_perfom} summarizes performance evaluation of our ILP based method and its comparison with PAUM classifier on the dataset \sectiondoubleref{sec:data_fireman_annotated}. The table shows results of the three main evaluation measures: strict precision, strict recall and strict $F_1$ for each extraction task as well as overall results. 8-fold cross validation was performed 8 times in the experiment. Average values and standard deviations are printed in the table and statistical significance is indicated. Root/subtree preprocessing/postprocessing (Section~\ref{sec:learning_root_subtree}) was performed in the first three tasks: `cars’, `damage’ and `end subtree’.

Although the precision of the ILP method was better in the majority of tasks and also its overall precision is statically better than the precision of the PAUM method, its recall was worse in all the measurements and also $F_1$ score is indicating better results of the PAUM method. 


Table~\ref{tab:learning_eval_fir_time} provides learning and application (testing) times of the compared methods. It is clear that learning time of the ILP method is many times higher than the time of the PAUM method. For example the `cars’ task took the ILP method approximately three and half minutes while the PAUM method spent only two seconds on it. On the other hand, the testing time of the ILP method is not that much higher than the time of the PAUM method 
%and once the extraction rules are done, the time requirements of the ILP method are acceptable.
but we have to notice that these times do not include linguistic preprocessing and deep language parsing needed by the ILP method will increase the time significantly.

Detailed results with much more evaluation measurements including lenient variants are available in the appendix -- Section~\ref{sec:complete_eval_fire}.

%\clearpage
%\subsection{Czech Fireman Time}


\subsection{Acquisitions Performance} \label{sec:learning_eval_acq}

\begin{table}
\centering 
\begin{tabular}
{lcrclcrcl@{\hspace{0.1cm}}cc}

\multicolumn{11}{c}{Strict Precision}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
            acqabr &&      0.457 &  $\pm$  &       0.034 & &      0.408 &  $\pm$  &       0.015 & $\bullet$ \\
          acquired &&      0.376 &  $\pm$  &       0.028 & &      0.441 &  $\pm$  &       0.027 & $\circ$ \\
            dlramt &&      0.286 &  $\pm$  &       0.052 & &      0.597 &  $\pm$  &       0.030 & $\circ$ \\
          purchabr &&      0.318 &  $\pm$  &       0.050 & &      0.390 &  $\pm$  &       0.015 & $\circ$ \\
         purchaser &&      0.403 &  $\pm$  &       0.037 & &      0.474 &  $\pm$  &       0.029 & $\circ$ \\
            seller &&      0.261 &  $\pm$  &       0.071 & &      0.245 &  $\pm$  &       0.041 &  \\
         sellerabr &&      0.306 &  $\pm$  &       0.111 & &      0.219 &  $\pm$  &       0.036 & $\bullet$ \\
\hline
           overall &&      0.344 &  $\pm$  &       0.088 & &      0.396 &  $\pm$  &       0.125 & $\circ$ \\
\hline
\\

\multicolumn{11}{c}{Strict Recall}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
            acqabr &&      0.180 &  $\pm$  &       0.025 & &      0.517 &  $\pm$  &       0.027 & $\circ$ \\
          acquired &&      0.272 &  $\pm$  &       0.054 & &      0.512 &  $\pm$  &       0.033 & $\circ$ \\
            dlramt &&      0.276 &  $\pm$  &       0.043 & &      0.740 &  $\pm$  &       0.063 & $\circ$ \\
          purchabr &&      0.120 &  $\pm$  &       0.041 & &      0.514 &  $\pm$  &       0.033 & $\circ$ \\
         purchaser &&      0.340 &  $\pm$  &       0.064 & &      0.556 &  $\pm$  &       0.028 & $\circ$ \\
            seller &&      0.131 &  $\pm$  &       0.047 & &      0.226 &  $\pm$  &       0.046 & $\circ$ \\
         sellerabr &&      0.050 &  $\pm$  &       0.027 & &      0.190 &  $\pm$  &       0.032 & $\circ$ \\
\hline
           overall &&      0.196 &  $\pm$  &       0.106 & &      0.465 &  $\pm$  &       0.184 & $\circ$ \\
\hline
\\

\multicolumn{11}{c}{Strict $F_1$}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
            acqabr &&      0.258 &  $\pm$  &       0.030 & &      0.456 &  $\pm$  &       0.015 & $\circ$ \\
          acquired &&      0.313 &  $\pm$  &       0.046 & &      0.473 &  $\pm$  &       0.024 & $\circ$ \\
            dlramt &&      0.280 &  $\pm$  &       0.046 & &      0.659 &  $\pm$  &       0.022 & $\circ$ \\
          purchabr &&      0.172 &  $\pm$  &       0.050 & &      0.443 &  $\pm$  &       0.016 & $\circ$ \\
         purchaser &&      0.367 &  $\pm$  &       0.051 & &      0.511 &  $\pm$  &       0.022 & $\circ$ \\
            seller &&      0.170 &  $\pm$  &       0.050 & &      0.232 &  $\pm$  &       0.036 & $\circ$ \\
         sellerabr &&      0.085 &  $\pm$  &       0.044 & &      0.202 &  $\pm$  &       0.030 & $\circ$ \\
\hline
           overall &&      0.235 &  $\pm$  &       0.101 & &      0.425 &  $\pm$  &       0.150 & $\circ$ \\
\hline
\\
\multicolumn{11}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular}

\caption{Evaluation on Acquisitions dataset} \label{tab:learning_acq_eval}
\end{table}




The second experiment made with our ILP based extraction method was performed on the dataset \sectiondoubleref{sec:data_corporate_acquisitions}. Table~\ref{tab:learning_acq_eval} provides the same measurements as in the previous experiment. This time, 2-fold cross validation was performed 10 times; root/subtree preprocessing/postprocessing was performed in the `dlramt’ task only and the technique of learning on named entity roots (see Section~\ref{sec:learning_ne_roots}) was used on the rest of the tasks.

On this dataset, the PAUM method performed significantly better than the ILP method. There are only three tasks (`acqabr’, `seller’ and `sellerabr’), where the ILP method achieved better precision than the PAUM method and only two of the measurements are statistically significant. 

\begin{table}[th!]
\centering 
\begin{tabular}
{lcrclcrcl@{\hspace{0.1cm}}cc}

\multicolumn{11}{c}{Time Training}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
            acqabr &&    16:24.9 &  $\pm$  &      8:22.9 & &     0:10.0 &  $\pm$  &      0:00.6 & $\bullet$ \\
          acquired &&    10:16.0 &  $\pm$  &      7:11.1 & &     0:09.8 &  $\pm$  &      0:00.4 & $\bullet$ \\
            dlramt &&    23:27.6 &  $\pm$  &     15:45.7 & &     0:10.8 &  $\pm$  &      0:02.8 & $\bullet$ \\
          purchabr &&    16:51.6 &  $\pm$  &      9:14.7 & &     0:10.2 &  $\pm$  &      0:01.4 & $\bullet$ \\
         purchaser &&    12:27.1 &  $\pm$  &      7:22.6 & &     0:10.4 &  $\pm$  &      0:01.2 & $\bullet$ \\
            seller &&     9:12.2 &  $\pm$  &      3:55.2 & &     0:10.1 &  $\pm$  &      0:01.0 & $\bullet$ \\
         sellerabr &&     6:11.5 &  $\pm$  &      4:03.3 & &     0:09.7 &  $\pm$  &      0:00.3 & $\bullet$ \\
\hline
           overall &&    13:33.0 &  $\pm$  &     10:00.1 & &     0:10.1 &  $\pm$  &      0:01.3 & $\bullet$ \\
\hline
\\

\multicolumn{11}{c}{Time Testing}\\
\hline
Task && \multicolumn{3}{c}{ILP}  && \multicolumn{3}{c}{PAUM} && \\
\hline
            acqabr &&     0:27.4 &  $\pm$  &      0:00.8 & &     0:13.6 &  $\pm$  &      0:00.3 & $\bullet$ \\
          acquired &&     0:26.7 &  $\pm$  &      0:00.7 & &     0:11.8 &  $\pm$  &      0:00.3 & $\bullet$ \\
            dlramt &&     0:30.5 &  $\pm$  &      0:00.8 & &     0:09.8 &  $\pm$  &      0:00.5 & $\bullet$ \\
          purchabr &&     0:27.4 &  $\pm$  &      0:00.9 & &     0:13.2 &  $\pm$  &      0:00.5 & $\bullet$ \\
         purchaser &&     0:27.5 &  $\pm$  &      0:02.0 & &     0:11.5 &  $\pm$  &      0:01.2 & $\bullet$ \\
            seller &&     0:27.3 &  $\pm$  &      0:03.2 & &     0:10.7 &  $\pm$  &      0:00.8 & $\bullet$ \\
         sellerabr &&     0:26.1 &  $\pm$  &      0:00.7 & &     0:11.5 &  $\pm$  &      0:00.3 & $\bullet$ \\
\hline
           overall &&     0:27.6 &  $\pm$  &      0:02.0 & &     0:11.7 &  $\pm$  &      0:01.4 & $\bullet$ \\
\hline
\\
\multicolumn{11}{c}{$\bullet$ statistically significant decrease}\\
\end{tabular}

\caption{Time spent by ML engines on the Acquisitions dataset.} \label{tab:learning_acq_time}
\end{table}

Table~\ref{tab:learning_acq_time} provides learning and application (or testing) times on this dataset. Again, learning times of the ILP method are several times higher than the times of the PAUM method and there is not so big difference between application times, although the same notice about the necessity of deep language parsing in the preprocessing step pays also in this case. 

An interesting observation can be made when comparing training times of the ILP method on tasks, where the technique of learning on named entity roots was performed, with the remaining task `dlramt’. Table~\ref{tab:data_acquisitions} presents numbers of annotations per extraction task and it can be seen that the training time quite correlate with the number of annotations. But the task `dlramt’ has almost the least instances form the tasks used in the evaluation and its average training time is about two times higher than the time spent on other tasks. From there, it can be seen that the technique of learning on named entity roots can save training time. 

Detailed results with much more evaluation measurements including lenient variants are available in the appendix -- Section~\ref{sec:complete_eval_acq}.

\pagebreak
\subsection{Comparison with Results Reported in Literature} \label{sec:learning_eval_acq_third}

Other researchers used the Acquisitions dataset and reported the performance of their information extraction systems in the literature. Table~\ref{tab:learning_acq_eval_third} provides comparison of four extraction systems 
(SRV \citep{Freitag:1999_phd},
HMM \citep{Freitag1999InformationExtraction},
Elie \citep{DBLP:conf/ecml/FinnK04} and 
SVM+ILP \citep{DBLP:conf/ilp/RamakrishnanJBS07};
short description of these systems was provided in Section~\ref{sec:relwork_directly_comparable}) with our results obtained in the experiment described in the previous section. 
Table~\ref{tab:learning_acq_eval_third} compares values of $F_1$ measure (displayed as percentage, 100 times higher values than in previous tables) for individual tasks and it also provides overall weighted average for each method obtained by following formula:
$$
weighted\ overall\ F_1 = \frac{\sum_{t\in Tasks}{A_t {F_1}_t}}{\sum_{t\in Tasks}A_t}
$$
\noindent Where $A_t$ represents the number of annotations of the particular extraction task and ${F_1}_t$ represents the actual value of $F_1$ measure on the extraction task.

Unfortunately not all the results can be directly compared because experiments of \cite{DBLP:conf/ilp/RamakrishnanJBS07} (SVM+ILP) were performed on a different version of the dataset. First two columns of Table~\ref{tab:learning_acq_eval_third} show the difference of the two versions of the dataset in terms of numbers of annotations per extraction task. Columns corresponding to the second version (ver. B) of the dataset have gray background for better distinction. 
Although not all the numbers are directly comparable, we still compare them all together because, for the majority of the tasks (except `seller'), the difference of the datasets is very small. Maximum values are highlighted in bold and minimum in italic.

Very interesting observation relates to the results of the PAUM method, which performed the best on most tasks, which is quite surprising because the method is relatively simple and no special learning features were designed for it. On the other hand, our ILP based method (the one before the last column of the table) did not demonstrate any major improvement, although a few lower values can be found in the table (`seller' Elie and  `acquired' HMM). 

The reason why we computed the overall $F_1$ measure as the weighted average is that \cite{DBLP:conf/ilp/RamakrishnanJBS07} did so. Their paper provides also the overall $F_1$ measure for a plain ILP variant of their method, which is very close to our ILP based method. They provide following overall values of the $F_1$ measure:
	
	39.8 $\pm$ 0.9 for the original SVM + ILP approach and
	
	35.2 $\pm$ 1.5 for the plain ILP variant.

\noindent If we compare the first value with the one reported in Table~\ref{tab:learning_acq_eval_third} (Overall value in the SVM+ILP column) it is clear that they are not the same\footnote{Their values were probably collected during each experiment run based on the actual performance and annotation counts.}, but they are quite similar and from there we can see that also their plain ILP approach was more successful than ours (35.2 vs. 23.9 $F_1$ measure), although on different version of the dataset.


\begin{table}
	\centering
\begin{tabular}{l|r>{\columncolor{lightgray}}r|ccc>{\columncolor{lightgray}}ccc}
\hline%
& \multicolumn{2}{c|}{Annotations} & \multicolumn{6}{c}{Extraction Method}\\
Task 				 &  ver. A 				 &  ver. B 				 &  SRV  					 &  HMM 				  &  Elie				  &  SVM+ILP			  &  ILP				  &  PAUM\\\hline%\\
acquired		 &  \textbf{683} 	 &  651 					 &  38.5 					 &  \emph{30.9}	  &  43.5				  &  41.8					  &  31.3				  & \textbf{47.3}\\
acqabr   		 &  \textbf{1450}  &  1494					 &  38.1 					 &  40.1 				  &  39.7				  &  42.6					  &  \emph{25.8}  & \textbf{45.6}\\
purchaser 	 &  \textbf{624} 	 &  594 					 &  45.1 					 &  48.1				  &  46.2				  &  45.4					  &  \emph{36.7}  & \textbf{51.1}\\
purchabr 		 &  1263 					 &  \textbf{1347}  &  \textbf{48.5}	 &  n/a 				  &  28.7				  &  35.4					  &  \emph{17.2}  & 44.3\\
seller 			 &  267 					 &  \textbf{707} 	 &  23.4 					 &  n/a 				  &  \emph{15.6}  &  \textbf{51.5}  &  17.0				  & 23.2\\
sellerabr 	 &  431 					 &  \textbf{458} 	 &  \textbf{25.1}  &  n/a 				  &  13.4 			  &  21.7 				  &  \emph{8.5}	  & 20.2\\
dlramt 			 &  \textbf{283} 	 &  206 					 &  61.8 					 &  55.3 				  &  59.0				  &  53.0 				  &  \emph{28.0}  & \textbf{65.9}\\\hline%\\
Total/Overall&  5001 					 &  \textbf{5457}	 &  41.1 					 &  n/a				 	  &  33.5				  &  40.8					  &  \emph{23.9}  & \textbf{44.0}\\
\hline%
\end{tabular}
\caption{Performance comparison of reported results, $F_1$ measure.}\label{tab:learning_acq_eval_third}
\end{table}
%\clearpage

%\subsection{Acquisitions Time}


%\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation of Shareable Extraction Ontologies}  \label{sec:onto_experiment}

In this section we present an experiment that should serve as a proof of a concept that the proposed idea of independent extraction ontologies is realizable. We have selected several reasoners (namely Jena, HermiT, Pellet and FaCT++) and tested them on two slightly different datasets from two different domains and languages (see Table~\ref{tab:datasets}). This should at least partially demonstrate the universality of the proposed approach.

In both cases the task is to find all instances (corresponding to words in a document) that should be uncovered by the extraction rules. The extraction rules are saved in single extraction ontology for each dataset. The datasets are divided into individual document ontologies (owl files) corresponding to the individual documents. During the experiment, the individual document ontologies are processed separately (one ontology in a step) by a selected reasoner. The total time taken to process all document ontologies of a dataset is the measured result of the reasoner for the dataset.

The actual reasoning tasks are more difficult than a simple retrieval of all facts entailed by the extraction rules. Such simple retrieval task took only a few seconds for the Acquisitions dataset (including parsing) in the native Prolog environment that the IE engine uses. There were several more inferences needed in the reasoning tasks because the schema of the input files was a little bit different from the schema used in rules. The mapping of the schemas was captured in another ``mapping'' ontology that was included in the reasoning. The mapping ontology is a part of the publically available project ontologies, see Section~\ref{sec:download_notes} for avalability details.
%\footnote{See ``Data/ontologies'' link on the project page \url{http://czsem.berlios.de/}}%and a potentially interested reader can find the complete mapping ontology on the project's web site.


\subsection{Datasets} \label{sec:onto_datasets}

In the experiment, we used two slightly different datasets from two different domains and languages.  Table~\ref{tab:datasets} summarizes some basic information about them. The fist dataset is called Czech Fireman, it is based on Czech texts that are reporting on fire and traffic accidents and it was already described in Section~\ref{sec:data_rdf_fireman}. The second dataset is called Corporate Acquisition Events, it is based on news articles describing acquisition events, taken from the Reuters dataset and it was already described in Section~\ref{sec:data_rdf_acquisitions}.

\begin{table}
\begin{center}
\begin{tabular}{|r||l|l|b{20mm}|b{20mm}|b{20mm}|}
%\begin{tabular}{|r||r|r|c|r|c|}
\hline
dataset & domain & language & number of~files &  dataset size (MB) &  number of~rules  \\
\hline
\hline
\textbf{Czech Fireman} & accidents & Czech &  50 &  16 &  2\\
\hline
\textbf{Acquisitions} & finance & English &  600 &  126 &  113\\
\hline
\end{tabular}
\caption{Description of datasets that were used.}\label{tab:datasets}
\end{center}
\end{table}




\subsection{Reasoners} \label{sec:onto_reasoners}

%In the experiment we used four OWL reasoners
Four OWL reasoners were used in the experiment (namely
Jena\footnote{\url{http://jena.sourceforge.net}}
,HermiT\footnote{\url{http://hermit-reasoner.com}}
,Pellet\footnote{\url{http://clarkparsia.com/pellet}}
and FaCT++\footnote{\url{http://code.google.com/p/factplusplus}})
and the time they spent on processing a particular dataset was measured. The time also includes time spent on parsing the input. HermiT, Pellet and FaCT++ were called through OWL API-3.1, so the same parser was used for them. Jena reasoner was used in its native environment with the Jena parser.

In the early beginning of the experiment we had to exclude the FaCT++ reasoner from both tests. It turned out that FaCT++ does not work with rules\footnote{\url{http://en.wikipedia.org/wiki/Semantic_reasoner#Reasoner_comparison}} and it did not return any result instances.  All the remaining reasoners strictly agreed on the results and returned the same sets of instances.

Also HermiT was not fully evaluated on the Acquisitions dataset because it was too slow. The reasoner spent 13 hours of running to process only 30 of 600 files of the dataset. And it did not seem useful to let it continue. We contacted the authors of HermiT to clarify this issue and we were reminded that HermiT was mainly designed as a TBox reasoner and it was particularly optimized for the ontology classification problem \citep{ghms10classification}; it was never optimized for simple rule based (ABox) inferences.












\subsection{Evaluation Results}



\begin{table}
\begin{center}
\begin{tabular}{|r||r|r||r|r|}
\hline
reasoner & \textbf{czech\_fireman} & stdev & \textbf{acquisitions-v1.1} & stdev\\
\hline
\hline
\textbf{Jena} & 161 s & 0.226 & 1259 s & 3.579\\
\hline
\textbf{HermiT} & 219 s & 1.636 & $\gg$ 13 hours & \\
\hline
\textbf{Pellet} & 11 s & 0.062 & 503 s & 4.145\\
\hline
\textbf{FaCT++} & \multicolumn{4}{|c|}{Does not support rules.}\\
\hline
\end{tabular}
\end{center}

Time is measured in seconds. Average values from 6 measurements. Experiment environment: Intel Core I7-920 CPU 2.67GHz, 3GB of RAM, Java SE 1.6.0\_03, Windows XP.

\caption{Time performance of tested reasoners on both datasets.}
\label{tab:results}
\end{table}

Table~\ref{tab:results} summarizes results of the experiment. The standard deviations are relatively small when compared to the differences between the average times.  So there is no doubt about the order of the tested reasoners. Pellet performed the best and HermiT was the slowest amongst the tested and usable reasoners in this experiment.

From the results we can conclude that similar tasks can be satisfactorily solved by contemporary OWL reasoners because three of four tested reasoners were working correctly and two reasoners finished in bearable time.

On the other hand even the fastest system took 8.5 minutes to process 113 rules over 126MB of data. This is clearly   significantly longer than a operational system would require. 
Contemporary Semantic Web reasoners are known still to be often quite inefficient and the experiment showed that using them today to do information extraction will result in quite poor performance. However, efficiency problems can be solved
and in the context of Linked Data providing shareable descriptions of information extraction rules may be valuable.










\clearpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation of Fuzzy ILP Classification} \label{sec:fuzzy_eval}
\graphicspath{{../img/ch80/}}

In this section, evaluation experiments with the Fuzzy ILP Classifier will be presented. Two main experiments were performed, one with the Czech Fireman dataset and one with several UCI datasets.

We have evaluated both ILP methods and compared them with other machine learning procedures used in data mining. To make the comparison clear and easy to perform, we have implemented an interface between the ILP methods and Weka (see some details about Weka in Section~\ref{sec:third_weka}). This interface makes it possible to use the ILP methods as an ordinary Weka classifier for any\footnote{For the fuzzy ILP method, there is a requirement on the target (class) attribute: it has to be monotonizable (e.g. numeric).} classification task inside the Weka software. This also makes the presented experiments easily repeatable.% (see Secrion~\ref{sec:conclusion_repeatablity} for details.) 

%\pagebreak

\subsection{Czech Fireman Performance} \label{sec:fuzzy_eval_fireman}


For our experiment we used the Weka Experimenter (see in Section~\ref{sec:third_weka}) and performed an experiment in which the Crisp and Fuzzy ILP classifiers were compared with five additional classifiers:
\begin{itemize}
	\item Multilayer Perceptron \citep{biblio:bishop-1995},
	\item Support Vector Machine classifier SMO \citep{biblio:SMO},
	\item J48 decision tree \citep{biblio:J48},
	\item JRip rules \citep{weka:JRip} and
	\item Additive logistic regression LogitBoost \citep{biblio:LogitBoost}.
\end{itemize}
We have evaluated all the methods two times by 10-fold cross validation. %(section~\ref{sec:experiment_desc}).
The obtained results (average values) are described by the graph in Figure~\ref{fig:graph2x10} and in Table~\ref{tab:table2x10} (with standard deviations and marked statistically significant values).

There is no clear winner in our experiment. But the Fuzzy ILP classifier proved better results than a majority of the methods on our data and the results are statistically significant in many cases. Very good results were also obtained using LogitBoost. 

\begin{figure}
\centerline{\includegraphics[width=\hsize]{2x10cross}}
\caption[Evaluation of the methods on the Fireman dataset.]{Evaluation of the methods on the Fireman dataset -- average values.}
\label{fig:graph2x10}
\end{figure}


\begin{table}
%\scriptsize
{\centering \begin{tabular}{lr@{\hspace{0cm}}c@{\hspace{0cm}}rr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}c}
\hline
& \multicolumn{3}{c}{Fuzzy}& \multicolumn{4}{c}{Crisp} & \multicolumn{4}{c}{MultPerc} & \multicolumn{4}{c}{SMO} & \multicolumn{4}{c}{J48} & \multicolumn{4}{c}{JRip} & \multicolumn{4}{c}{LBoost} \\
\hline
Corr	& 0.61 & $\pm$ & .19 & .22 & $\pm$ & .17 & $\bullet$ & .41 & $\pm$ & .19 & $\bullet$ & .36 & $\pm$ & .24 & $\bullet$ & .41 & $\pm$ & .22 & $\bullet$ & .44 & $\pm$ & .17 & $\bullet$ & .59 & $\pm$ & .26 &        \\
Incor	&  .39 & $\pm$ & .19 & .27 & $\pm$ & .24 &         	 & .59 & $\pm$ & .19 & $\circ$ 	 & .64 & $\pm$ & .24 & $\circ$ 	 & .59 & $\pm$ & .22 & $\circ$ 	 & .56 & $\pm$ & .17 & $\circ$ 	 & .41 & $\pm$ & .26 &        \\
Uncl	&  .00 & $\pm$ & .00 & .51 & $\pm$ & .29 & $\circ$   & .00 & $\pm$ & .00 &         	 & .00 & $\pm$ & .00 &         	 & .00 & $\pm$ & .00 &         	 & .00 & $\pm$ & .00 &         	 & .00 & $\pm$ & .00 &        \\
Prec	&  .56 & $\pm$ & .24 & .53 & $\pm$ & .37 &         	 & .35 & $\pm$ & .20 & $\bullet$ & .33 & $\pm$ & .26 &         	 & .39 & $\pm$ & .22 &         	 & .34 & $\pm$ & .21 & $\bullet$ & .56 & $\pm$ & .28 &        \\
Rec		&  .61 & $\pm$ & .19 & .49 & $\pm$ & .32 &         	 & .41 & $\pm$ & .19 & $\bullet$ & .36 & $\pm$ & .24 & $\bullet$ & .41 & $\pm$ & .22 & $\bullet$ & .44 & $\pm$ & .17 & $\bullet$ & .59 & $\pm$ & .26 &        \\
F			&  .56 & $\pm$ & .20 & .49 & $\pm$ & .33 &         	 & .36 & $\pm$ & .19 & $\bullet$ & .32 & $\pm$ & .24 & $\bullet$ & .39 & $\pm$ & .21 &         	 & .36 & $\pm$ & .19 & $\bullet$ & .56 & $\pm$ & .27 &        \\
\hline
\multicolumn{21}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \par}
\footnotesize
\smallskip
Legend:\\
{\centering
\begin{tabular}{p{2cm}@{}p{10.5cm}}\\
Fuzzy \dotfill{}& czsem.ILP.FuzzyILPClassifier '' \\
Crisp \dotfill{} & czsem.ILP.CrispILPClassifier '' \\
MultPerc \dotfill{} & functions.MultilayerPerceptron '-L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a' \\
SMO \dotfill{} & functions.SMO '-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -C 250007 -E 1.0\textbackslash"' \\
J48 \dotfill{} & trees.J48 '-C 0.25 -M 2' \\
JRip \dotfill{} & rules.JRip '-F 3 -N 2.0 -O 2 -S 1' \\
LBoost \dotfill{} & meta.LogitBoost '-P 100 -F 0 -R 1 -L -1.7976931348623157E308 -H 0.1 -S 1 -I 10 -W trees.DecisionStump' \\
\\
Corr \dotfill{} & Percent correct\\
Inor \dotfill{} & Percent incorrect\\
Uncl \dotfill{} & Percent unclassified\\
Prec \dotfill{} & IR precision, weighted average from all classes\\
Rec \dotfill{} 	& IR recall, weighted average from all classes\\
F \dotfill{} 		& F measure, weighted average from all classes\\
\end{tabular}
}
\caption[Evaluation of the methods on the Fireman dataset.]{Evaluation of the methods on the Fireman dataset in 2 times 10-fold cross validation.}
\label{tab:table2x10}
\end{table}





\clearpage
\subsection{UCI Performance} \label{sec:fuzzy_eval_uci}


\begin{table}
\footnotesize
{\centering \begin{tabular}{l|r@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.3cm}}r@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}l@{\hspace{0cm}}r@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.05cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0cm}}c@{\hspace{0.25cm}}|c@{\hspace{0.20cm}}c@{\hspace{0.10cm}}}
dataset & \multicolumn{3}{c}{Fuzzy}& \multicolumn{4}{c}{Crisp} & \multicolumn{4}{c}{MultPerc} & \multicolumn{4}{c}{SMO} & \multicolumn{4}{c}{J48} & \multicolumn{4}{c}{JRip} & \multicolumn{4}{c|}{LBoost} & train & test\\
\hline
car & .39 & $\pm$ & .03 & .36 & $\pm$ & .03 & $\bullet$ & .53 & $\pm$ & .02 & $\circ$ & .57 & $\pm$ & .01 & $\circ$ & .50 & $\pm$ & .02 & $\circ$ & .51 & $\pm$ & .03 & $\circ$ & .54 & $\pm$ & .02 & $\circ$ & 173 & 1554\\
\hline
wine & .44 & $\pm$ & .03 & .42 & $\pm$ & .02 & $\bullet$ & .48 & $\pm$ & .02 & $\circ$ & .46 & $\pm$ & .02 & $\circ$ & .47 & $\pm$ & .02 & $\circ$ & .48 & $\pm$ & .03 & $\circ$ & .52 & $\pm$ & .02 & $\circ$ & 160 & 1439\\
\hline
cmc & .79 & $\pm$ & .02 & .77 & $\pm$ & .03 & $\bullet$ & .89 & $\pm$ & .02 & $\circ$ & .81 & $\pm$ & .01 & $\circ$ & .88 & $\pm$ & .02 & $\circ$ & .82 & $\pm$ & .03 & $\circ$ & .85 & $\pm$ & .02 & $\circ$ & 147 & 1325\\
\hline
tae & .50 & $\pm$ & .12 & .39 & $\pm$ & .11 & $\bullet$ & .59 & $\pm$ & .11 & $\circ$ & .55 & $\pm$ & .12 & $\circ$ & .50 & $\pm$ & .12 &  & .37 & $\pm$ & .11 & $\bullet$ & .55 & $\pm$ & .11 & $\circ$ & 135 & 15\\
\hline
pop & .66 & $\pm$ & .09 & .54 & $\pm$ & .17 & $\bullet$ & .57 & $\pm$ & .13 & $\bullet$ & .70 & $\pm$ & .06 & $\circ$ & .70 & $\pm$ & .07 & $\circ$ & .70 & $\pm$ & .06 & $\circ$ & .66 & $\pm$ & .10 &  & 80 & 9\\
\hline
nurs & .79 & $\pm$ & .04 & .68 & $\pm$ & .06 & $\bullet$ & .81 & $\pm$ & .04 & $\circ$ & .73 & $\pm$ & .04 & $\bullet$ & .80 & $\pm$ & .04 & $\circ$ & .79 & $\pm$ & .06 &  & .83 & $\pm$ & .02 & $\circ$ & 52 & 12907\\
\hline
\multicolumn{23}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \par}
\footnotesize
\smallskip
Legend:\\
{\centering
\begin{tabular}{p{2cm}@{}p{10.5cm}}\\
train \dotfill{} & average number ($\pm$1) of training instances in each run\\
test \dotfill{} & average number ($\pm$1) of testing instances in each run\\
\end{tabular}
}


\smallskip


Learning parameters for both ILP methods:\\set(noise,20). set(i,3). set(clauselength,13). set(search,heuristic). set(evalfn,wracc). set(samplesize,3).
\caption[Evaluation of the methods on UCI datasets.]{Evaluation of the methods on UCI datasets, \textbf{percent correct}, average values from 100 repetitions.}
\label{tab:UCItable}
\end{table}




The Fuzzy ILP Classifier performed quite well on our dataset, but the next question is: How is it with other data, with more learning instances, and what about the time complexity? To answer these questions we performed another experiment. We selected several datasets from the UCI repository (see Section~\ref{sec:data_uci_datasets}) and evaluated all the methods against them. 



%The list of selected datasets can be found in the legend of Table~\ref{tab:UCItable}. 
All the selected datasets are monotonizable (the target attribute can be naturally ordered), so the fuzzy classifier could take advantage of that. Learning settings are the same as before (Table~\ref{tab:table2x10}) except for settings of both ILP classifiers, which  performed a little bit better with modified settings on a majority of the datasets (see in the legend of Table~\ref{tab:UCItable}). 

Table~\ref{tab:UCItable} compares the numbers of correctly classified instances on all the datasets. The last two columns show numbers of training and testing instances. The numbers of training instances are quite low; this is because the ILP classifiers are probably not capable of fully exploiting higher numbers of training instances and the difference between ILP classifiers and the others would be even a bit higher. This is demonstrated in Figure~\ref{fig:corect_growing_learninig_instances} (for the `nursery' dataset only). It can be seen that when the number of training instances was under about 40, the fuzzy classifier performed better than some of the others (SMO, JRip and Multilayer Perceptron), but from about 60 training instances further, both ILP classifiers performed worse than the others.


\begin{figure}[p]
\centerline{\includegraphics[width=0.85\hsize]{corect_growing_learninig_instances}}
\caption[The impact of dataset size on classification performance.]{The impact of dataset size on classification performance,\\x-axis: number of training instances, \\y-axis: percent of correctly classified instances, average values from 10 repetitions, `nursery' dataset.}
\label{fig:corect_growing_learninig_instances}
\end{figure}


\subsection{UCI Time} \label{sec:fuzzy_eval_uci_time}


Figure~\ref{fig:learning_speed} demonstrates time complexity of the classifiers in the same experiment as in Figure~\ref{fig:corect_growing_learninig_instances}. Despite the fact that the Fuzzy ILP Classifier was several times slower than the Crisp ILP Classifier and even more than the others, it is still computable on current processors (e.g. P9600, 2.66 GHz, which we used) and the curve of time complexity did not grow rapidly during the experiment. Because ILP is a heuristic and iterative method, the time complexity can be quite directly managed by the setting of learning parameters.

\begin{figure}[p]
\centerline{\includegraphics[width=\hsize]{learning_speed}}
\caption[The impact of dataset size on classification time.]{The impact of dataset size on classification time,\\x-axis: number of training instances, \\y-axis: training time in seconds, average values from 10 repetitions, `nursery' dataset.}
\label{fig:learning_speed}
\end{figure}


