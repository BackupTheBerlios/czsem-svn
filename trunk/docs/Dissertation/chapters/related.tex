\chapter{Related Work} \label{sec:ch_related}

This chapter shortly introduces mostly related work of other researchers. The chapter is split to three main sections. Section~\ref{sec:relwork_ie} is dedicated to information extraction approaches, Section~\ref{sec:relwork_ext_ont} to extraction ontologies and Section~\ref{sec:relwork_doc_classification} to document classification.

\section{Information Extraction Approaches} \label{sec:relwork_ie}

\subsection{Deep Linguistic Parsing and Information Extraction}

\begin{quotation}
The choice of the actual learning algorithm depends on the type of structural information available. For example, deep syntactic information provided by current parsers for new types of corpora such as biomedical text is seldom reliable, since most parsers have been trained on different types of narrative. If reliable syntactic information is lacking, sequences of words around and between the two entities can be used as alternative useful discriminators. \citep{Bunescu:phd}
\end{quotation}
Since that time, the situation has improved and deep linguistic parsing is often used even for biomedical texts (because new biomedical corpora are available for retraining of the parsers; see e.g. in \citep{Buyko:2010:EIA:1870658.1870754}) and  dependency graphs constitute the fundamental data structure for syntactic structuring and subsequent knowledge extraction from natural language documents in many contemporary approaches to information extraction (see details about individual information extraction systems in Section~\ref{sec:relwork_deep_IE_systems}). 

Currently, quite a lot of parsers can be used for generation of syntactic dependency structures from natural language texts. Some of them will be briefly described in Section~\ref{sec:third_ling_tools}. Besides the possibility of using different parsers there are also various language dependency representations (LDR) such as the Stanford \citep{stanfordDeps} and CoNLL-X \citep{johansson2007a} dependencies and the Functional Generative Description (FGD) \citep{SgallHajicovaPanevova1986} studied mainly in Prague. All the dependency representations are very similar from the structural point of view; they differ mainly in the number of different dependency kinds they offer. FGD provides also additional node attributes and so called layered approach (see in Section~\ref{sec:third_pdt}) and is the only representation available for Czech.


It is also worthy to investigate the impact of usage of different LDRs on the performance of information extraction. \cite{Buyko:2010:EIA:1870658.1870754} compared the impact of different LDRs and parsers in their IE system, namely Stanford and CoNLL-X dependencies and usage of different trimming operations. Their findings are definitely not supporting one choice against another because one representation was better for some tasks and another one for other tasks, see examples in their paper. More important seems to be the quality of used parser.



\subsection{IE Systems Based on Deep Language Parsing} \label{sec:relwork_deep_IE_systems}


We describe in this section several information extraction systems based on deep language parsing. The systems differ greatly in the manner of using LDR.

\subsubsection{Rule Based Systems}

There are many systems using hand crafted extraction rules based on LDR. These systems need assistance from a human expert that is able to design the extraction rules manually. The advantage of these systems is that there is no need of learning (or training) data collection. For example \cite{RelEx} used a simple set of rules and the Stanford parser\footurl{http://nlp.stanford.edu/software/lex-parser.shtml} for biomedical relation extraction. Shallow and deep parsers were used by \cite{Yakushiji2001} in combination with mapping rules from linguistic expressions to biomedical events.


\subsubsection{Classical Machine Learning Systems}

Classical machine learning (ML) approaches rely on the existence of a learning collection. They usually use LDR just for construction of learning features for propositional learners like decision trees, neural networks, SVM, etc. Learning features are selected manually when the system is being adapted to new domain or extraction task. For example in \citep{Bunescu:DependencyPaths}, learning features based on so called dependency paths constructed from LDR are used for relation extraction. Similar approach was used in \citep{Buyko:dependencyGraphs} for biomedical event extraction.


\citep{Yaoyong09a} described machine learning facilities available in GATE. This approach is an example of classical adaptation of propositional learners for information extraction, but it is not necessarily connected with LDR. GATE itself does not provide any prebuilt functions for working with LDR but they can be added as they were in our case (see in Section~\ref{sec:LDR_in_GATE}). 
%This is also the software component (GATE Machine Learning PR) to that we have compared our solution. 
%Our solution is also based on GATE (See next sections.)


\subsubsection{Inductive Systems}

There are also systems using some inductive technique e.g. Inductive Logic Programming (ILP, see also Section~\ref{sec:third_ILP}) to induce learning features or extraction rules automatically from learning collection. In these cases it is neither necessary to manually design extraction rules nor select the right learning features. For example \cite{DBLP:conf/ilp/RamakrishnanJBS07} used the dependency parser MINIPAR\footurl{http://webdocs.cs.ualberta.ca/~lindek/minipar.htm} \citep{minipar} and ILP for construction of both:
\begin{enumerate}
	\item learning features for SVM classifier and
	\item plain extraction rules.
\end{enumerate}
They compared the success of the two approaches and discovered that the extraction model constructed by SVM (based on the induced learning features) was more successful than the plain extraction rules directly constructed by ILP.


\subsubsection{Similarity Based Systems}

Several information extraction related systems do some kind of similarity search based on the syntactic structure. For example
%[Banko: Open Information Extraction from the Web]
\cite{Etzioni08informationExtraction} designed a data-driven extraction system performing a single-pass-over-a-corpus extraction of a large
set of relational tuples without requiring any human input (e.g. without the definition of extraction tasks).  And for example 
%[Wang: Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons]
\cite{Wang:SimilarityTreeSkeletons} used syntactic structure similarity for textual entailment.

%\subsubsection{Biomedical Domain}

%The
%Stanford format is widely used in the biomedical
%domain (e.g., by \cite{MiyaoACL2008}, (\cite{Yakushiji2001}) or
%\cite{Clegg2005Evaluating}).



\subsection{Inductive Logic Programming and Information Extraction}

There are many users of ILP in the linguistic and information extraction area.
For example \cite{stasinos:phd} used ILP for shallow parsing and phonotactics.
\cite{Junker99learningfor} summarized some basic principles of using ILP for learning from text without any linguistic preprocessing. One of the most related approaches to ours was described by \cite{aitken02:_learn_infor_extrac_rules}. The authors use ILP for extraction of information about chemical compounds and other concepts related to global warming and they try to express the extracted information in terms of ontology. They use only the part of speech analysis and named entity recognition in the preprocessing step. But their inductive procedure uses also additional domain knowledge for the extraction. \cite{DBLP:conf/ilp/RamakrishnanJBS07} used ILP to construct good features for propositional learners like SVM to do information extraction. It was discovered that this approach is a little bit more successful than a direct use of ILP but it is also more complicated. The later two approaches could be also employed in our solution.


\subsection{Directly Comparable Systems} \label{sec:relwork_directly_comparable}

Thanks to the fact that evaluation of our extraction method based on ILP was performed also on a commonly used dataset, its performance can be compared with other information extraction systems that were evaluated on the dataset as well. Details about the results of the comparison will be presented in Section~\ref{sec:learning_eval_acq_third}. Brief introduction of the directly comparable systems will be present in this section.

\subsubsection{PAUM}

The PAUM (Perceptron Algorithm with Uneven Margins) algorithm \citep{Li:Paum} is one of machine learning alternatives provided by GATE. The algorithm represents a slight modification of the classical Perceptron \citep{rosenblatt1957perceptron} used in neural networks and extended by SVM \citep{springerlink:10.1007/BF00994018}. PAUM belongs to the category of classical propositional learners working on a set of learning features manually extracted from text.

Thanks to the easy accessibility of PAUM in GATE, all our machine learning experiments could be performed with PAUM in the same time as our extraction method based on ILP. Hence these two methods were directly compared with absolutely equal conditions (the same learning and evaluation sets) and statistically significant results were recorded; see the details in Section~\ref{sec:learning_eval}.

\subsubsection{SRV}

SRV \citep{Freitag:1999_phd} is a rule induction extraction system working on a set of learning features; extensible set of features is predefined allowing quick porting to novel domains. \cite{biblio:Survey_of_Web_Information_Extraction_Systems} provided a great and concise description of SRV in their survey article:
\begin{quotation}
SRV is a top-down relational algorithm that generates single-slot extraction rules. It regards IE as a kind of classification
problem. The input documents are tokenized and all substrings of continuous tokens (i.e. text fragments) are labeled as either extraction target (positive examples) or not (negative examples). The rules generated by SRV are logic rules that rely on a set of token-oriented features (or predicates). These features have two basic varieties: simple and relational. A simple feature is a function that maps a token into some discrete value such as length, character type (e.g., numeric), orthography (e.g., capitalized) and part of speech (e.g., verb). A relational feature maps a token to another
token, e.g. the contextual (previous or next) tokens of the input tokens. The learning algorithm proceeds as FOIL \citep{Quinlan:1990:LLD:97128.637817}, starting with entire set of examples and adds predicates greedily to cover as many positive examples and as few negative examples as possible.
\end{quotation}

\cite{Freitag:1999_phd} experimented with the usage of WordNet \citep{Miller:1995:WLD:219717.219748} and the link grammar parser \citep{linkparser}. Surprisingly most of the extraction performance was achieved with only the simplest of information, without the usage of the linguistic resources.

\subsubsection{HMM}

\cite{Freitag1999InformationExtraction} described how Hidden Markov Models can be successfully applied to information extraction. They used a statistical technique called \emph{shrinkage} for balancing the trade-of between the complexity of models and small quantifies of available training data. The combination of shrinkage with appropriately designed topologies yields a learning algorithm comparable with the state of the art systems.

The authors do not report usage of any additional linguistic resources like WordNet, POS tagger or syntactical parser.

\subsubsection{Elie}

Elie \citep{DBLP:conf/ecml/FinnK04} is another representative of a classical propositional learner working on a set of learning features manually extracted from text. \cite{DBLP:conf/ecml/FinnK04} show that the use of an off-the-shelf support vector machines implementation is competitive with current IE algorithms and that their ``multilevel boundary classification'' approach outperforms current IE algorithms on a variety of benchmark tasks.

The multilevel boundary classification approach consists in a combination of the predictions of two sets of classifiers, one set (L1) with high precision and one (L2) with high recall. The L1 classifiers adopt the the standard ``IE as classification'' approach. L2 uses a second level of ``biased'' classifiers trained only on surrounding tokens of positive learning instances.

The authors used Brill's POS tagger \citep{Brill:1994:ATP:199288.199378} providing, apart from POSs, also chunking information (noun-phrases, verb-phrases) about the tokes. Their second resource was a gazetteer list containing U.S. fist-names and last-names, list of countries, cities, streets, titles, etc. See the details in their paper.

\subsubsection{SVM with ILP Feature Induction}

We already mentioned the work of \cite{DBLP:conf/ilp/RamakrishnanJBS07} in previous sections. Once because they used deep language parsing (MINIPAR parser \citep{minipar}) and once because they used ILP. Their approach is very close to ours and it is even more complex. Instead of using ILP directly for the construction of extraction rules, they used ILP just for construction of good learning features for SVM classifier. They also used WordNet as a source of additional information. 

Surprising fact about their solution is that it was not that much successful as one would expect (see the results in Section~\ref{sec:learning_eval_acq_third}), especially when compared with the relatively simple solution of the PAUM algorithm that we have been experimenting with. It could be caused by the usage of different version of the dataset or by too large number of learning features (the authors reported up to 20,000 learning features).


\subsection{Semantic Annotation}
Last category of related work goes in the direction of semantics and ontologies. \cite{Bon04b} described  ontology features in GATE. They can be easily used to populate an ontology with extracted data. We discus this topic later in Section~\ref{sec:learning_SemanticInterpretation}.

See also references to related work connected with ontologies and information extraction in the following section. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extraction Ontologies} \label{sec:relwork_ext_ont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ontology-based Information Extraction (OBIE) \citep{citeulike:7291004} or Ontology-driven Information Extraction \citep{Yildiz:2007:OMO:1793154.1793216} has recently emerged as a subfield of information extraction. Furthermore, Web Information Extraction \citep{biblio:Survey_of_Web_Information_Extraction_Systems} is a closely related discipline. Many extraction and annotation tools can be found in the above mentioned surveys (\cite{biblio:Survey_of_Web_Information_Extraction_Systems}). Many of the tools also use an ontology as the output format, but almost all of them store their extraction models in proprietary formats and the models are accessible only by the corresponding tool.

In the literature, we have found only two approaches that use extraction ontologies. The former one was published by D. Embley \citep{DBLP:conf/er/EmbleyTL02,Embley:2004:TSU:1012294.1012295}
and the later one -- IE system Ex\footnote{\url{http://eso.vse.cz/~labsky/ex/}} was developped by M. Labsk\'{y} \citep{springerlink:10.1007/978-3-642-01891-6_5}. 
But in both cases the extraction ontologies are dependent on the particular tool and they are kept in XML files with a proprietary structure.


By contrast \cite{citeulike:7291004} (a recent survey of OBIE systems) do not agree with allowing for extraction rules to be a part of an ontology. They use two arguments against that:
\begin{enumerate}
	\item Extraction rules are known to contain errors (because they are never 100\% accurate), and objections can be raised on their inclusion in ontologies in terms of formality and accuracy.

	\item It is hard to argue that linguistic extraction rules should be considered a part of an ontology while information extractors based on other IE techniques (such as SVM, HMM, CRF, etc. classifiers used to identify instances of a class when classification is used as the IE technique) should be kept out of it: all IE techniques perform the same task with comparable effectiveness (generally successful but not 100\% accurate). But the techniques advocated for the inclusion of linguistic rules in ontologies cannot accommodate such IE techniques.
	
The authors then conclude that either all information extractors (that use different IE techniques) should be included in the ontologies or none should be included.
\end{enumerate}



Concerning the first argument, we have to take into account that extraction ontologies are not ordinary ontologies, it should be agreed that they do not contain 100\% accurate knowledge. The estimated accuracy of the extraction rules can be saved in the extraction ontology and it can then help potential users to decide how much they will trust the extraction ontology.

Concerning the second argument, we agree that in the case of complex classification based models (SVM, HMM, CRF, etc.) serialization of such model to RDF does not make much sense (cf. Section~\ref{sec:onto_tasks}). But on the other hand we think that there are cases when (shareable) extraction ontologies can be useful and in the context of Linked Data\footnote{\url{http://linkeddata.org/}} providing shareable descriptions of information extraction rules may be valuable. It is also possible that new standard ways how to encode such models to an ontology will appear in the future.


%it is not always possible to save an extraction model to an ontology (at least not currently). \subsection{Notes on Ontology Definitions}
Let us briefly remind main ontology definitions because they are touched and in a sense misused in our work on shareable extraction ontologies. The most widely agreed definitions of an ontology emphasize the shared aspect of ontologies: 
\begin{quote}
An ontology is a formal specification of a shared conceptualization.	\citep{so17864}
\end{quote}

\begin{quote}
An ontology is a formal, explicit specification of a shared conceptualization. \citep{Studer1998161}
\end{quote}

Of course the word `shareable' has different meaning from `shared'. (Something that is shareable is not necessarily shared, but on the other hand something that is shared should be shareable.) We do not think that shareable extraction ontologies will contain shared knowledge about how to extract data from documents in certain domain. This is for example not true for all extraction models artificially learned from a training corpus. Here shareable simply means that the extraction rules can be shared amongst software agents and can be used separately from the original tool. This is the deviation in use of the term `ontology' in the context of (shareable) extraction ontologies (similarly for ``document ontologies'', see in Section~\ref{sec:onto_doc_ont}).






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Document Classification} \label{sec:relwork_doc_classification} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{General Document Classification}

There are plenty of systems dealing with text mining and text classification. \cite{biblio:ReYaLiOntoText08} use ontology modeling to enhance text identification. \cite{biblio:CAP} use preprocessed data from National Automotive Sampling System and test various soft computing methods to model severity of injuries (some hybrid methods showed best performance). Methods of Information Retrieval (IR) are numerous, mainly based on key word search and similarities. The Connection of IR and text mining techniques with web information retrieval can be found in the chapter ``Opinion Mining'' in the book of Bing \cite{biblio:WebDataMining}. 

\subsection{ML Classification with Monotonicity Constraint}
The Fuzzy ILP Classifier can be seen as an ordinary classifier for data with the monotonicity constraint (the target class attribute has to be monotonizable -- a natural ordering has to exist for the target class attribute). There are several other approaches addressing the classification problems with the monotonicity constraint.

The CART-like algorithm for decision tree construction does not guarantee a resulting monotone tree even on a monotone dataset. The algorithm can be modified \citep{biblio:mon_trees} to provide a monotone tree on the dataset by adding the corner elements of a node with an appropriate class label to the existing data whenever necessary.

An interesting approach was presented by \cite{biblio:mon_transf}: first, the dataset is ``corrected'' to be monotone (a minimal number of target class labels is changed to get a monotone dataset), then a learning algorithm (linear programming boosting in the cited paper) is applied.

Several other approaches to monotone classification have been presented, including instance based learning \citep{biblio:ibl} and rough sets \citep{biblio:rough_sets}.








