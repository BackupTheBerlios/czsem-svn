\chapter{Models and Methods} \label{sec:ch_methods} 


In this chapter, the four methods corresponding to the four topics of this thesis will be introduced and described. The chapter is structured to four main sections according to the four topics. 



\section{Manual Design of Extraction Rules} \label{sec:manual_methods}
\graphicspath{{../img/ch50/}}

\begin{wrapfigure}[21]{r}{.3\hsize}
\vspace{-0.5cm}
\centerline{\includegraphics[width=\hsize]{ap_schema}}
\caption{Schema of the extraction process.}
\label{fig:manual_ap_schema}
\end{wrapfigure}

%\begin{figure}
	%\centering
		%\includegraphics[width=0.2\hsize]{ap_schema}
	%\caption{Schema of the extraction process.}
	%\label{fig:manual_ap_schema}
%\end{figure}


In this section, the extraction method based on manually created linguistic rules will be described. First of all a data flow schema of the extraction process will be presented. Then a description of several stages of evolution of the method will demonstrate how this method came to its existence and which decisions stood behind the development and the final implementation, which will be described in the next chapter.

The approach is based on linguistic preprocessing, oriented to text structured to individual sentences. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Flow} \label{sec:manual_data_flow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




The method was designed as a method for extraction of information from web resources. Thus the extraction process starts on the Web. On the other hand the method was intended to serve the evolution of the Semantic Web, so the final goal of the extraction process is the extracted information stored in the form of a semantic web ontology. A schema in Figure~\ref{fig:manual_ap_schema} splits the process into four steps (phases) among five media types. The schema does not cover the extraction rules design phase; it is assumed that extraction rules were already designed by a human designer; Section~\ref{sec:manual_rules_design} provides details about that. A description of individual steps follows. 


\begin{figure}[t!]
	\centering
		\includegraphics[width=0.8\hsize]{article}
	\caption{Example web page with an accident report.}
	\label{fig:manual_article}
\end{figure}



\begin{enumerate}
\item \emph{Extraction of text} \\ First of all, target web pages have to be identified, downloaded and text has to be extracted form them. This issue is not studied in the present work. A RSS feed of the fire department web-site was used to identify and download relevant pages and the desired text (see highlighted area in the Figure~\ref{fig:manual_article}) was extracted by means of a regular expression. The text is an input of the second phase.

\item \emph{Linguistic annotation} \\ In this phase the extracted text is processed by several linguistic tools. The tools analyze the text and produce corresponding set of linguistic dependency trees. There is a rich choice of linguistic tools available (see Section~\ref{sec:third_ling_tools}), but only PDT based tools were used in illustration examples and linguistic trees are always of the form of Tectogrammatical trees, but note that the method is general and it is not limited to the PDT presentation of linguistic dependency trees.

\item \emph{Data extraction} \\ The structure of linguistic dependency trees is used for the extraction of relevant structured data form the text. The extraction method used in this phase is the main topic of this section (Section~\ref{sec:manual_methods}) and details about the method are discuses in following subsections.

\item \emph{Semantic representation} \\ Although the output of the previous phase is already of a structured form, it is not necessarily of the form of a semantic web ontology. The output has to be converted to some ontology format (RDF, OWL) and appropriate schema for given domain and type of extracted information. 

This last step of the extraction process represents a logical distinction between two functionally different tasks of the extraction method. The first task represented by the previous (Data extraction) phase is responsible for choosing of ``what’’ should be extracted, while the second task (Semantic representation) should determine what to do with the extracted data or how to formulate the pieces of information discovered by the Data extraction phase. The border between these two tasks is rather vague and they could be merged together, but we think that the distinction between them can help to understand the problem better.


In the present work only a design of this phase is provided (with a small exception -- using shareable extraction ontologies, see Section~\ref{sec:onto_extraction_ontologies}) because: (1) this task seems to be strongly dependent on manual work of human designers and (2) its potential for meaningful scientific investigation seems to be rather small. Details about this step are further discussed in Section~\ref{sec:manual_sem_interpret}.
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evolution of the Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our first attempt to extract some structured data from linguistically annotated text was done in a standard procedural programming environment (more precisely in Perl, Btred). After an initial phase of development first extraction rule was created as a single executable procedure. This procedure will be described in the next chapter (Section~\ref{sec:manual_Procedural_Extraction_Rules}) and listed in Figure~\ref{fig:btred_rule}. There are many drawbacks of the procedural rule design: such extraction rules are difficult to read, tedious to create, error prone, graphical or assisted design is impossible. On the other hand, this approach has the advantage of the programming language proximity. When a designer designs a procedural extraction rule he or she actually codes it in a procedural programming language and it is easy to add some additional functionality that will be executed and evaluated along with the extraction rule. Thus the designer has the full power of the programming language in hand and he or she can use it inside of the extraction rule. This possibility will be discussed later in the context of semantic interpretation of extracted data.

Dissatisfaction from tedious and time consuming design of procedural extraction rules led us to the idea of a special rule language. We were looking for a language that would allow expressing tree patterns and consequent extraction actions of extraction rules. It turned out that the Netgraph query language is very suitable for the first purpose -- expressing tree patterns. An extension of the Netgraph query language to a language for extraction rules was quite simple then. See the details in the next section.

Last two steps in the evolution of the extraction method were (1) creation of machine learning procedure that is capable to learn extraction rules from manually annotated corpus (see in Section~\ref{sec:learning_methods}) and (2) possibility to export extraction rules to a shareable extraction ontology so the extraction rules can be evaluated on a document by an ordinary semantic web reasoner outside of the original extraction tool (see in Section~\ref{sec:onto_extraction_ontologies}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Netgraph Based Extraction Rules}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
	\centering
		\includegraphics[width=0.7\hsize]{extract_patern}		
\\Transcript:\\
\begin{tabular}{|c|c|c|c|c|}
\hline
zranit & usmrtit & zemřít & zahynout & přežít\\
to injure & to kill & to die & to wane & to survive\\
\hline
\end{tabular}
\\\begin{tabular}{|c|c|c|c|c|c|}
\hline
kdo & člověk & osoba & muž & žena & dítě\\
somebody & (hu)man & person & man & woman & child\\
\hline
\end{tabular}
\\\begin{tabular}{|c|c|c|c|}
\hline
řidič & řidička & spolujezdec & spolujezdkyně\\
driver & woman driver & passenger & woman passenger\\	
\hline
\end{tabular}		
	\caption{A manually created extraction rule investigating numbers of injuries and fatalities.}
	\label{fig:manual_extract_patern}
\end{figure}



Netgraph based extraction rules are declarative, they do not specify a sequential procedure ``what to do with a linguistic tree'', they are rather based on conditions and selections similar to SQL.

Netgraph is a linguistic tool used for searching through a syntactically annotated corpus (see Section~\ref{sec:third_netgraph} for details). Netgraph queries are written in a special query language with a graphical representation. The graphical representation of a query is much better readable than its linear textual representation and we will use the graphical representation only. Figure~\ref{fig:manual_extract_patern} shows an example Netgraph query. It specifies necessary tree structure that has to be present in a matching tree and attribute restrictions that have to hold true for corresponding nodes (the restrictions are printed beside the nodes).



We adopted Netgraph queries and extended them to extraction rules that can be written in following pseudo SQL SELECT form:


\begin{minted}{sql}
	SELECT node1_name.attr1_name, node2_name.attr2_name, ... 
	FROM netgraph_query
\end{minted}

where \emph{netgraph\_query} stands for an arbitrary Netgraph query, \emph{node1\_name}, \emph{node2\_name}, etc. stand for individual names of nodes defined in \emph{netgraph\_query} and \emph{attr1\_name}, \emph{attr2\_name}, etc. stand for names of linguistic attributes whose values should be picked out from the corresponding matching tree nodes.

The extraction works as follows: the Netgraph query is evaluated by searching through a corpus of linguistic trees. Matching trees are returned and the desired information defined by the SELECT part of the extraction rule is taken from particular tree nodes and printed to the output.

Let us explain the extraction procedure in detail, using the example of extraction rule from the Figure~\ref{fig:manual_extract_patern}, which is looking for information about killed and injured people during a (usually car) accident. This rule consists of five nodes. Each node of the rule will match with the corresponding node in each matching tree. So we can investigate the relevant information by reading values of linguistic attributes of matching nodes. We can find out the number (node number 5) and kind (4) of people, which were or were not (2) killed or injured (1) by an accident that is presented in the given sentence. And we can also identify the manner of injury (light or heavy) in the node number 3.

Implementation details and some additional examples of these extraction rules will be presented in the next chapter (Section~\ref{sec:manual_impl_rules}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Methodology for Rule Designers} \label{sec:manual_rules_design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
	\centering
		\includegraphics[angle=-90, width=0.65\hsize]{coverge_tuning}
	\caption{Gradual refinement of an extraction rule.}
	\label{fig:manual_coverge_tuning}
\end{figure}


The process of manual design of extraction rules is heavily dependent on skills and experience of a human designer and fulfillment of the process is quite creative task. In this section we try to pick it up as precisely as possible because we assume that a formal description of this process can help in two ways. First -- a new designer can use it as a cook book and progress more quickly. Second -- it can help with development of tools for assisted rule design. We will concentrate on the Netgraph based extraction rules because we think they are more useful.

The process consists of two parts: construction of a Netgraph query and semantic interpretation of the query. The semantic interpretation part will be discussed in the next section.

One obvious preposition of the procedure is that we have a collection of training texts.
The procedure is demonstrated in Figure~\ref{fig:manual_coverge_tuning} and it starts with frequency analysis of words (their lemmas) occurring in the texts. Especially frequency analysis of verbs is very useful --- meaning of a clause is usually strongly dependent on the meaning of the corresponding verb.

\textbf{Frequency analysis} helps the designer to choose some representative words (\textbf{key-words}) that will be further used for searching the training text collection. Ideal choice of key-words would cover the majority of sentences that express the information we are looking for and it should cover minimal number of the not-intended sentences. An initial choice need not be always sufficient and the process could iterate.

Next step of the procedure consists in \textbf{investigating trees} that are covered by key-words. The designer is examining \textbf{matching trees} --- looking for positions of key-words and their \textbf{neighboring} nodes.

After that the designer can formulate an initial \textbf{Netgraph query} and he or she can compare result of the Netgraph query with the coverage of key-words. Based on this he or she can reformulate the query and gradually refine the query and \textbf{tune the query coverage}.

There are two goals of the query tuning. The first goal is maximization of the relevance of the query. An ideal result is a query that covers all sentences expressing given type of information and no other. The second goal is to involve all important tree-nodes to the query. The second goal is important because the \textbf{complexity of the query} (number of involved nodes) makes it possible to extract more complex information. For example see the query on the Figure~\ref{fig:manual_extract_patern} --- each node keeps different kind of information.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Interpretation of Extracted Data} \label{sec:manual_sem_interpret}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After the designer has successfully formulated the Netgraph query he or she has to supply semantic interpretation of the query. This interpretation expresses how to transform matching nodes of the query (and the available linguistic information connected with the nodes) to the output data.

We did not talk about the extraction output so far. It will be described in the next chapter. For the current description, it is sufficient to say that both methods (the procedural one and the declarative one) have, although structured but still, proprietary XML extraction output. This corresponds to the penultimate stage (raw data) of our data flow schema presented in Section~\ref{sec:manual_data_flow}. In this section, we will describe details about the last step of the data flow schema -- semantic representation of extracted data.

In Section~\ref{sec:manual_impl_output}, the difference between the output of the procedural extraction rule (Section~\ref{sec:manual_Procedural_Extraction_Rules}, Figure~\ref{fig:btred_xml}) and the Netgraph based extraction rule (Section~\ref{sec:manual_Netgraph_Based_Extraction_Rules}, Figure~\ref{fig:select_xml}) can be observed. The procedural one is closer to the semantics of the extracted data while the Netgraph based one is more general, rather based on the semantics of the matching tree and extraction rule. The difference is connected with the difference of the design processes of these extraction rules. While Netgraph based rules are designed in a comfortable way using a graphical tool, the procedural rules have to be coded manually in the programming language of Perl. A Perl programmer has great freedom in the design of a procedural rule and he or she can adapt the rule such that it precisely respects the semantics of extracted data. A designer of a Netgraph based rule has the only freedom in the construction of a Netgraph query and in selection particular query nodes and linguistic attributes that will be selected for the output.

As stated in Section~\ref{sec:manual_data_flow} about the data flow, the goal of our extraction process is in the form of a semantic web ontology. This is not difficult in the case of procedural rules. Once the schema (or vocabulary) of the target ontology is selected, the extraction rules can be simply designed to produce the output of that form (Note that semantic web ontologies can be captured in a specific XML format.)

In the case of Netgraph based queries, the situation is more complex and different solutions can be discovered. All the solutions have one thing in common: additional manual work is necessary. The problem is basically to create a mapping of the data in one format (results of Netgraph based rules) to another format (target ontology). It can be done by a variety of technical means (coded in an arbitrary programming language, XSLT, or using a graphical mapping tool like 
Altova MapForce\footnote{\url{http://www.altova.com/mapforce/xml-mapping.html}}
or
Stylus Studio\footnote{\url{http://www.stylusstudio.com/xsd_to_xsd.html}}
). 



\begin{figure}[b!]
	\centering
		\includegraphics[angle=-90, width=\hsize]{semantic_interpretation}
	\caption{Semantic interpretation of the extraction rule.}
	\label{fig:manual_semantic_interpretation}
\end{figure}


\begin{figure}
	\centering
		\includegraphics[angle=-90, width=\hsize]{instances}
	\caption{Extracted instances of the target ontology.}
	\label{fig:manual_instatnces}
\end{figure}


\begin{wrapfigure}[15]{r}{.35\hsize}
	\centering
		\includegraphics[angle=-90, width=\hsize]{classes}
	\caption{Schema of the target ontology.}
	\label{fig:manual_classes}
\end{wrapfigure}



Similar but in a sense different solution is to ground the mapping directly in extraction rules. Instead of creating mapping of the extraction output, extraction rules will contain also the information about the form of the extraction output. Selection of particular query nodes and linguistic attributes for the output will be extended by the specification of how the attributes will be rendered on the output. A graphical representation of such extraction rule can look like in Figure~\ref{fig:manual_semantic_interpretation}. It shows the connection between a Netgraph query on the left and an ontology instance on the right. Every node of the query can be translated to the ontology and the translation can be configured. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[b!]
\begin{minted}[linenos,  fontsize=\footnotesize,
               frame=lines]{sparql}

SELECT ?action ?participant ?participant_type ?quantity
WHERE {
	{
		?action rdf:type :Incident;
			:actionType "death";
			:negation false.
	} UNION {
		?action rdf:type :Incident;
			:actionType "survival";
			:negation true.
	}
	?action :hasParticipant ?participant.
	?participant :participantType ?participant_type.
	OPTIONAL {
		?participant :participantQuantity ?quantity.
	}
}
\end{minted}
\caption{\emph{SPARQL} query that summarizes fatalities of particular incidents.}
\label{fig:sparql_aggregation}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The configurable translations are the most interesting part of these extraction rules. The linguistic information on one side has to be converted to the ontological information on the other side. In Figure~\ref{fig:manual_semantic_interpretation}, following translation types were used: a translation of numerals to numbers, lexical translation from a source language (Czech), and detection of negation present in a query node.

For better illustration, Figure~\ref{fig:manual_instatnces} shows how the extraction output would look like in the semantic case\footnote{The same data will be used in the next chapter in the example of raw extraction output (Figure~\ref{fig:select_xml}).}. The presented ontology was designed only for the illustration. Schema of the ontology can be seen in the Figure~\ref{fig:manual_classes}. It consists of two classes (or concepts): \emph{Incident} and \emph{Participant}. These classes are connected with a relation \emph{hasParticipant}. There are also some data-type properties (\emph{actionType}, \emph{actionManner}, \emph{negation}, \emph{participantType}, \emph{participantQuantity}) to cover the extracted data. 

The last illustration is a SPARQL query (Figure~\ref{fig:sparql_aggregation}) that would display a table of fatalities present in extracted RDF data. The query is based on the previous ontology and it demonstrates possible use of the schema and the extracted data.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Machine Learning of Extraction Rules} \label{sec:learning_methods} \graphicspath{{../img/ch60/}}

In this section we present %main results and reflections of our ongoing PhD project, 
our method for information extraction and annotation of texts, which is based on a deep linguistic analysis and Inductive Logic Programming (ILP) and implemented with the great help of the GATE framework. This approach is quite novel because it directly combines deep linguistic parsing with machine learning (ML). This combination and the use of ILP as a ML engine have following benefits: Manual selection of learning features is not needed. 
The learning procedure has full available linguistic information at its disposal and it is capable to select relevant parts itself. Extraction rules learned by ILP can be easily visualized, understood and adapted by human.


\subsection{Data Flow}


Similarly to the previous case with manually designed rules, also this extraction method was designed as a method for extraction of information from web resources and the goal is in extracted data in the form of semantic web ontology. The schema of the extraction process is different than the one presented in the previous section because it includes also the learning phase when extraction rules are learned from a learning collection. The present schema can be found on Figure~\ref{fig:ILP_data_flow}, it is an adaptation of the general schema presented in Section~\ref{sec:third_gate_ML}. The schema is a little complicated because it illustrates two cases (or phases) in one picture:
\begin{enumerate}
	\item the learning phase and
	\item the application phase when existing extraction rules are applied to new documents and new information is being extracted.
\end{enumerate}

\subsubsection{Learning Phase}
We will start the description with the learning phase and we will start on the Web (upper left corner of the schema). We need to obtain a learning collection of texts. They can be found on the web (the violet cloud), downloaded and text has to be extracted form them (the texts image). Now we need a human annotator (the worker image bellow), who will look at each of the texts and create gold standard annotations for these texts. These steps can be done using the GATE framework and we will not describe them further. 

Before we can execute the learning procedure of Inductive Logic Programming (green circle), three more steps have to be done:
\begin{enumerate}
	\item perform automated linguistic analysis, which will construct linguistic trees from the texts,
	\item transform these linguistic trees to ILP background knowledge and
	\item transform the gold standard annotations to ILP learning examples.
\end{enumerate}
These steps will be described in following sections and in the next chapter about implementation (Section~\ref{sec:learning_impl}). The ILP learning procedure will produce extraction rules (orange oval) that will be used in the application phase.

\subsubsection{Application Phase}
Again, let us start the description on the web. Assume that target web pages for extraction have been identified, text extracted from them and linguistic trees constructed from the text. In this phase, we do not need ILP, but ordinary Logic Programming is still used for deciding which linguistic trees are covered by extraction rules. Therefore, again, the linguistic trees have to be transformed to ILP background knowledge.
 The previously learned extraction rules are applied on the ILP background knowledge constructed from the new trees during the main extraction process (green circle in the top right corner of the schema) and it will produce new extracted data with the same semantics that was used during manual annotation.

\begin{figure}
	\centering
		\includegraphics[angle=-90, width=0.85\hsize]{ILP_data_flow}
	\caption{ILP data flow.}
	\label{fig:ILP_data_flow}
\end{figure}


\subsection{Closer Investigation}
The description of the data flow schema in the previous section was quite general, not containing any implementation specific details. In this section, we want to provide more details about the actual realization of that general data flow because some interesting problems are connected with it.

First of all, the interface of this extraction method is different from the previously described method based on manually designed extraction rules. The previous method was realized as purely information extraction method while this method performs document annotation and therefore the correspondence of extracted information with its position in text has to be preserved, see details in following sections. 

Another difference with the previous approach is that the previous approach is based on Netgraph and Netgraph is responsible for the management of linguistic trees in that case. The present method is based on GATE and ILP and these tools do not provide any special functions for working with linguistic trees. These functions were added by us and they provide:
\begin{itemize}	
	\item conversion of PDT linguistic trees to GATE annotations (see details in Section~\ref{sec:learning_pdt_in_gate}) and the possibility of calling TectoMT linguistic analysis directly from GATE (see details in Section~\ref{sec:learning_tectomt_wrapper}) and
	
	\item integration of Prolog and ILP with GATE (see details in Section~\ref{sec:learning_ilp_wrapper}), which includes conversion of GATE annotations and the linguistic tree structure to ILP background knowledge (see details in Section~\ref{sec:learning_ilp_serialization}).
\end{itemize}





\subsection{Correspondence of GATE Annotations with Tree Nodes}

On the one hand, our extraction method is based on linguistic trees that are composed of individual tree nodes, but on the other hand, we are performing document annotation using GATE, which means that annotations can occupy any segment of text, e.g. a segment starting or ending in the middle of some word. The correspondence of tree nodes with individual words (or tokens) of text is quite clear\footnote{Tectogrammatical tree nodes make it a little more difficult, see details in Section~\ref{sec:third_PDT_layers}.} but the learning algorithm is not capable to deal with partial word matches. This is not a problem in the case when annotations discovered by the extraction technique are put to the text because all the annotations will simply occupy whole words only. The problem can occur when ILP learning data are constructed from GATE annotations. During this construction, only those tree nodes that are completely covered by corresponding annotations will be accordingly marked in the ILP learning data.

%kvuli footenote-um, aby se vesly...
%\pagebreak

Let us for example have a phrase ``eight thousand Crowns’’ and an annotation labeled as ``amount’’ that will almost cover the whole phrase up to the word ``Crowns’’, which will be covered only partly, without the ending ``s’’. Thus the annotated text will be ``eight thousand Crown’’.\footnote{Such annotation could be created by an uninformed annotator who could think that currencies should be marked as in singular form.} During the construction of ILP learning data, only the tree nodes corresponding to words ``eight’’ and ``thousand’’ will be marked as ``amount’’, not the node that corresponds with the word ``Crowns’’.

Fortunately, this phenomenon does not occur very often, in fact we have not met it in our experiments so far. But there is another issue that is connected with multi-word annotations and it is quite common. See in the next section.


\subsection{Root/Subtree Preprocessing/Postprocessing} \label{sec:learning_root_subtree}
Sometimes annotations span over more than one token. This situation complicates the process of machine learning and this situation is often called as ``chunk learning''. Either we have to split a single annotation to multiple learning instances and after the application of extraction rules we have to merge them back together, or we can change the learning task from learning annotated tokens to learning borders of annotations (start tokens and end tokens). The later approach is implemented in GATE in \emph{Batch Learning PR}\footnote{\url{http://gate.ac.uk/userguide/sec:ml:batch-learning-pr}} in the `SURROUND' mode.

We have used another approach to solve this issue. Our approach is based on syntactic structure of a sentence and we call it ``root/subtree preprocessing/postprocessing''. The idea is based on the observation that tokens of a multi-token annotation usually have a common parent node in a syntactic tree. So we can
\begin{enumerate}
	\item extract the parent nodes (in dependency linguistics this node is also a token and it is usually one of the tokens inside the annotation), 
	\item learn extraction rules for parent nodes only and 
	\item span annotations over the whole subtrees of root tokens found during the application of extraction rules.
\end{enumerate}
We call the first point as \emph{root preprocessing} and the last point as \emph{subtree postprocessing}. The situation is illustrated on Figure~\ref{fig:tree-subtree}. We have successfully used this technique for the `damage' task of our evaluation corpus (see Section~\ref{sec:learning_eval} for details.)

\begin{figure}
	\centering
		\includegraphics[width=0.85\hsize]{tree-subtree}
	\caption{Root/Subtree Preprocessing/Postprocessing example, see also Figure~\ref{fig:intro_damage_tree} with the original tree.}
	\label{fig:tree-subtree}
\end{figure}


\subsection{Learning on Named Entity Roots} \label{sec:learning_ne_roots}
Another technique, that we have used to improve the efficiency of ILP machine learning, is closely connected with named entities. It can be used when target entities of an extraction task always (or most often) overlap with annotations of some available type (e.g. named entity). For example entities of the task `acquired' always overlap with some named entity, or in other words, `acquired' entities are merely a special case of named entities.

In this case, the extraction task can be rephrased as ``decide which of the named entities belong to the extraction task.'' The collection of learning examples is reduced only to named entities and the learning algorithm is then faster and can be also more accurate.

In order to ILP had the opportunity to decide about these named entities, they have to be connected with additional information present in linguistic trees. But how to connect a potentially multi-token named entity with the nodes of linguistic trees? One possibility is to connect each named entity with all tree nodes it occupies but we have used a simpler solution when only the root token of a named entity is connected.

This solution was used for the majority of extraction tasks of the Acquisitions dataset; see evaluation details in Section~\ref{sec:learning_eval_acq}.

\subsection{Semantic Interpretation} \label{sec:SemanticInterpretation}

Information extraction can solve the task ``how to get documents annotated'', but as we aim on the semantic annotation, there is a second step of ``semantic interpretation'' that has to be done. In this step we have to interpret the annotations in terms of a standard ontology. On a very coarse level this can be done easily. Thanks to GATE ontology tools \citep{Bon04b} we can convert all the annotations to ontology instances with a quite simple JAPE \citep{Cunningham00jape:a} rule, which takes the content of an annotation and saves it as a label of a new ontology instance or as a value of some property of a shared ontology instance. For example in our case of traffic and fire accidents, there will be a new instance of an accident class for each document and the annotations would be attached to this instance as values of its properties. Thus from all annotations of the same type, instances of the same ontology class or values of the same property would be constructed. This is very inaccurate form of semantic interpretation but still it can be useful. It is similar to the GoodRelation \citep{DBLP:conf/ekaw/Hepp08} design principle of \emph{incremental enrichment}\footnote{\url{http://www.ebusiness-unibw.org/wiki/Modeling_Product_Models#Recipe:_.22Incremental_Enrichment.22}}:

\begin{quote}
``...you can still publish the data, even if not yet perfect. The Web will do the rest -- new tools and people.''	
\end{quote}

But of course we are not satisfied with this fashion of semantic interpretation and we plan to further develop the semantic interpretation step as a sophisticated ``annotation $\rightarrow$ ontology'' transformation process that we have proposed in one of our previous works \citep{biblio:DeVoComputingaggregations2008}.



%kvuli footenote-um, aby se vesly...
%\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Shareable Extraction Ontologies} \label{sec:onto_extraction_ontologies}
\graphicspath{{../img/ch70/}}

In this section we present an extension of the idea of extraction ontologies that was originally presented by \cite{DBLP:conf/er/EmbleyTL02}. We adopt the point that extraction models are kept in extraction ontologies and we add that the extraction ontologies should not be dependent on the particular extraction/annotation tool. In such case the extraction/annotation process can be done separately by an ordinary reasoner.


We present a proof of concept for the idea: a case study with our linguistically based IE engine and an experiment with several OWL reasoners. In the case study (see Section~\ref{sec:onto_case}) the IE engine exports its extraction rules to the form of an extraction ontology. Third party linguistic tool linguistically annotates an input document and the linguistic annotations are translated to so-called document ontology. After that an ordinary OWL reasoner is used to apply the extraction ontology on the document ontology, which has the same effect as a direct application of the extraction rules on the document. The process is depicted in Figure~\ref{fig:rules_app_schema} and it will be described in detail in Section~\ref{sec:onto_case}.



The most closely related work was already presented in Section~\ref{sec:relwork_ext_ont}. The main idea will be illustrated on our case study in Section~\ref{sec:onto_case}, its implementation in Section~\ref{sec:onto_implement} and in Section~\ref{sec:onto_experiment} an experiment with several OWL reasoners and IE datasets will be presented. In Section~\ref{sec:onto_discuss} related issues are discussed and Section~\ref{sec:onto_conclusion} concludes the discussion.

%kvuli footenote-um, aby se vesly...
\pagebreak


\subsection{Document Ontologies and Annotated Document Ontologies} \label{sec:onto_doc_ont}

The idea of shareable extraction ontologies assumes that extraction ontologies will be shareable and they can be applied on a document outside of the original extraction/annotation tool. We further assert that the extraction ontologies can be applied by ordinary reasoners. This assumption implies that both extraction ontologies and documents have to be in a reasoner readable format. In the case of contemporary OWL reasoners there are standard reasoner-readable languages: OWL and RDF in a rich variety of possible serializations (XML, Turtle, N-Triples, etc.) Besides that there exists standard ways like GRDDL or RDFa how to obtain a RDF document from an ``ordinary document'' (strictly speaking XHTML and XML documents).

We call `document ontology' an ontology that formally captures content of a document. A document ontology can be for example obtained from the source document by a suitable GRDDL transformation (as in our experiment). A document ontology should contain all relevant data of a document and preferably the document could be reconstructed from the document ontology on demand.

When a reasoner is applying an extraction ontology to a document, it only has ``to annotate'' the corresponding document ontology, not the document itself. Here ``to annotate'' means to add new knowledge -- new class membership or property assertions. In fact it means just to do the inference tasks prescribed by the extraction ontology on the document ontology. 

Obviously when a document can be reconstructed from its document ontology (this is very often true, it is necessary just to save all words and formatting instructions) then also an annotated document can be reconstructed from its annotated document ontology. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Main Idea Illustrated -- a Case Study} \label{sec:onto_case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, realization of the main idea will be described and illustrated on a case study.

A schema of the case study is presented in Figure~\ref{fig:rules_app_schema}.  
The top row of the image illustrates how TectoMT (third party linguistic tool) linguistically annotates an input document and the linguistic annotations are translated to so-called document ontology by a GRDDL/XSLT transformation.

In the bottom of the picture our IE engine learns extraction rules and exports them to an extraction ontology. The reasoner in the middle is used to apply the extraction ontology on the document ontology and it produces the ``annotated'' document ontology, which was described in the previous section.


\begin{figure}
\centerline{\includegraphics[angle=-90, width=0.8\hsize]{semantic_rules_app_schema}}
\caption{Semantic annotation driven by an extraction ontology and a reasoner -- schema of the process.}
\label{fig:rules_app_schema}
\end{figure}

Implementation of the case study will be described in Section~\ref{sec:onto_implement}. It exploits the approaching support for Semantic Web Rule Language (SWRL) \citep{SWRL}. Although SWRL is not yet approved by W3C it is already widely supported by Semantic Web tools including many OWL reasoners. The SWRL support makes it much easier to transfer the semantics of extraction rules used by our IE tool to extraction ontology. The translation of the native extraction rules to SWRL rules that form the core of the extraction ontology will be also presented in Section~\ref{sec:onto_implement}.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fuzzy ILP Classification} \label{sec:fuzzy_methods}
\graphicspath{{../img/ch80/}}

In this section, we study the problem of classification of textual reports. We are %specifically
 focused on the situation in which structured information extracted from the reports is used for such classification. We present a~proposal and a partial implementation of an experimental classification system based on our previous work on information extraction (see Sections~\ref{sec:manual_methods} and \ref{sec:learning_methods} for details) and fuzzy inductive logic programming (fuzzy ILP).
Our description is based on a case study of seriousness classification of accident reports. 
%Figure~\ref{fig:message} presents an example of an accident report from a message on the web.
We would like to have a tool that is able to classify the accident's degree of seriousness on the basis of information obtained through information extraction.
%Our approach is based on information extraction and on a fuzzy-based machine learning procedure that provides rules for classification of the reports.
In this section,
% we do not provide any details about the information extraction part of the solution (it was already described in Chapters~\ref{sec:manual_manual_rules_chapter} and \ref{ch:ILP_Learning}). 
we concentrate on the classification part and present a detailed study of the fuzzy ILP classification method (so-called `Fuzzy ILP Classifier'). 

This problem represents a challenge of induction and/or mining on several occasions. First we need an inductive procedure when extracting attributes of an accident from text. Second (the subject of this section) we need an inductive procedure when trying to explain an accident's degree of seriousness by its attributes. ILP is used as the inductive procedure in both of these places.
\begin{itemize}
	\item During the information extraction phase, we exploit the fact that ILP can work directly with multirelational data, e.g., deep syntactic (tectogrammatical) trees built from sentences of the processed text.
	\item During the classification phase, we are experimentally using the Fuzzy ILP Classifier because it performs quite well on our dataset (see Section~\ref{sec:fuzzy_eval}) and it is naturally capable of handling fuzzy data, which occurs when the information extraction engine returns confidence probability values along with extracted data. But the description does not go so far and only the approach is fuzzy in the present demonstration.
\end{itemize}





The rest of this section is organized as follows: Design of the experimental system is presented in Section~\ref{sec:fuzzy_system}. %including a short description of the information extraction method and linguistic analyzers. 
Section~\ref{sec:fuzzy_case} provides details about our case study, which is later used in examples. Formal models of the system (including several translations of a fuzzy ILP task to classical ILP) are presented in Section~\ref{sec:fuzzy_ilp_task_translation}, followed by a description of implementation of the models in the system. In Section~\ref{sec:fuzzy_results} we present the main results of the work, and then we evaluate and compare the methods with other well-known classifiers. Section~\ref{sec:conclusion} concludes the chapter.

See also Section~\ref{sec:relwork_doc_classification}, where some closely related works were introduced. 

 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Flow} \label{sec:fuzzy_system}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%\centerline{\includegraphics[width=0.4\hsize]{schema}}
%\caption{Schema of the experimental system.}
%\label{fig:schema}
%\end{figure}


\begin{wrapfigure}[25]{r}{.45\hsize}
\vspace{-0.75cm}
\centerline{\includegraphics[width=\hsize]{schema}}
\caption{Schema of the experimental system.}
\label{fig:schema}
\end{wrapfigure}


A general schema of the experimental system is shown in Figure~\ref{fig:schema}. In the schema, previously developed information extraction tools based on third party linguistic analyzers are used (the upper two dashed arrows). The information extraction tools are supposed to extract structured information from the reports and the extracted information is then translated to ILP background knowledge and, along with a user rating, it is used for the classification (we assume that a small amount of learning data is annotated by a human user). The classification is based on ILP and it could be \emph{fuzzy} or \emph{crisp}. Crisp denotes a straightforward application of ILP and fuzzy stands for the fuzzy method (subject of the present description), see in the next sections.

%\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Case Study -- Accident Seriousness Classification} \label{sec:fuzzy_case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



The main experiment leads to the seriousness classification of an accident presented in a report. %Our long term goal is extraction of semantic information from reports. 
%which is one of possible utilizations of the extracted information. 

For the experiment a classification dataset (Section~\ref{sec:data_classify_fireman}) was built based on a collection of 50 textual reports. We have identified several features presented in these reports and manually extracted the corresponding values. To each report we have also assigned a value of overall ranking of seriousness of the presented accident, which is the target of the classification. The dataset is described in Section~\ref{sec:data_classify_fireman}. 
%The whole dataset can be downloaded from our Fuzzy ILP classifier's web page\footnote{\url{http://www.ksi.mff.cuni.cz/~dedek/fuzzyILP/}}.

In the experiment, we have not used any information extracted by our automated information extraction tools. Instead, we concentrate on~the classification; the actual source of the information is not so important for us and the integration step still lies ahead.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Translation of Fuzzy ILP Task to Several Classical ILP Tasks} \label{sec:fuzzy_ilp_task_translation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Formal definitions of classical and fuzzy ILP tasks were presented in Section~\ref{sec:third_ILP}. In this section, additional formal constructions will be presented, because they are the basis for the implementation.


As far as there is no implementation of fuzzy ILP, we have to use a~classical ILP system. Fortunately, any fuzzy ILP task can be translated to several classical ILP tasks (subject to some rounding and using a finite set of truth values).

Moreover, GAP -- Generalized Annotated Programs \citep{Kifer:1992:TGA:139720.139723} are used in practice, so graded formulas will sometimes be understood as annotated (with classical connectives and with a more complex annotation of the heads of rules). This is possible because \cite{biblio:KLV} showed that (some extension of) fuzzy logic programming is equivalent to (some restriction of) generalized annotated programs. 

The proposed experimental classification system is based on two ILP classification methods -- crisp and fuzzy. 
Technically, the difference between the approaches consists in a different setting of the underlying classical ILP tasks. 
In the following text, translation of the original fuzzy ILP task to the two classical ILP tasks will be presented; the first will be denoted as \emph{crisp}, the second as \emph{monot} (because so called monotonization technique is used). The original \textbf{fuzzy} ILP task can be informally expressed as: ``From a degree of injury, a degree of damage, a degree of duration, etc. determine the degree of seriousness.''   

\bigskip
In the following text, assume that all fuzzy sets take truth values only from a finite set of truth values $T: \{0,1\}\subseteq T\subseteq [0,1]$.

\begin{definition}[Transformation of background knowledge]
A fuzzy background knowledge ${\mathcal B}:B\longrightarrow [0,1]$ is given. For each predicate $p(x)$ in $B$ we insert an additional attribute $t$ to express the truth value, thus we have created a new predicate $p(x,t)$. We construct two classical background knowledge sets $B^{crisp}_T$ and $B^{monot}_T$  as follows:

\begin{itemize}
	\item The first ($B^{crisp}_T$) is a direct coding of a fuzzy value by an additional attribute:
\\If ${\mathcal B}(p(x))=t,\  t \in T$, then we add $p(x,t)$ to  ${B}^{crisp}_T$.
	\item The second ($B^{monot}_T$) is obtained by a process called monotonization:
\\If ${\mathcal B}(p(x))=t,\  t \in T$, then for all $t'\in T,\  t'\le t$ we add $p(x,t')$ to ${B}^{monot}_T$.
This corresponds to the natural meaning of truth values $t$.
\end{itemize}
\end{definition}



Additionally, example sets are constructed in two ways.
\begin{definition}[Transformation of examples]
Given is a fuzzy set of examples ${\mathcal E}:E\longrightarrow [0,1]$. For all $t\in T$ we construct two classical sets of examples $E_t$ and $E_{\ge t}$  as follows:
\begin{itemize}
	\item $E_t=P_t\cup N_t$, where 
$e\in P_t \ \ iff \ \ {\mathcal E}(e)= t$
and $N_t$ is the rest of $E$.
	\item $E_{\ge t}=P_{\ge t}\cup N_{\ge t}$, where 
$e\in P_{\ge t} \ \ iff \ \ {\mathcal E}(e)\ge t$
and $N_t$ is the rest of $E$.
\end{itemize}
\end{definition}



These two translations create two classical ILP tasks for each truth value $t\in T$, the first one is crisp and the second one (\emph{monot}) can be understood as (and translated back to) fuzzy ILP.

\begin{itemize}
	\item The \textit{crisp ILP task} is given by $B^{crisp}_{T}$ and $E_t$ for each $t\in T$.  As a~result, it produces a set of hypotheses $H_t$.

	\item The \textit{monot ILP task} is given by ${B}^{monot}_T$ and $E_{\ge t}$ for each $t\in T$. As a~result, it produces a set of hypotheses $H_{\ge t}$ guaranteeing examples of a degree of at least $t$.
\end{itemize}

Note that among variable boundings in $B$ there are no boundings on the truth value attribute which was added to each predicate; hence, there are no variable boundings on the truth value attribute in $H_{\ge t}$. We did not add an additional truth value attribute to the predicates in $E$. 

Now we sketch the translation of the \emph{monot ILP task} to GAP (fuzzy ILP) rules. 

\begin{theorem}[Translation of the \emph{monot ILP task}]
A fuzzy ILP (or equivalent GAP) task is given by ${\mathcal E}$ and ${\mathcal B}$. Let us assume that $C$ is the target predicate in the domain of ${\mathcal E}$ and for each $t \in T$, $H_{\ge t}$ is a correctly learned solution of the corresponding \textit{monot ILP task} according to the definitions introduced above. We define ${\mathcal H}$ consisting of one GAP rule:
$$C(y):u(x_1,\dots,x_m)\leftarrow B_1(y):x_1\; \&\dots\& \;B_m(y):x_m,$$
here $B_1:x_1 \&\dots\& B_m:x_m$ is the enumeration of all predicates in $B$.

Assume that $B_1(y_1,t_1),\dots,B_n(y_n,t_n)$ are some of the predicates in $B$ (for simplicity enumerated from 1 to $n, \ n \le m$). Then for each rule 
$$
R=C_{\ge t}(y)\Leftarrow B_1(y,t_1),\dots,B_n(y,t_n)
$$
in $H_{\ge t}$ ($C_{\ge t}$ is the monotonized target predicate) we give a constraint in the definition of $u$ as follows:
$$
U_R=u(x_1,\dots,x_m)\ge t \hbox{ if }x_1\ge t_1,\dots,x_n\ge t_n.
$$
Note that all $x_i$ and $y$ are variables, $t_i$ and $t$ are constants and $x_{n+1},\dots,x_m$ have no restrictions.
We learn the function $u$ as a ``monotone'' completion of the rules. 

We claim that if all $H_{\ge t}$ were correctly learned by a classical ILP system, then, for the minimal solution $u$ of all constraints $U_R$, the rule
$$
C(y):u(x_1,\dots,x_m)\leftarrow B_1(y):x_1\; \&\dots\& \;B_m(y):x_m
$$
is a correct solution of the fuzzy ILP task given by ${\mathcal E}$ and ${\mathcal B}$, for all $R\in H_{\ge t}$ and $t\in T$. 
\end{theorem}



\subsubsection{Illustration Example}

In our case \verb@serious(Accident_ID)@ is the fuzzy or GAP target predicate $C$. Let for example $t=3$ and $H_{\ge t}$ be the same as in Figure~\ref{fig:rules} (the last two (25, 26) rows correspond with $H_{\ge 3}$). Then \verb@serious_atl_3(Accident_ID)@ is the monotonized target predicate $C_{\ge t}$ and $B_1$, $B_2$ and $u$ are realized as follows:\\
\hspace*{1cm}$B_1(y,t_1)$ \dots \verb@fatalities_atl(Accident_ID, 1)@,\\
\hspace*{1cm}$B_2(y,t_2)$ \dots \verb@damage_atl(Accident_ID, 1500000)@,\\
$u$ is an arbitrary function of $m$ arguments ($m$ is the number of all predicates used in background knowledge) for which following restrictions hold true:\\
for $t=3$:\\
\hspace*{1cm}$u(x_1,\dots,x_m) \ge 3 \ \hbox{ if }x_1\ge 1$\\
\hspace*{1cm}$u(x_1,\dots,x_m) \ge 3 \ \hbox{ if }x_2\ge 1500000$\\
and similar restrictions for $t=1,2$.


\bigskip
Our presentation is slightly simplified here and we freely switch between fuzzy and GAP programs, which are known to be equivalent, see in~\citep{biblio:KLV}.




