\chapter{Extraction Method Based on ILP Machine Learning} \label{ch:ILP_Learning}
\graphicspath{{../img/ch60/}}


\section{Introduction}
Automated semantic annotation (SA) is considered to be one of the most important elements in the evolution of the Semantic Web. Besides that, SA can provide great help in the process of data and information integration and it could also be a basis for intelligent search and navigation.


A description, implementation and initial evaluation of the method are the main contributions of the present work.







%\section{Data Flow ?or Schema of the Extraction Process?}








\section{Evaluation} %\label{sec:learning_eval}


For the evaluation we selected two attributes of different kind. The first one is `damage' -- an amount (in CZK - Czech Crowns) of summarized damage arisen during a reported accident. The second one is `injuries', it marks mentions of people injured during an accident. These two attributes differ. Injuries annotations always cover only a single token, while damage annotations usually consist of two or three tokens -- one or two numerals express the amount and one extra token is for currency.

These two attributes differ in two directions:
\begin{enumerate}
	\item Injuries annotations always cover only a single token while damage usually consists of two or three tokens - one or two numerals express the amount and one extra token is for currency.
	\item The complexity of the marked information (and the difficulty of the corresponding extraction task) differs slightly. While labeling of all money amounts in the corpus will result in 75\% accuracy for damage annotations, in the case of injured persons mentions there are much more possibilities and indications are more spread in context.
\end{enumerate}


\subsection{Results}


Results of a 10-fold cross validation are summarized in Table~\ref{tab:EvaluationResults}. We used standard information retrieval performance measures: precision, recall and $F_1$ measure and also theirs lenient variants (overlapping annotations are added to the correctly matching ones, the measures are the same if no overlapping annotations are present).

\begin{table}[t]
	\centering
			
\begin{tabular}{|l||r|r|r|r|r|r|r|}
\hline
\textbf{task/method} & \textbf{matching} & \textbf{missing} & \textbf{excessive} & \textbf{overlap} & \textbf{prec.}\% & \textbf{recall}\% & \textbf{F1.0}\%\\
\hline
\hline
\textbf{damage/ILP} & 14 & 0 & 7 & 6 & 51.85 & 70.00 & 59.57\\
\hline
\multicolumn{5}{|l|}{\textbf{damage/ILP -- lenient measures}} & 74.07 & 100.00 & 85.11\\
\hline
\textbf{dam./ILP-roots} & 16 & 4 & 2 & 0 & 88.89 & 80.00 & 84.21\\
\hline
\textbf{damage/Paum} & 20 & 0 & 6 & 0 & 76.92 & 100.00 & 86.96\\
\hline
\hline
\textbf{injuries/ILP} & 15 & 18 & 11 & 0 & 57.69 & 45.45 & 50.85\\
\hline
\textbf{injuries/Paum} & 25 & 8 & 54 & 0 & 31.65 & 75.76 & 44.64\\
\hline
\textbf{inj./Paum-afun} & 24 & 9 & 38 & 0 & 38.71 & 72.73 & 50.53\\
\hline
\end{tabular}
						
	\caption{Evaluation results }
	\label{tab:EvaluationResults}
	\vspace{-0.80cm}
\end{table}

In the first task (`damage') the methods obtained much higher scores then in the second (`injuries') because the second task is more difficult. In the first task also the root/subtree preprocessing/postprocessing improved results of ILP such that afterwards, annotation borders were all placed precisely. The ILP method had better precision and worse recall than the Paum learner but the $F_1$ score was very similar in both cases.

















