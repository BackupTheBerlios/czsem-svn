
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Towards Semantic Annotation Supported by Dependency Linguistics and ILP}

% a short form should be given in case it is too long for the running head
\titlerunning{Semantic Annotation with Dependency Linguistics and ILP}
%\titlerunning{Towards Semantic Annotation Supported by Dependency Linguistics and ILP}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Jan D\v{e}dek}
%\thanks{Please note that the LNCS Editorial assumes that all authors have used
%the western naming convention, with given names preceding surnames. This determines
%the structure of the names in the running heads and the author index.}%
%
\authorrunning{Jan D\v{e}dek}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Department of Software Engineering, Charles University,\\
Prague, Czech Republic
\\\url{dedek@ksi.mff.cuni.cz}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Towards Semantic Annotation Supported by Dependency Linguistics and ILP}
\tocauthor{Jan D\v{e}dek and Peter Vojt\'{a}\v{s}}
\maketitle


\begin{abstract}
In this paper we present a method for semantic annotation of texts, which is based on deep linguistic analysis (DLA) and Inductive Logic Programming (ILP). The combination of DLA and ILP have following benefits: Manual selection of learning features is not needed. The learning procedure has full available linguistic information at its disposal and it is capable to select relevant parts itself. Learned extraction rules can be easily visualized, understood and adapted by human.
A description, implementation and initial evaluation of the method are the main contributions of the paper.
\keywords{Semantic Annotation, Dependency Linguistics, Inductive Logic Programming, Information Extraction, Machine Learning}
\end{abstract}


\section{Introduction}
Automated semantic annotation (SA) is considered to be one of the most important elements in the evolution of the Semantic Web. Besides that, SA can provide great help in the process of data and information integration and it could also be a basis for intelligent search and navigation.

In this paper we present main results and reflections of our ongoing PhD project, a method for classical and semantic information extraction and annotation of texts, which is based on deep linguistic analysis and Inductive Logic Programming (ILP). This approach is quite novel because it directly combines deep linguistic parsing with machine learning (ML). This combination and the use of ILP as ML engine have following benefits: Manual selection of learning features is not needed. 
The learning procedure has full available linguistic information at its disposal and it is capable to select relevant parts itself. Extraction rules learned by ILP can be easily visualized, understood and adapted by human.

A description, implementation and initial evaluation of the method are the main contributions of the paper.




\section{Related work}
%\subsection{ILP users}
There are many users of ILP in the linguistic and information extraction area.
%For example in \cite{stasinos:phd} ILP was used for shallow parsing and phonotactics.
Authors of \cite{Junker99learningfor} summarized some basic principles of using ILP for learning from text without any linguistic preprocessing. One of the most related approaches to ours can be found in \cite{aitken02:_learn_infor_extrac_rules}. The authors use ILP for extraction of information about chemical compounds and other concepts related to global warming and they try to express the extracted information in terms of ontology. They use only part of speech analysis and named entity recognition in the preprocessing step. But their inductive procedure uses also additional domain knowledge for the extraction. In \cite{Ramakrishnan:UsingILPforFeatures} ILP was used to construct good features for propositional learners like SVM to do information extraction. It was discovered that this approach is a little bit more successful than a direct use of ILP but it is also more complicated. The later two approaches could be also employed in our solution.

%\subsection{Deep parsing users}
%As stated in 
%[Unescu:  p. 6 Learning for Information Extraction: From Named Entity Recognition and Disambiguation To Relation Extraction]
%The choice of the actual learning algorithm depends on the type of structural information available. For example, deep syntactic information provided by current parsers for new types of corpora such as biomedical text is seldom reliable, since most parsers have been trained on different types of narrative. If reliable syntactic information is lacking, sequences of words around and between the two entities can be used as alternative useful discriminators.

%As stated in \cite{Bunescu:phd} deep syntactic information provided by current parsers is not always reliable (e.g. for biomedical texts). But in our case deep linguistic parsing plays an essential role.

There are other approaches that use deep parsing, but they often use the syntactic structure only for relation extraction and either do not use machine learning at all (extraction rules have to be handcrafted) 
%[Yakushiji: Event extraction from biomedical papers using a full parser]
\cite{Yakushiji2001},
%[Funde: RelEx-Relation extraction using dependency parse trees]
\cite{RelEx},
%[Buyko: Event extraction from trimmed dependency graphs]
\cite{Buyko:dependencyGraphs}
or do some kind of similarity search based on the syntactic structure
%[Banko: Open Information Extraction from the Web]
\cite{Etzioni08informationExtraction},
%[Wang: Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons]
\cite{Wang:SimilarityTreeSkeletons}
or the syntactic structure plays only very specific role in the process of feature selection for propositional learners %[Mooney: Extracting Relations from Text: From Word Sequences to Dependency Paths].
\cite{Bunescu:DependencyPaths}.

%\subsection{Classical propositional learning (Information extraction - GATE)}
There is also a long row of information extraction approaches that use classical propositional learners like SVM on a set of features manually selected from input text. We do not cite them here. We just refer to \cite{Yaoyong09a} -- using machine learning facilities in GATE. This is the software component (Machine Learning PR) to that we have compared our solution.

%\subsection{Semantic annotation - GATE}
Last category of related works goes in the direction of semantics and ontologies. Because we do not develop this topic in this paper, we just refer to the ontology features in GATE \cite{Bon04b}, which can be easily used to populate an ontology with the extracted data. We discus this topic later in Section~\ref{sec:SemanticInterpretation}.




\section{Exploited methods -- linguistics and ILP}
In our solution we have exploited several tools and formalisms. These can be divided into two groups: linguistics and (inductive) logic programming. First we describe the linguistic tools and formalisms, the rest will follow.

\subsection{GATE}
GATE\footnote{\url{http://gate.ac.uk/}} \cite{dedek:GATE_ACL2002} is probably the most widely used tool for text processing. In our solution the capabilities of document and annotation management, utility resources for annotation processing, JAPE grammar rules \cite{Cunningham00jape:a}, machine learning facilities and performance evaluation tools are the most helpful features of GATE that we have used.

\subsection{PDT and TectoMT}
As we have started with our native language -- Czech (a language with rich morphology and free word order), we had to make tools for processing Czech available in GATE. We have implemented a wrapper for the TectoMT system\footnote{\url{http://ufal.mff.cuni.cz/tectomt/}} \cite{dedek:ZaPtTectoMTHighly2008} to GATE. TectoMT is a Czech project that contains many linguistic analyzers for different languages including Czech and English. We have used a majority of applicable tools from TectoMT: a tokeniser, a sentence splitter, morphological analyzers (including POS tagger), a syntactic parser and the deep syntactic (tectogrammatical) parser. All the tools are based on the dependency based linguistic theory and formalism of the Prague Dependency Treebank project \cite{dedek:PDT20_CD}. So far our solution does not include any coreference and discourse analysis.

%\textbf{Rozepsat PDT - mozna v uvodu?}

%Czech language
%Slavic language, with rich morphology, free word order
%Stanford dependencies


\subsection{Inductive Logic Programming}
Inductive Logic Programming (ILP) \cite{dedek:MuggletonILP} is a machine learning technique based on logic programming. Given an encoding of the known background knowledge (in our case linguistic structure of all sentences) and a set of examples represented as a logical database of facts (in our case tokens annotated with the target annotation type are positive examples and the remaining tokens negative ones), an ILP system will derive a hypothesised logic program (in our case extraction rules) which entails all the positive and none of the negative examples.

%\subsection{ILP tool}
As an ILP tool we have used ``A Learning Engine for Proposing Hypotheses'' (Aleph v5)\footnote{\url{http://www.comlab.ox.ac.uk/activities/machinelearning/Aleph/}}, which we consider very practical. It uses quite effective method of inverse entailment \cite{biblio:InverseEntailment} and keeps all handy features of a Prolog system (we have used YAP Prolog\footnote{\url{http://www.dcc.fc.up.pt/~vsc/Yap/}}) in its background.


From our experiments (Section~\ref{sec:evaluation}) can be seen that ILP is capable to find complex and meaningful rules that cover the intended information.

%\textbf{?? large amount of training data ??}
%
%As we do not have large amount of training data, there is no problem with excessive time demands during learning and the application of the learned rules is simple and quick.





\section{Implementation}
Here we just briefly describe implementation of our system. The system consists of several modules, all integrated in GATE as processing resources.

\subsection{TectoMT wrapper (linguistic anlysis)}
First is the TectoMT wrapper, which takes the text of a GATE document, sends it to TectoMT linguistic analyzers, parses the results and converts the results to the form of GATE annotations.

\subsection{ILP wrapper (machine learninig)}
After a human annotator have annotated several documents with desired target annotations, machine learning takes place. 
This consists of two steps: 
\begin{enumerate}
	\item learning of extraction rules from the target annotations and
	\item application of the extraction rules on new documents.
\end{enumerate}
In both steps the linguistic analysis has to be done before and in both steps background knowledge (a logical database of facts) is constructed from linguistic structures of documents that are beeing processed. We call the process of background knowledge construction as \emph{ILP serialization}. Although this topic is quite interesting we do not present details here because of space limitations.
%; more details are presented below \textbf{(??Sect. 1??)}.

After the ILP serialization is done, in the learning case, positive and negative examples are constructed from target annotations and the machine learning ILP inductive procedure is executed to obtain extraction rules.

In the application case a Prolog system is used to check if the extraction rules entail any of target annotation candidates.
%Learning / application
%1.	serialization -> learning in ILP
%2.	serialization -> application in ILP

The learning examples and annotation candidates are usually constructed from all document tokens (and we did so in the present solution), but it can be optionally changed to any other textual unit, for example only numerals or tectogrammatical nodes (words with lexical meaning) can be selected. This can be done easily with help of \emph{Machine Learning PR} (LM PR) from GATE\footnote{\emph{Machine Learning PR} is an old GATE interface for ML and it is almost obsolete but in contrast to the new \emph{Batch Learning PR} the LM PR is easy to extend for a new ML engine.}.

ML PR provides an interface for exchange of features (including target class) between annotated texts and propositional learners in both directions -- during learning as well as during application. We have used ML PR and developed our \emph{ILP Wrapper} for it. The implementation was a little complicated because complex linguistic structures cannot be easily passed as propositional features, so in our solution we use the ML PR interface only for exchange of the class attribute and annotation id and we access the linguistic structures directly in a document.

%ILP tool presunuto do Exploited methods
%ILP serialization, annotation / tree node intersection 

\subsection{Root/subtree preprocessing/postprocessing}
Sometimes annotations span over more then one token. This situation complicates the process of machine learning and this situation is often called as ``chunk learning''. Either we have to split a single annotation to multiple learning instances and after application we have to merge them back together, or we can change the learning task from learning annotated tokens to learning borders of annotations (start tokens and end tokens). The later approach is implemented in GATE in \emph{Batch Learning PR} in the `SURROUND' mode.

We have used another approach to solve this issue. Our approach is based on syntactic structure of a sentence and we call it ``root/subtree preprocessing/postprocessing''. The idea is based on the observation that tokens of a multi-token annotation usually have a common parent node in a syntactic tree. So we can
\begin{enumerate}
	\item extract the parent nodes (in dependency linguistics this node is also a token and it is usually one of the tokens inside the annotation), 
	\item learn extraction rules for parent nodes only and 
	\item span annotations over the whole subtrees of root tokens found during the application of extraction rules.
\end{enumerate}
We call the first point as \emph{root preprocessing} and the last point as \emph{subtree postprocessing}. We have successfully used this technique for the `damage' task of our evaluation corpus (See Section~\ref{sec:evaluation} for details.)

\subsection{Semantic interpretation}
\label{sec:SemanticInterpretation}
Information extraction can solve the task ``how to get documents annotated'', but as we aim on the semantic annotation, there is a second step of ``semantic interpretation'' that has to be done. In this step we have to interpret the annotations in terms of a standard ontology. On a very coarse level this can be done easily. Thanks to GATE ontology tools \cite{Bon04b} we can convert all the annotations to ontology instances with a quite simple JAPE \cite{Cunningham00jape:a} rule, which takes the content of an annotation and saves it as a label of a new instance or as a value of some property of a shared instance. For example in our case of traffic and fire accidents, there will be a new instance of an accident class for each document and the annotations would be attached to this instance as values of its properties. Thus from all annotations of the same type, instances of the same ontology class or values of the same property would be constructed. This is very inaccurate form of semantic interpretation but it still can be useful. It is similar to the GoodRelation \cite{DBLP:conf/ekaw/Hepp08} design principle of \emph{incremental enrichment}\footnote{\url{http://www.ebusiness-unibw.org/wiki/Modeling_Product_Models#Recipe:_.22Incremental_Enrichment.22}}:
%\begin{quote}
``...you can still publish the data, even if not yet perfect. The Web will do the rest -- new tools and people.''	
%\end{quote}

But of course we are not satisfied with this fashion of semantic interpretation and we plan to develop the semantic interpretation step further as a sophisticated ``annotation $\rightarrow$ ontology'' transformation process that we have proposed in one of our previous works \cite{biblio:DeVoComputingaggregations2008}.

\subsection{How to download}
So far we do not provide our solution as a ready-made installable tool. But a middle experienced Java programmer can build it from source codes in our SVN repository\footnote{Follow the instructions at \url{http://czsem.berlios.de/}}.




\section{Evaluation}
\label{sec:evaluation}

%\subsection{Dataset}
We have evaluated our state of the art solution on a small dataset that we use for development. It is a collection of 50 Czech texts that are reporting on some accidents (car accidents and other actions of fire rescue services). These reports come from the web of Fire rescue service of Czech Republic\footnote{\url{http://www.hzscr.cz/hasicien/}}. The labeled corpus is publically available on the web of our project\footnote{\url{http://czsem.berlios.de/}}.
The corpus is structured such that each document represents one event (accident) and several attributes of the accident are marked in text. For the evaluation we selected two attributes of different kind. The first one is `damage' -- an amount (in CZK - Czech Crowns) of summarized damage arisen during a reported accident. The second one is `injuries', which marks mentions of people injured during an accident. These two attributes differ. Injuries annotations always cover only a single token while damage usually consists of two or three tokens -- one or two numerals express the amount and one extra token is for currency.

%These two attributes differ in two directions:
%\begin{enumerate}
%	\item Injuries annotations always cover only a single token while damage usually consists of two or three tokens - one or two numerals express the amount and one extra token is for currency.
%	\item The complexity of the marked information (and the difficulty of the corresponding extraction task) differs slightly. While labeling of all money amounts in the corpus will result in 75\% accuracy for damage annotations, in the case of injured persons mentions there are much more possibilities and indications are more spread in context.
%\end{enumerate}

%\subsection{Comparison with Paum classifier}
To compare our solution with other alternatives we took the Paum propositional learner from GATE \cite{Li:Paum}. The quality of propositional learning from texts is strongly dependent on selection of right features. We obtained quite good results with features of a window of two preceding and two following token lemmas and morphological tags. The precision was further improved by adding the feature of \emph{analytical function} from syntactic parser (see the last row of Table~\ref{tab:EvaluationResults}).

%Bu we did not want to invest much time to this and the feature setting of the Paum learner was quite simple (a window of two preceding and following token lemmas and morphological tags). We admit that looking for better features could further improve the results of the Paum learner.

%\subsection{Cross validation}
%We used the 10-fold cross validation in the evaluation. Thanks to this technique the evaluation is simple. After processing all the folds each document is processed with some of the ten learned models such that the particular document was not used in learning of that model, so all documents are unseen by the model applied on them. At the end we just compare the gold standard annotations with the learned ones in all documents.

%\subsection{Results}
Results of 10-fold cross validation are summarized in Table~\ref{tab:EvaluationResults}. We used standard information retrieval performance measures: precision, recall and $F_1$ measure and also theirs lenient variants (overlapping annotations are added to the correctly matching ones).

\begin{table}[t]
	\centering
			
\begin{tabular}{|l||r|r|r|r|r|r|r|}
\hline
\textbf{task/method} & \textbf{matching} & \textbf{missing} & \textbf{excessive} & \textbf{overlap} & \textbf{prec.}\% & \textbf{recall}\% & \textbf{F1.0}\%\\
\hline
\hline
\textbf{damage/ILP} & 14 & 0 & 7 & 6 & 51.85 & 70.00 & 59.57\\
\hline
\multicolumn{5}{|l|}{\textbf{damage/ILP -- lenient measures}} & 74.07 & 100.00 & 85.11\\
\hline
\textbf{dam./ILP-roots} & 16 & 4 & 2 & 0 & 88.89 & 80.00 & 84.21\\
\hline
\textbf{damage/Paum} & 20 & 0 & 6 & 0 & 76.92 & 100.00 & 86.96\\
\hline
\hline
\textbf{injuries/ILP} & 15 & 18 & 11 & 0 & 57.69 & 45.45 & 50.85\\
\hline
\textbf{injuries/Paum} & 25 & 8 & 54 & 0 & 31.65 & 75.76 & 44.64\\
\hline
\textbf{inj./Paum-afun} & 24 & 9 & 38 & 0 & 38.71 & 72.73 & 50.53\\
\hline
\end{tabular}
						
	\caption{Evaluation results }
	\label{tab:EvaluationResults}
\end{table}

In the first task (`damage') the methods obtained much higher scores then in the second (`injuries') because the second task is more difficult. In the first task also the root/subtree preprocessing/postprocessing improved results of ILP such that afterwards, annotation borders were all placed precisely. The ILP method had better precision and worse recall than the Paum learner but the $F_1$ score was very similar in both cases.

%\subsection{Examples of learned rules}

In Figure~\ref{fig:rules} we present some examples of the rules learned from the whole dataset. The rules demonstrate a connection of a target token with other parts of a sentence through linguistic syntax structures. For example the first rule connects a root numeral (\emph{n.quant.def}) of `damage' with a mention of `investigator' that stated the mount. In the last rule only positive occurrence of the verb `injure' is allowed.

\begin{figure}
	\scriptsize
[Rule 1] [Pos cover = 14 Neg cover = 0]\\
\verb@damage_root(A) :- lex_rf(B,A), has_sempos(B,'n.quant.def'), tDependency(C,B),@
\verb@   tDependency(C,D), has_t_lemma(D,'investigator').@ %\emph{\%vy353et345ovatel = investigator}
\smallskip\newline
[Rule 2] [Pos cover = 13 Neg cover = 0]\\
\verb@damage_root(A) :- lex_rf(B,A), has_functor(B,'TOWH'), tDependency(C,B),@
\verb@   tDependency(C,D), has_t_lemma(D,'damage').@\\


[Rule 1] [Pos cover = 7 Neg cover = 0]\\
\verb@injuries(A) :- lex_rf(B,A), has_functor(B,'PAT'), has_gender(B,anim),@
\verb@   tDependency(B,C), has_t_lemma(C,'injured').@
\smallskip\newline
[Rule 8] [Pos cover = 6 Neg cover = 0]\\
\verb@injuries(A) :- lex_rf(B,A), has_gender(B,anim), tDependency(C,B),@
\verb@   has_t_lemma(C,'injure'), has_negation(C,neg0).@
	\caption{Examples of learned rules, Czech words are translated.}
	\label{fig:rules}
\end{figure}





%Experience with human-designed rules.










\section{Conclusion and future work}
From our experiments can be seen that ILP is capable to find complex and meaningful rules that cover the intended information. But in terms of the performance measures the results are not better than those from a propositional learner. This is quite surprising observation because Czech is a language with free word order and we would expect much better results of the dependency approach than those of the position based approach, which was used by the propositional learner.

Our method is still missing an intelligent semantic interpretation procedure and it should be evaluated on bigger datasets (e.g. MUC, ACE, TAC, CoNLL) and other languages. So far we also do not provide a method for classical relation extraction. In the present solution we deal with relations implicitly. The method has to be adapted for explicit learning of relations in the form of ``subject predicate object''.

Our method can also provide a comparison of linguistic formalisms and tools because on the same data we could run our method using different linguistic analyzers and compare the results.


\bigskip
\noindent\textbf{Acknowledgments}\\
This work was partially supported by Czech projects: GACR P202/10/0761, GACR-201/09/H057, GAUK 31009 and MSM-0021620838.
The author would like to thank his supervisor Peter Vojt\'{a}\v{s} for the guidance of the PhD thesis.




\bibliographystyle{splncs03}
\bibliography{DedekVojtas_ISWC2010}
\end{document}
